# data_exploration.py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import json
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä—É—Å—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'SimHei']

DIR_DATASET = '../data/google_asl_signs/'

def load_sign_mapping():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–∞–ø–ø–∏–Ω–≥–∞ –∑–Ω–∞–∫–æ–≤ –∫ –∏–Ω–¥–µ–∫—Å–∞–º"""
    with open(os.path.join(DIR_DATASET, 'sign_to_prediction_index_map.json'), 'r') as f:
        sign_mapping = json.load(f)
    return sign_mapping

def load_train_data(sample_size=None):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ train.csv"""
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ train.csv...")
    df = pd.read_csv(os.path.join(DIR_DATASET, 'train.csv'))
    
    if sample_size:
        df = df.sample(n=min(sample_size, len(df)), random_state=42)
    
    print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
    return df

def analyze_signs_distribution(df, sign_mapping):
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞–∫–æ–≤"""
    print("\nüìà –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞–∫–æ–≤...")
    
    # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞–∫–∞
    sign_counts = df['sign'].value_counts()
    
    print(f"   –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤: {len(sign_counts)}")
    print(f"   –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∑–Ω–∞–∫–∏:")
    for i, (sign, count) in enumerate(sign_counts.head(10).items()):
        print(f"     {i+1}. {sign}: {count} –∑–∞–ø–∏—Å–µ–π")
    
    print(f"   –°–∞–º—ã–µ —Ä–µ–¥–∫–∏–µ –∑–Ω–∞–∫–∏:")
    for i, (sign, count) in enumerate(sign_counts.tail(10).items()):
        print(f"     {i+1}. {sign}: {count} –∑–∞–ø–∏—Å–µ–π")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    plt.figure(figsize=(15, 8))
    
    plt.subplot(2, 2, 1)
    sign_counts.head(20).plot(kind='bar')
    plt.title('–¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∑–Ω–∞–∫–æ–≤')
    plt.xlabel('–ó–Ω–∞–∫')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')
    plt.xticks(rotation=45)
    
    plt.subplot(2, 2, 2)
    sign_counts.tail(20).plot(kind='bar')
    plt.title('20 —Å–∞–º—ã—Ö —Ä–µ–¥–∫–∏—Ö –∑–Ω–∞–∫–æ–≤')
    plt.xlabel('–ó–Ω–∞–∫')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')
    plt.xticks(rotation=45)
    
    plt.subplot(2, 2, 3)
    plt.hist(sign_counts.values, bins=50, alpha=0.7, edgecolor='black')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –∑–Ω–∞–∫–æ–≤')
    plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –Ω–∞ –∑–Ω–∞–∫')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞–∫–æ–≤')
    
    plt.subplot(2, 2, 4)
    plt.boxplot(sign_counts.values)
    plt.title('Box plot —á–∞—Å—Ç–æ—Ç—ã –∑–Ω–∞–∫–æ–≤')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –Ω–∞ –∑–Ω–∞–∫')
    
    plt.tight_layout()
    plt.savefig(os.path.join("exploration_output", 'signs_distribution.png'), dpi=150, bbox_inches='tight')
    plt.show()
    
    return sign_counts

def analyze_participants(df):
    """–ê–Ω–∞–ª–∏–∑ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤"""
    print("\nüë• –ê–Ω–∞–ª–∏–∑ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤...")
    
    participant_counts = df['participant_id'].value_counts()
    
    print(f"   –í—Å–µ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {len(participant_counts)}")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞: {participant_counts.mean():.1f}")
    print(f"   –ú–µ–¥–∏–∞–Ω–∞ –∑–∞–ø–∏—Å–µ–π –Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞: {participant_counts.median():.1f}")
    print(f"   –ú–∏–Ω–∏–º—É–º –∑–∞–ø–∏—Å–µ–π –Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞: {participant_counts.min()}")
    print(f"   –ú–∞–∫—Å–∏–º—É–º –∑–∞–ø–∏—Å–µ–π –Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞: {participant_counts.max()}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.hist(participant_counts.values, bins=50, alpha=0.7, edgecolor='black')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π –Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞')
    plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤')
    
    plt.subplot(1, 2, 2)
    participant_counts.head(20).plot(kind='bar')
    plt.title('–¢–æ–ø-20 —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∑–∞–ø–∏—Å–µ–π')
    plt.xlabel('ID —É—á–∞—Å—Ç–Ω–∏–∫–∞')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.savefig(os.path.join("exploration_output", 'participants_analysis.png'), dpi=150, bbox_inches='tight')
    plt.show()
    
    return participant_counts

def analyze_landmark_files(df, sample_size=100):
    """–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ —Å landmarks"""
    print(f"\nüéØ –ê–Ω–∞–ª–∏–∑ landmark —Ñ–∞–π–ª–æ–≤ (–≤—ã–±–æ—Ä–∫–∞ –∏–∑ {sample_size} —Ñ–∞–π–ª–æ–≤)...")
    
    # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)
    
    frame_counts = []
    landmark_counts = []
    file_sizes = []
    missing_data_ratios = []
    
    data_dir = Path(DIR_DATASET)
    
    for idx, row in sample_df.iterrows():
        file_path = data_dir / row['path']
        
        if file_path.exists():
            # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            file_size = file_path.stat().st_size
            file_sizes.append(file_size)
            
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º parquet —Ñ–∞–π–ª
                landmarks_df = pd.read_parquet(file_path)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                frame_counts.append(len(landmarks_df))
                
                # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã landmarks
                # –§–æ—Ä–º–∞—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å: frame, x_0, y_0, z_0, x_1, y_1, z_1, ...
                coord_columns = [col for col in landmarks_df.columns if col.startswith(('x_', 'y_', 'z_'))]
                if coord_columns:
                    landmark_count = len(coord_columns) // 3  # 3 –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–∞ landmark
                    landmark_counts.append(landmark_count)
                else:
                    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç - –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∫—Ä–æ–º–µ frame
                    non_frame_cols = [col for col in landmarks_df.columns if col != 'frame']
                    landmark_count = len(non_frame_cols) // 3
                    landmark_counts.append(landmark_count)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                missing_ratio = landmarks_df.isnull().sum().sum() / landmarks_df.size
                missing_data_ratios.append(missing_ratio)
                
            except Exception as e:
                print(f"   –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {file_path}: {e}")
                continue
    
    if frame_counts:
        print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–¥—Ä–æ–≤:")
        print(f"     –ú–∏–Ω–∏–º—É–º: {min(frame_counts)} –∫–∞–¥—Ä–æ–≤")
        print(f"     –ú–∞–∫—Å–∏–º—É–º: {max(frame_counts)} –∫–∞–¥—Ä–æ–≤")
        print(f"     –°—Ä–µ–¥–Ω–µ–µ: {np.mean(frame_counts):.1f} –∫–∞–¥—Ä–æ–≤")
        print(f"     –ú–µ–¥–∏–∞–Ω–∞: {np.median(frame_counts):.1f} –∫–∞–¥—Ä–æ–≤")
    
    if landmark_counts:
        print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ landmarks:")
        print(f"     –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫: {landmark_counts[0]} (–æ–¥–∏–Ω–∞–∫–æ–≤–æ –¥–ª—è –≤—Å–µ—Ö)")
    
    if file_sizes:
        print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤:")
        print(f"     –ú–∏–Ω–∏–º—É–º: {min(file_sizes)} –±–∞–π—Ç")
        print(f"     –ú–∞–∫—Å–∏–º—É–º: {max(file_sizes)} –±–∞–π—Ç")
        print(f"     –°—Ä–µ–¥–Ω–µ–µ: {np.mean(file_sizes):.1f} –±–∞–π—Ç")
        print(f"     –ú–µ–¥–∏–∞–Ω–∞: {np.median(file_sizes):.1f} –±–∞–π—Ç")
    
    if missing_data_ratios:
        print(f"   –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"     –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {np.mean(missing_data_ratios):.3%}")
        print(f"     –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {max(missing_data_ratios):.3%}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    if frame_counts:
        plt.figure(figsize=(15, 5))
        
        plt.subplot(1, 3, 1)
        plt.hist(frame_counts, bins=30, alpha=0.7, edgecolor='black')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∞–¥—Ä–æ–≤')
        plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤')
        plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
        
        plt.subplot(1, 3, 2)
        plt.boxplot(frame_counts)
        plt.title('Box plot –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∞–¥—Ä–æ–≤')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤')
        
        plt.subplot(1, 3, 3)
        plt.hist(file_sizes, bins=30, alpha=0.7, edgecolor='black')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤')
        plt.xlabel('–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (–±–∞–π—Ç—ã)')
        plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
        
        plt.tight_layout()
        plt.savefig(os.path.join("exploration_output", 'landmark_analysis.png'), dpi=150, bbox_inches='tight')
        plt.show()
    
    return {
        'frame_counts': frame_counts,
        'landmark_counts': landmark_counts,
        'file_sizes': file_sizes,
        'missing_data_ratios': missing_data_ratios
    }

def analyze_landmark_structure(sample_files=5):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã landmark —Ñ–∞–π–ª–æ–≤"""
    print(f"\nüîç –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã landmark —Ñ–∞–π–ª–æ–≤ (–≤—ã–±–æ—Ä–∫–∞ –∏–∑ {sample_files} —Ñ–∞–π–ª–æ–≤)...")
    
    data_dir = Path(DIR_DATASET)
    train_df = pd.read_csv(os.path.join(DIR_DATASET, 'train.csv'))
    
    # –í—ã–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    sample_df = train_df.sample(n=sample_files, random_state=42)
    
    for idx, row in sample_df.iterrows():
        file_path = data_dir / row['path']
        sign = row['sign']
        
        print(f"\n   –§–∞–π–ª: {row['path']}")
        print(f"   –ó–Ω–∞–∫: {sign}")
        
        if file_path.exists():
            try:
                landmarks_df = pd.read_parquet(file_path)
                print(f"   –†–∞–∑–º–µ—Ä: {landmarks_df.shape}")
                print(f"   –ö–æ–ª–æ–Ω–∫–∏: {list(landmarks_df.columns)}")
                print(f"   –ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏:")
                print(landmarks_df.head(3))
                
                # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
                print(f"   –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:")
                print(landmarks_df.dtypes)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
                nan_count = landmarks_df.isnull().sum().sum()
                print(f"   NaN –∑–Ω–∞—á–µ–Ω–∏–π: {nan_count}")
                
            except Exception as e:
                print(f"   –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏: {e}")
        else:
            print(f"   –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")

def analyze_signs_by_participant(df):
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞–∫–æ–≤ –ø–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º"""
    print("\nüë•üìà –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞–∫–æ–≤ –ø–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º...")
    
    # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —É—á–∞—Å—Ç–Ω–∏–∫-–∑–Ω–∞–∫
    participant_sign_matrix = df.groupby(['participant_id', 'sign']).size().unstack(fill_value=0)
    
    print(f"   –†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã: {participant_sign_matrix.shape}")
    print(f"   –£—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {participant_sign_matrix.shape[0]}")
    print(f"   –ó–Ω–∞–∫–æ–≤: {participant_sign_matrix.shape[1]}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º
    signs_per_participant = participant_sign_matrix.sum(axis=1)
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞–∫–æ–≤ –Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞: {signs_per_participant.mean():.1f}")
    print(f"   –ú–µ–¥–∏–∞–Ω–∞ –∑–Ω–∞–∫–æ–≤ –Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞: {signs_per_participant.median():.1f}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–Ω–∞–∫–∞–º
    participants_per_sign = (participant_sign_matrix > 0).sum(axis=0)
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –Ω–∞ –∑–Ω–∞–∫: {participants_per_sign.mean():.1f}")
    print(f"   –ú–µ–¥–∏–∞–Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –Ω–∞ –∑–Ω–∞–∫: {participants_per_sign.median():.1f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(15, 10))
    
    plt.subplot(2, 2, 1)
    plt.hist(signs_per_participant, bins=30, alpha=0.7, edgecolor='black')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–Ω–∞–∫–æ–≤ –Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞')
    plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞–∫–æ–≤')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤')
    
    plt.subplot(2, 2, 2)
    plt.hist(participants_per_sign, bins=30, alpha=0.7, edgecolor='black')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –Ω–∞ –∑–Ω–∞–∫')
    plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞–∫–æ–≤')
    
    plt.subplot(2, 2, 3)
    # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ (—Ç–æ–ø-20 —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏ —Ç–æ–ø-20 –∑–Ω–∞–∫–æ–≤)
    top_participants = signs_per_participant.nlargest(20).index
    top_signs = participants_per_sign.nlargest(20).index
    heatmap_data = participant_sign_matrix.loc[top_participants, top_signs]
    
    sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π'})
    plt.title('–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞: —É—á–∞—Å—Ç–Ω–∏–∫–∏ vs –∑–Ω–∞–∫–∏ (—Ç–æ–ø-20)')
    plt.xlabel('–ó–Ω–∞–∫–∏')
    plt.ylabel('–£—á–∞—Å—Ç–Ω–∏–∫–∏')
    
    plt.subplot(2, 2, 4)
    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏
    correlation_matrix = participant_sign_matrix.T.corr()
    sns.heatmap(correlation_matrix.iloc[:20, :20], cmap='coolwarm', center=0)
    plt.title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏ (—Ç–æ–ø-20)')
    
    plt.tight_layout()
    plt.savefig(os.path.join("exploration_output", 'participant_sign_analysis.png'), dpi=150, bbox_inches='tight')
    plt.show()

def generate_summary_report(df, sign_mapping, landmark_stats):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
    print("\nüìã –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print("=" * 50)
    
    print(f"üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df):,}")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤: {df['sign'].nunique()}")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {df['participant_id'].nunique()}")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {df['sequence_id'].nunique()}")
    
    print(f"\nüéØ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–Ω–∞–∫–æ–≤:")
    sign_counts = df['sign'].value_counts()
    print(f"   –°–∞–º—ã–π —á–∞—Å—Ç—ã–π –∑–Ω–∞–∫: {sign_counts.index[0]} ({sign_counts.iloc[0]} –∑–∞–ø–∏—Å–µ–π)")
    print(f"   –°–∞–º—ã–π —Ä–µ–¥–∫–∏–π –∑–Ω–∞–∫: {sign_counts.index[-1]} ({sign_counts.iloc[-1]} –∑–∞–ø–∏—Å–µ–π)")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –Ω–∞ –∑–Ω–∞–∫: {sign_counts.mean():.1f}")
    
    print(f"\nüë• –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤:")
    participant_counts = df['participant_id'].value_counts()
    print(f"   –°–∞–º—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π —É—á–∞—Å—Ç–Ω–∏–∫: {participant_counts.index[0]} ({participant_counts.iloc[0]} –∑–∞–ø–∏—Å–µ–π)")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞: {participant_counts.mean():.1f}")
    
    if landmark_stats['frame_counts']:
        print(f"\nüé¨ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–∏–¥–µ–æ:")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤: {np.mean(landmark_stats['frame_counts']):.1f}")
        print(f"   –ú–∏–Ω–∏–º—É–º –∫–∞–¥—Ä–æ–≤: {min(landmark_stats['frame_counts'])}")
        print(f"   –ú–∞–∫—Å–∏–º—É–º –∫–∞–¥—Ä–æ–≤: {max(landmark_stats['frame_counts'])}")
    
    if landmark_stats['landmark_counts']:
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ landmark —Ç–æ—á–µ–∫: {landmark_stats['landmark_counts'][0]}")
    
    print(f"\nüíæ –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞:")
    total_size = sum(landmark_stats['file_sizes']) if landmark_stats['file_sizes'] else 0
    print(f"   –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä landmark —Ñ–∞–π–ª–æ–≤: {total_size / (1024**3):.1f} GB")
    
    print("=" * 50)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
    print("üîç –ê–ù–ê–õ–ò–ó GOOGLE ASL SIGNS –î–ê–¢–ê–°–ï–¢–ê")
    print("=" * 50)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    sign_mapping = load_sign_mapping()
    df = load_train_data(sample_size=10000)  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—ã–±–æ—Ä–∫—É –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    
    # –ê–Ω–∞–ª–∏–∑—ã
    sign_counts = analyze_signs_distribution(df, sign_mapping)
    participant_counts = analyze_participants(df)
    landmark_stats = analyze_landmark_files(df, sample_size=200)
    analyze_signs_by_participant(df)
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–Ω–µ–±–æ–ª—å—à–∞—è –≤—ã–±–æ—Ä–∫–∞)
    analyze_landmark_structure(sample_files=3)
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    generate_summary_report(df, sign_mapping, landmark_stats)
    
    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print("üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≥—Ä–∞—Ñ–∏–∫–∏:")
    print("   - signs_distribution.png")
    print("   - participants_analysis.png") 
    print("   - landmark_analysis.png")
    print("   - participant_sign_analysis.png")

if __name__ == "__main__":
    main()
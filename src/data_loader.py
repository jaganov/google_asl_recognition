# data_loader.py
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
from preprocessing import ASLPreprocessor
from augmentations import ASLAugmentations

class ASLDataset(Dataset):
    """Dataset –¥–ª—è Google ASL Signs"""
    
    def __init__(self, 
                 data_dir: str,
                 split_file: str,
                 preprocessor: ASLPreprocessor,
                 max_len: int = 384,
                 augment: bool = False):
        """
        Args:
            data_dir: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
            split_file: –§–∞–π–ª —Å–æ —Å–ø–ª–∏—Ç–æ–º (train.csv, val.csv, test.csv)
            preprocessor: –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä landmarks
            max_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            augment: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é
        """
        self.data_dir = Path(data_dir)
        self.preprocessor = preprocessor
        self.max_len = max_len
        self.augment = augment
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self.augment:
            self.augmenter = ASLAugmentations()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–ø–ª–∏—Ç–∞
        self.df = pd.read_csv(self.data_dir / split_file)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞–∫–æ–≤
        with open(self.data_dir / "sign_to_prediction_index_map.json", 'r') as f:
            self.sign_mapping = json.load(f)
        
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω {split_file}: {len(self.df)} –∑–∞–ø–∏—Å–µ–π")
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å"""
        row = self.df.iloc[idx]
        file_path = row['path']
        sign = row['sign']
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∫—É
        label = self.sign_mapping[sign]
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º landmarks
        full_path = self.data_dir / file_path
        landmarks = self.preprocessor.load_landmark_file(str(full_path))
        
        # –û–±—Ä–µ–∑–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        if landmarks.shape[0] > self.max_len:
            landmarks = landmarks[:self.max_len]
        
        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ landmarks –≤ features
        landmarks_batch = landmarks.unsqueeze(0)  # (1, frames, landmarks, 3)
        features = self.preprocessor(landmarks_batch).squeeze(0)  # (frames, features)
        
        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞) - –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ features
        if self.augment:
            features = self._augment_features(features)
        
        return {
            'landmarks': features,  # –¢–µ–ø–µ—Ä—å —ç—Ç–æ features, –∞ –Ω–µ landmarks
            'label': label,
            'sign': sign,
            'file_path': file_path
        }
    
    def _augment_features(self, features: torch.Tensor) -> torch.Tensor:
        """–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è features —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ASLAugmentations"""
        if not self.augment or not hasattr(self, 'augmenter'):
            return features
            
        # –î–æ–±–∞–≤–ª—è–µ–º batch dimension –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π
        features_batch = features.unsqueeze(0)  # (1, frames, features)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        augmented = self.augmenter(features_batch)
        
        # –£–±–∏—Ä–∞–µ–º batch dimension
        return augmented.squeeze(0)

class ASLDataLoader:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Google ASL Signs"""
    
    def __init__(self, 
                 data_dir: str = "../data/google_asl_signs",
                 batch_size: int = 32,
                 max_len: int = 384,
                 num_workers: int = 4,
                 preprocessor: Optional[ASLPreprocessor] = None):
        """
        Args:
            data_dir: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            max_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            num_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            preprocessor: –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
        """
        self.data_dir = Path(data_dir)
        self.batch_size = batch_size
        self.max_len = max_len
        self.num_workers = num_workers
        
        if preprocessor is None:
            self.preprocessor = ASLPreprocessor(max_len=max_len)
        else:
            self.preprocessor = preprocessor
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞–∫–æ–≤
        with open(self.data_dir / "sign_to_prediction_index_map.json", 'r') as f:
            self.sign_mapping = json.load(f)
        
        self.num_classes = len(self.sign_mapping)
        print(f"üéØ –ù–∞—Å—Ç—Ä–æ–µ–Ω –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {self.num_classes}")
        print(f"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {max_len}")
    
    def get_dataloaders(self, 
                       train_ratio: float = 0.8,
                       val_ratio: float = 0.1,
                       test_ratio: float = 0.1,
                       augment_train: bool = True) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """
        –°–æ–∑–¥–∞–µ—Ç DataLoader'—ã –¥–ª—è train/val/test
        
        Returns:
            train_loader, val_loader, test_loader
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ø–ª–∏—Ç–æ–≤
        splits_dir = self.data_dir / "splits"
        
        if not splits_dir.exists():
            print("‚ö†Ô∏è –°–ø–ª–∏—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ...")
            from data_utils import ASLDataAnalyzer
            analyzer = ASLDataAnalyzer(str(self.data_dir))
            analyzer.create_balanced_splits(train_ratio, val_ratio, test_ratio)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
        train_dataset = ASLDataset(
            data_dir=self.data_dir,
            split_file="splits/train.csv",
            preprocessor=self.preprocessor,
            max_len=self.max_len,
            augment=augment_train
        )
        
        val_dataset = ASLDataset(
            data_dir=self.data_dir,
            split_file="splits/val.csv",
            preprocessor=self.preprocessor,
            max_len=self.max_len,
            augment=False
        )
        
        test_dataset = ASLDataset(
            data_dir=self.data_dir,
            split_file="splits/test.csv",
            preprocessor=self.preprocessor,
            max_len=self.max_len,
            augment=False
        )
        
        # –°–æ–∑–¥–∞–µ–º DataLoader'—ã
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
            collate_fn=self._collate_fn
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
            collate_fn=self._collate_fn
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
            collate_fn=self._collate_fn
        )
        
        print(f"üìä –°–æ–∑–¥–∞–Ω—ã DataLoader'—ã:")
        print(f"   Train: {len(train_loader)} –±–∞—Ç—á–µ–π")
        print(f"   Val: {len(val_loader)} –±–∞—Ç—á–µ–π")
        print(f"   Test: {len(test_loader)} –±–∞—Ç—á–µ–π")
        
        return train_loader, val_loader, test_loader
    
    def _collate_fn(self, batch):
        """
        –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –±–∞—Ç—á–∞
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
        """
        features_list = [item['landmarks'] for item in batch]  # –¢–µ–ø–µ—Ä—å —ç—Ç–æ features
        labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)
        
        # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –≤ –±–∞—Ç—á–µ
        max_frames = max(features.shape[0] for features in features_list)
        max_frames = min(max_frames, self.max_len)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä features –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
        feature_dim = features_list[0].shape[1]
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä –¥–ª—è –±–∞—Ç—á–∞
        batch_size = len(features_list)
        batch_tensor = torch.zeros(batch_size, max_frames, feature_dim)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º —Ç–µ–Ω–∑–æ—Ä
        for i, features in enumerate(features_list):
            frames = min(features.shape[0], max_frames)
            batch_tensor[i, :frames] = features[:frames]
        
        return {
            'features': batch_tensor,  # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –≤ features
            'labels': labels,
            'lengths': torch.tensor([min(features.shape[0], max_frames) 
                                   for features in features_list])
        }
    
    def get_class_weights(self, split: str = 'train') -> torch.Tensor:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        split_file = f"splits/{split}.csv"
        df = pd.read_csv(self.data_dir / split_file)
        
        sign_counts = df['sign'].value_counts()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ (–æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —á–∞—Å—Ç–æ—Ç–µ)
        total_samples = len(df)
        class_weights = total_samples / (len(sign_counts) * sign_counts)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º –∫–ª–∞—Å—Å–æ–≤
        class_weights = class_weights.sort_index()
        
        return torch.tensor(class_weights.values, dtype=torch.float32)
    
    def get_sample_batch(self, split: str = 'train') -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–∏–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        split_file = f"splits/{split}.csv"
        dataset = ASLDataset(
            data_dir=self.data_dir,
            split_file=split_file,
            preprocessor=self.preprocessor,
            max_len=self.max_len,
            augment=False
        )
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–±—Ä–∞–∑—Ü–æ–≤
        sample_batch = [dataset[i] for i in range(min(4, len(dataset)))]
        
        return self._collate_fn(sample_batch)

def test_dataloader():
    """–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö...")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫
        dataloader = ASLDataLoader(batch_size=4, max_len=64)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–ª–∏—Ç—ã
        train_loader, val_loader, test_loader = dataloader.get_dataloaders()
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–¥–∏–Ω –±–∞—Ç—á
        sample_batch = next(iter(train_loader))
        
        print(f"‚úÖ –¢–µ—Å—Ç —É—Å–ø–µ—à–µ–Ω!")
        print(f"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {sample_batch['features'].shape}")
        print(f"   –ú–µ—Ç–∫–∏: {sample_batch['labels']}")
        print(f"   –î–ª–∏–Ω—ã: {sample_batch['lengths']}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∑–∞–≥—Ä—É–∑—á–∏–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return False

if __name__ == "__main__":
    test_dataloader() 
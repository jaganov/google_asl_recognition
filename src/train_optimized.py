# train_optimized.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.amp import autocast
from torch.cuda.amp import GradScaler
import numpy as np
from tqdm import tqdm
import time
import os
from pathlib import Path
import json
from datetime import datetime
import platform
import psutil

# ÐŸÐ¾Ð´Ð°Ð²Ð»ÑÐµÐ¼ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Triton Ð½Ð° Windows
if platform.system() == 'Windows':
    try:
        import torch._dynamo
        torch._dynamo.config.suppress_errors = True
        print("ðŸ”§ ÐŸÐ¾Ð´Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Triton Ð´Ð»Ñ Windows")
    except:
        pass

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° TensorFloat32 Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
if torch.cuda.is_available():
    torch.set_float32_matmul_precision('high')
    print("ðŸ”§ Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½ TensorFloat32 Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸")

from data_loader import ASLDataLoader
from preprocessing import ASLPreprocessor
from models import get_model, ASLEnsemble

class OptimizedASLTrainer:
    """ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ‚Ñ€ÐµÐ½ÐµÑ€ Ð´Ð»Ñ ASL Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸"""
    
    def __init__(self, 
                 data_dir: str = "../data/google_asl_signs",
                 model_dir: str = "models",
                 max_len: int = 384,
                 batch_size: int = 16,  # Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð»Ð¸ Ð´Ð»Ñ RTX 4070
                 dim: int = 192,
                 lr: float = 5e-4,
                 epochs: int = 400,
                 device: str = None,
                 use_augmentations: bool = True,
                 use_mixed_precision: bool = True,
                 gradient_clip_val: float = 1.0,
                 gradient_accumulation_steps: int = 3,  # Ð£Ð¼ÐµÐ½ÑŒÑˆÐ¸Ð»Ð¸ Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ
                 num_workers: int = 2,  # Ð£Ð¼ÐµÐ½ÑŒÑˆÐ¸Ð»Ð¸ Ð´Ð»Ñ Windows
                 pin_memory: bool = True):
        
        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        
        self.max_len = max_len
        self.batch_size = batch_size
        self.dim = dim
        self.lr = lr
        self.epochs = epochs
        self.use_augmentations = use_augmentations
        self.use_mixed_precision = use_mixed_precision
        self.gradient_clip_val = gradient_clip_val
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        
        # Device
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # CUDA Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        if self.device.type == 'cuda':
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.cuda.empty_cache()
        
        print(f"ðŸŽ¯ ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ‚Ñ€ÐµÐ½ÐµÑ€ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½:")
        print(f"   Device: {self.device}")
        print(f"   ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ð½Ð°: {max_len}")
        print(f"   Ð Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð°: {batch_size}")
        print(f"   Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ batch size: {batch_size * gradient_accumulation_steps}")
        print(f"   Ð Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {dim}")
        print(f"   Learning rate: {lr}")
        print(f"   Ð­Ð¿Ð¾Ñ…Ð¸: {epochs}")
        print(f"   ÐÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸: {'Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½Ñ‹' if use_augmentations else 'ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ñ‹'}")
        print(f"   Mixed Precision: {'Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½' if use_mixed_precision else 'ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½'}")
        print(f"   Gradient Clipping: {gradient_clip_val}")
        print(f"   Gradient Accumulation: {gradient_accumulation_steps} steps")
        print(f"   Num Workers: {num_workers}")
        print(f"   Pin Memory: {pin_memory}")
        
        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²
        self._setup_components()
        
    def _setup_components(self):
        """ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð² Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸"""
        print("\nðŸ“¦ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²...")
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€ Ñ ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð½Ñ‹Ð¼ ÐºÑÑˆÐµÐ¼
        self.preprocessor = ASLPreprocessor(max_len=self.max_len)
        self.preprocessor._max_cache_size = 2000  # Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÐºÑÑˆ
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸Ðº Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸
        self.dataloader = ASLDataLoader(
            data_dir=str(self.data_dir),
            batch_size=self.batch_size,
            max_len=self.max_len,
            preprocessor=self.preprocessor,
            num_workers=self.num_workers
        )
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ DataLoader'Ñ‹
        print("ðŸ“‚ Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ DataLoader'Ñ‹...")
        self.train_loader, self.val_loader, self.test_loader = self.dataloader.get_dataloaders(
            augment_train=self.use_augmentations
        )
        
        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ñ„Ð¸Ñ‡ Ð¸Ð· Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ Ð±Ð°Ñ‚Ñ‡Ð°
        print("ðŸ” ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
        sample_batch = next(iter(self.train_loader))
        input_dim = sample_batch['features'].shape[-1]
        
        # ÐœÐ¾Ð´ÐµÐ»ÑŒ
        print("ðŸ¤– Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ...")
        self.model = get_model(
            input_dim=input_dim,
            num_classes=self.dataloader.num_classes,
            max_len=self.max_len,
            dim=self.dim
        ).to(self.device)
        
        # PyTorch 2.0+ compile (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Linux/Mac)
        if hasattr(torch, 'compile') and self.device.type == 'cuda' and platform.system() != 'Windows':
            try:
                self.model = torch.compile(self.model, mode='reduce-overhead')
                print("   âœ… PyTorch 2.0+ compile Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½")
            except Exception as e:
                print(f"   âš ï¸ PyTorch compile Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½: {e}")
                if hasattr(self.model, '_orig_mod'):
                    self.model = self.model._orig_mod
        else:
            print("   â„¹ï¸ PyTorch compile Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½ (Windows Ð¸Ð»Ð¸ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½)")
        
        # Loss function
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # Optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # Scheduler
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=self.epochs,
            eta_min=1e-6
        )
        
        # Mixed precision scaler
        if self.use_mixed_precision:
            self.scaler = GradScaler()
        
        # Ð’ÐµÑÐ° ÐºÐ»Ð°ÑÑÐ¾Ð²
        self.class_weights = self.dataloader.get_class_weights('train').to(self.device)
        
        print(f"âœ… ÐšÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ñ‹:")
        print(f"   Ð Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ñ„Ð¸Ñ‡: {input_dim}")
        print(f"   ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ»Ð°ÑÑÐ¾Ð²: {self.dataloader.num_classes}")
        print(f"   ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÑÑˆÐ° Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°
        cache_stats = self.preprocessor.get_cache_stats()
        print(f"   ÐšÑÑˆ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°: {cache_stats['cache_size']}/{cache_stats['max_cache_size']} Ñ„Ð°Ð¹Ð»Ð¾Ð²")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ð°Ð¼ÑÑ‚Ð¸ GPU
        if self.device.type == 'cuda':
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   GPU Ð¿Ð°Ð¼ÑÑ‚ÑŒ: {gpu_memory:.1f} GB")
    
    def train_epoch(self, epoch: int):
        """ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ð´Ð½Ð¾Ð¹ ÑÐ¿Ð¾Ñ…Ð¸ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        # Ð˜Ð·Ð¼ÐµÑ€ÑÐµÐ¼ Ð²Ñ€ÐµÐ¼Ñ ÑÐ¿Ð¾Ñ…Ð¸
        epoch_start_time = time.time()
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{self.epochs}")
        
        for batch_idx, batch in enumerate(pbar):
            # ÐŸÐµÑ€ÐµÐ¼ÐµÑ‰Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð° device
            features = batch['features'].to(self.device, non_blocking=True)
            labels = batch['labels'].to(self.device, non_blocking=True)
            
            # Mixed precision forward pass
            if self.use_mixed_precision and self.device.type == 'cuda':
                with autocast(device_type='cuda', dtype=torch.float16):
                    outputs = self.model(features)
                    loss = self.criterion(outputs, labels)
                    loss = loss / self.gradient_accumulation_steps
            else:
                outputs = self.model(features)
                loss = self.criterion(outputs, labels)
                loss = loss / self.gradient_accumulation_steps
            
            # Mixed precision backward pass
            if self.use_mixed_precision and hasattr(self, 'scaler'):
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # Gradient accumulation
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                # Gradient clipping
                if self.use_mixed_precision and hasattr(self, 'scaler'):
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                    self.optimizer.step()
                
                self.optimizer.zero_grad()
            
            # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
            total_loss += loss.item() * self.gradient_accumulation_steps
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ progress bar Ñ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÐµÐ¹
            epoch_time = time.time() - epoch_start_time
            batches_per_sec = (batch_idx + 1) / epoch_time if epoch_time > 0 else 0
            
            pbar.set_postfix({
                'Loss': f'{loss.item() * self.gradient_accumulation_steps:.4f}',
                'Acc': f'{100.*correct/total:.2f}%',
                'LR': f'{self.scheduler.get_last_lr()[0]:.6f}',
                'Speed': f'{batches_per_sec:.1f} batch/s'
            })
            
            # ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð¿Ð°Ð¼ÑÑ‚Ð¸ ÐºÐ°Ð¶Ð´Ñ‹Ðµ 20 Ð±Ð°Ñ‚Ñ‡ÐµÐ¹ (ÑƒÐ¼ÐµÐ½ÑŒÑˆÐ¸Ð»Ð¸ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñƒ)
            if batch_idx % 20 == 0 and self.device.type == 'cuda':
                torch.cuda.empty_cache()
        
        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ scheduler
        self.scheduler.step()
        
        epoch_time = time.time() - epoch_start_time
        print(f"   â±ï¸ Ð’Ñ€ÐµÐ¼Ñ ÑÐ¿Ð¾Ñ…Ð¸: {epoch_time:.1f} ÑÐµÐº")
        
        return total_loss / len(self.train_loader), 100. * correct / total
    
    def validate(self, epoch: int):
        """Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        val_start_time = time.time()
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                # ÐŸÐµÑ€ÐµÐ¼ÐµÑ‰Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð° device
                features = batch['features'].to(self.device, non_blocking=True)
                labels = batch['labels'].to(self.device, non_blocking=True)
                
                # Forward pass
                if self.use_mixed_precision:
                    with autocast('cuda'):
                        outputs = self.model(features)
                        loss = self.criterion(outputs, labels)
                else:
                    outputs = self.model(features)
                    loss = self.criterion(outputs, labels)
                
                # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        val_time = time.time() - val_start_time
        print(f"   â±ï¸ Ð’Ñ€ÐµÐ¼Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸: {val_time:.1f} ÑÐµÐº")
        
        return total_loss / len(self.val_loader), 100. * correct / total
    
    def save_checkpoint(self, epoch: int, val_acc: float, is_best: bool = False):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚Ð°"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_acc': val_acc,
            'config': {
                'max_len': self.max_len,
                'batch_size': self.batch_size,
                'dim': self.dim,
                'lr': self.lr,
                'use_augmentations': self.use_augmentations,
                'use_mixed_precision': self.use_mixed_precision,
                'gradient_clip_val': self.gradient_clip_val,
                'gradient_accumulation_steps': self.gradient_accumulation_steps
            }
        }
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ scaler Ð´Ð»Ñ mixed precision
        if self.use_mixed_precision:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚
        checkpoint_path = self.model_dir / f"checkpoint_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚
        if is_best:
            best_path = self.model_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚: {best_path}")
    
    def train(self):
        """ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ†Ð¸ÐºÐ» Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸"""
        print(f"ðŸš€ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ...")
        print(f"   Ð’Ñ€ÐµÐ¼Ñ Ð½Ð°Ñ‡Ð°Ð»Ð°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        best_val_acc = 0
        train_losses = []
        train_accs = []
        val_losses = []
        val_accs = []
        
        total_start_time = time.time()
        
        for epoch in range(self.epochs):
            epoch_start_time = time.time()
            
            # ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
            train_loss, train_acc = self.train_epoch(epoch)
            train_losses.append(train_loss)
            train_accs.append(train_acc)
            
            # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ
            val_loss, val_acc = self.validate(epoch)
            val_losses.append(val_loss)
            val_accs.append(val_acc)
            
            epoch_time = time.time() - epoch_start_time
            
            # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
            print(f"Epoch {epoch+1}/{self.epochs} ({epoch_time:.1f} ÑÐµÐº):")
            print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            print(f"  LR: {self.scheduler.get_last_lr()[0]:.6f}")
            
            # Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸ GPU
            if self.device.type == 'cuda':
                gpu_memory_used = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_cached = torch.cuda.memory_reserved() / 1024**3
                print(f"  GPU Memory: {gpu_memory_used:.1f}GB used, {gpu_memory_cached:.1f}GB cached")
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚Ð°
            is_best = val_acc > best_val_acc
            if is_best:
                best_val_acc = val_acc
                print(f"ðŸŽ‰ ÐÐ¾Ð²Ñ‹Ð¹ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: {val_acc:.2f}%")
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ðµ 25 ÑÐ¿Ð¾Ñ… Ð¸Ð»Ð¸ ÐµÑÐ»Ð¸ ÑÑ‚Ð¾ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
            if (epoch + 1) % 25 == 0 or is_best:
                self.save_checkpoint(epoch, val_acc, is_best)
            
            # Ð Ð°Ð½Ð½ÑÑ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° (ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ð¹ 30 ÑÐ¿Ð¾Ñ…)
            if epoch > 30 and max(val_accs[-30:]) < best_val_acc:
                print(f"â¹ï¸ Ð Ð°Ð½Ð½ÑÑ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð½Ð° ÑÐ¿Ð¾Ñ…Ðµ {epoch+1}")
                break
        
        total_time = time.time() - total_start_time
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
        self.save_checkpoint(self.epochs-1, val_acc)
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
        history = {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'val_losses': val_losses,
            'val_accs': val_accs,
            'best_val_acc': best_val_acc,
            'total_training_time': total_time
        }
        
        history_path = self.model_dir / "training_history.json"
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2)
        
        print(f"âœ… ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾!")
        print(f"   Ð›ÑƒÑ‡ÑˆÐ°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ: {best_val_acc:.2f}%")
        print(f"   ÐžÐ±Ñ‰ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ: {total_time/3600:.1f} Ñ‡Ð°ÑÐ¾Ð²")
        print(f"   Ð’Ñ€ÐµÐ¼Ñ Ð¾ÐºÐ¾Ð½Ñ‡Ð°Ð½Ð¸Ñ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÑÑˆÐ° Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°
        cache_stats = self.preprocessor.get_cache_stats()
        print(f"ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÑÑˆÐ° Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°:")
        print(f"   Hit rate: {cache_stats['hit_rate']:.1%}")
        print(f"   Cache hits: {cache_stats['cache_hits']:,}")
        print(f"   Cache misses: {cache_stats['cache_misses']:,}")
        print(f"   Ð¤Ð°Ð¹Ð»Ð¾Ð² Ð² ÐºÑÑˆÐµ: {cache_stats['cache_size']}/{cache_stats['max_cache_size']}")
        
        return history

def main():
    """ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸"""
    print("ðŸ¤Ÿ ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—Ð˜Ð ÐžÐ’ÐÐÐÐžÐ• ÐžÐ‘Ð£Ð§Ð•ÐÐ˜Ð• ASL ÐœÐžÐ”Ð•Ð›Ð˜")
    print("=" * 70)
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ðµ Ñ€ÐµÑÑƒÑ€ÑÑ‹
    print("ðŸ” ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ñ… Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð²:")
    cpu_count = psutil.cpu_count()
    memory = psutil.virtual_memory()
    print(f"   CPU: {cpu_count} ÑÐ´ÐµÑ€")
    print(f"   RAM: {memory.total / 1024**3:.1f} GB")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   GPU: {gpu_name}")
        print(f"   GPU Memory: {gpu_memory:.1f} GB")
    
    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ‚Ñ€ÐµÐ½ÐµÑ€
    trainer = OptimizedASLTrainer(
        data_dir="../data/google_asl_signs",
        model_dir="models",
        max_len=384,
        batch_size=16,  # Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð»Ð¸ Ð´Ð»Ñ RTX 4070
        dim=192,
        lr=5e-4,
        epochs=400,
        use_augmentations=True,
        use_mixed_precision=True,
        gradient_clip_val=1.0,
        gradient_accumulation_steps=3,  # Ð£Ð¼ÐµÐ½ÑŒÑˆÐ¸Ð»Ð¸ Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ
        num_workers=2,  # Ð£Ð¼ÐµÐ½ÑŒÑˆÐ¸Ð»Ð¸ Ð´Ð»Ñ Windows
        pin_memory=True
    )
    
    # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
    history = trainer.train()
    
    print("ðŸŽ‰ ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾!")

if __name__ == "__main__":
    main() 
# train_optimized.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.amp import autocast
from torch.cuda.amp import GradScaler
import numpy as np
from tqdm import tqdm
import time
import os
from pathlib import Path
import json
from datetime import datetime
import platform
import psutil

# –ü–æ–¥–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏ Triton –Ω–∞ Windows
if platform.system() == 'Windows':
    try:
        import torch._dynamo
        torch._dynamo.config.suppress_errors = True
        print("üîß –ü–æ–¥–∞–≤–ª–µ–Ω—ã –æ—à–∏–±–∫–∏ Triton –¥–ª—è Windows")
    except:
        pass

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ TensorFloat32 –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
if torch.cuda.is_available():
    torch.set_float32_matmul_precision('high')
    print("üîß –í–∫–ª—é—á–µ–Ω TensorFloat32 –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")

from data_loader import ASLDataLoader
from preprocessing import ASLPreprocessor
from models import get_model, ASLEnsemble

class OptimizedASLTrainer:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è ASL –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self, 
                 data_dir: str = "../data/google_asl_signs",
                 model_dir: str = "models",
                 max_len: int = 384,
                 batch_size: int = 16,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª—è RTX 4070
                 dim: int = 192,
                 lr: float = 5e-4,
                 epochs: int = 400,
                 device: str = None,
                 use_augmentations: bool = True,
                 use_mixed_precision: bool = True,
                 gradient_clip_val: float = 1.0,
                 gradient_accumulation_steps: int = 3,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                 num_workers: int = 2,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è Windows
                 pin_memory: bool = True):
        
        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        
        self.max_len = max_len
        self.batch_size = batch_size
        self.dim = dim
        self.lr = lr
        self.epochs = epochs
        self.use_augmentations = use_augmentations
        self.use_mixed_precision = use_mixed_precision
        self.gradient_clip_val = gradient_clip_val
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        
        # Device
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if self.device.type == 'cuda':
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.cuda.empty_cache()
        
        print(f"üéØ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω:")
        print(f"   Device: {self.device}")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {max_len}")
        print(f"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
        print(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size: {batch_size * gradient_accumulation_steps}")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {dim}")
        print(f"   Learning rate: {lr}")
        print(f"   –≠–ø–æ—Ö–∏: {epochs}")
        print(f"   –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {'–í–∫–ª—é—á–µ–Ω—ã' if use_augmentations else '–û—Ç–∫–ª—é—á–µ–Ω—ã'}")
        print(f"   Mixed Precision: {'–í–∫–ª—é—á–µ–Ω' if use_mixed_precision else '–û—Ç–∫–ª—é—á–µ–Ω'}")
        print(f"   Gradient Clipping: {gradient_clip_val}")
        print(f"   Gradient Accumulation: {gradient_accumulation_steps} steps")
        print(f"   Num Workers: {num_workers}")
        print(f"   Pin Memory: {pin_memory}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._setup_components()
        
    def _setup_components(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        print("\nüì¶ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –∫—ç—à–µ–º
        self.preprocessor = ASLPreprocessor(max_len=self.max_len)
        self.preprocessor._max_cache_size = 2000  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫—ç—à
        
        # –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
        self.dataloader = ASLDataLoader(
            data_dir=str(self.data_dir),
            batch_size=self.batch_size,
            max_len=self.max_len,
            preprocessor=self.preprocessor,
            num_workers=self.num_workers
        )
        
        # –ü–æ–ª—É—á–∞–µ–º DataLoader'—ã
        print("üìÇ –°–æ–∑–¥–∞–µ–º DataLoader'—ã...")
        self.train_loader, self.val_loader, self.test_loader = self.dataloader.get_dataloaders(
            augment_train=self.use_augmentations
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∏—á –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞
        print("üîç –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        sample_batch = next(iter(self.train_loader))
        input_dim = sample_batch['features'].shape[-1]
        
        # –ú–æ–¥–µ–ª—å
        print("ü§ñ –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å...")
        self.model = get_model(
            input_dim=input_dim,
            num_classes=self.dataloader.num_classes,
            max_len=self.max_len,
            dim=self.dim
        ).to(self.device)
        
        # PyTorch 2.0+ compile (—Ç–æ–ª—å–∫–æ –¥–ª—è Linux/Mac)
        if hasattr(torch, 'compile') and self.device.type == 'cuda' and platform.system() != 'Windows':
            try:
                self.model = torch.compile(self.model, mode='reduce-overhead')
                print("   ‚úÖ PyTorch 2.0+ compile –≤–∫–ª—é—á–µ–Ω")
            except Exception as e:
                print(f"   ‚ö†Ô∏è PyTorch compile –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                if hasattr(self.model, '_orig_mod'):
                    self.model = self.model._orig_mod
        else:
            print("   ‚ÑπÔ∏è PyTorch compile –æ—Ç–∫–ª—é—á–µ–Ω (Windows –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)")
        
        # Loss function
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # Optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # Scheduler
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=self.epochs,
            eta_min=1e-6
        )
        
        # Mixed precision scaler
        if self.use_mixed_precision:
            self.scaler = GradScaler()
        
        # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        self.class_weights = self.dataloader.get_class_weights('train').to(self.device)
        
        print(f"‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã:")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∏—á: {input_dim}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {self.dataloader.num_classes}")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        cache_stats = self.preprocessor.get_cache_stats()
        print(f"   –ö—ç—à –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞: {cache_stats['cache_size']}/{cache_stats['max_cache_size']} —Ñ–∞–π–ª–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ GPU
        if self.device.type == 'cuda':
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   GPU –ø–∞–º—è—Ç—å: {gpu_memory:.1f} GB")
    
    def train_epoch(self, epoch: int):
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è —ç–ø–æ—Ö–∏
        epoch_start_time = time.time()
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{self.epochs}")
        
        for batch_idx, batch in enumerate(pbar):
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ device
            features = batch['features'].to(self.device, non_blocking=True)
            labels = batch['labels'].to(self.device, non_blocking=True)
            
            # Mixed precision forward pass
            if self.use_mixed_precision and self.device.type == 'cuda':
                with autocast(device_type='cuda', dtype=torch.float16):
                    outputs = self.model(features)
                    loss = self.criterion(outputs, labels)
                    loss = loss / self.gradient_accumulation_steps
            else:
                outputs = self.model(features)
                loss = self.criterion(outputs, labels)
                loss = loss / self.gradient_accumulation_steps
            
            # Mixed precision backward pass
            if self.use_mixed_precision and hasattr(self, 'scaler'):
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # Gradient accumulation
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                # Gradient clipping
                if self.use_mixed_precision and hasattr(self, 'scaler'):
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                    self.optimizer.step()
                
                self.optimizer.zero_grad()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_loss += loss.item() * self.gradient_accumulation_steps
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º progress bar —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            epoch_time = time.time() - epoch_start_time
            batches_per_sec = (batch_idx + 1) / epoch_time if epoch_time > 0 else 0
            
            pbar.set_postfix({
                'Loss': f'{loss.item() * self.gradient_accumulation_steps:.4f}',
                'Acc': f'{100.*correct/total:.2f}%',
                'LR': f'{self.scheduler.get_last_lr()[0]:.6f}',
                'Speed': f'{batches_per_sec:.1f} batch/s'
            })
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 20 –±–∞—Ç—á–µ–π (—É–º–µ–Ω—å—à–∏–ª–∏ —á–∞—Å—Ç–æ—Ç—É)
            if batch_idx % 20 == 0 and self.device.type == 'cuda':
                torch.cuda.empty_cache()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º scheduler
        self.scheduler.step()
        
        epoch_time = time.time() - epoch_start_time
        print(f"   ‚è±Ô∏è –í—Ä–µ–º—è —ç–ø–æ—Ö–∏: {epoch_time:.1f} —Å–µ–∫")
        
        return total_loss / len(self.train_loader), 100. * correct / total
    
    def validate(self, epoch: int):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        val_start_time = time.time()
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ device
                features = batch['features'].to(self.device, non_blocking=True)
                labels = batch['labels'].to(self.device, non_blocking=True)
                
                # Forward pass
                if self.use_mixed_precision:
                    with autocast('cuda'):
                        outputs = self.model(features)
                        loss = self.criterion(outputs, labels)
                else:
                    outputs = self.model(features)
                    loss = self.criterion(outputs, labels)
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        val_time = time.time() - val_start_time
        print(f"   ‚è±Ô∏è –í—Ä–µ–º—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {val_time:.1f} —Å–µ–∫")
        
        return total_loss / len(self.val_loader), 100. * correct / total
    
    def save_checkpoint(self, epoch: int, val_acc: float, is_best: bool = False):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_acc': val_acc,
            'config': {
                'max_len': self.max_len,
                'batch_size': self.batch_size,
                'dim': self.dim,
                'lr': self.lr,
                'use_augmentations': self.use_augmentations,
                'use_mixed_precision': self.use_mixed_precision,
                'gradient_clip_val': self.gradient_clip_val,
                'gradient_accumulation_steps': self.gradient_accumulation_steps
            }
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler –¥–ª—è mixed precision
        if self.use_mixed_precision:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        checkpoint_path = self.model_dir / f"checkpoint_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        if is_best:
            best_path = self.model_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç: {best_path}")
    
    def train(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")
        print(f"   –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        best_val_acc = 0
        train_losses = []
        train_accs = []
        val_losses = []
        val_accs = []
        
        total_start_time = time.time()
        
        for epoch in range(self.epochs):
            epoch_start_time = time.time()
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_acc = self.train_epoch(epoch)
            train_losses.append(train_loss)
            train_accs.append(train_acc)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_acc = self.validate(epoch)
            val_losses.append(val_loss)
            val_accs.append(val_acc)
            
            epoch_time = time.time() - epoch_start_time
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            print(f"Epoch {epoch+1}/{self.epochs} ({epoch_time:.1f} —Å–µ–∫):")
            print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            print(f"  LR: {self.scheduler.get_last_lr()[0]:.6f}")
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏ GPU
            if self.device.type == 'cuda':
                gpu_memory_used = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_cached = torch.cuda.memory_reserved() / 1024**3
                print(f"  GPU Memory: {gpu_memory_used:.1f}GB used, {gpu_memory_cached:.1f}GB cached")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
            is_best = val_acc > best_val_acc
            if is_best:
                best_val_acc = val_acc
                print(f"üéâ –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {val_acc:.2f}%")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 25 —ç–ø–æ—Ö –∏–ª–∏ –µ—Å–ª–∏ —ç—Ç–æ –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if (epoch + 1) % 25 == 0 or is_best:
                self.save_checkpoint(epoch, val_acc, is_best)
            
            # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ (–µ—Å–ª–∏ –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π 30 —ç–ø–æ—Ö)
            if epoch > 30 and max(val_accs[-30:]) < best_val_acc:
                print(f"‚èπÔ∏è –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                break
        
        total_time = time.time() - total_start_time
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        self.save_checkpoint(self.epochs-1, val_acc)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
        history = {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'val_losses': val_losses,
            'val_accs': val_accs,
            'best_val_acc': best_val_acc,
            'total_training_time': total_time
        }
        
        history_path = self.model_dir / "training_history.json"
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2)
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"   –õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.2f}%")
        print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_time/3600:.1f} —á–∞—Å–æ–≤")
        print(f"   –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        cache_stats = self.preprocessor.get_cache_stats()
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞:")
        print(f"   Hit rate: {cache_stats['hit_rate']:.1%}")
        print(f"   Cache hits: {cache_stats['cache_hits']:,}")
        print(f"   Cache misses: {cache_stats['cache_misses']:,}")
        print(f"   –§–∞–π–ª–æ–≤ –≤ –∫—ç—à–µ: {cache_stats['cache_size']}/{cache_stats['max_cache_size']}")
        
        return history

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
    print("ü§ü –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï ASL –ú–û–î–ï–õ–ò")
    print("=" * 70)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤:")
    cpu_count = psutil.cpu_count()
    memory = psutil.virtual_memory()
    print(f"   CPU: {cpu_count} —è–¥–µ—Ä")
    print(f"   RAM: {memory.total / 1024**3:.1f} GB")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   GPU: {gpu_name}")
        print(f"   GPU Memory: {gpu_memory:.1f} GB")
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä
    trainer = OptimizedASLTrainer(
        data_dir="../data/google_asl_signs",
        model_dir="models",
        max_len=384,
        batch_size=16,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª—è RTX 4070
        dim=192,
        lr=5e-4,
        epochs=400,
        use_augmentations=True,
        use_mixed_precision=True,
        gradient_clip_val=1.0,
        gradient_accumulation_steps=3,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        num_workers=2,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è Windows
        pin_memory=True
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    history = trainer.train()
    
    print("üéâ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")

if __name__ == "__main__":
    main() 
# data_utils.py
import pandas as pd
import numpy as np
import torch
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

class ASLDataAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö Google ASL Signs"""
    
    def __init__(self, data_dir: str = "../data/google_asl_signs"):
        self.data_dir = Path(data_dir)
        self.train_df = None
        self.sign_mapping = None
        self.load_data()
    
    def load_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º train.csv
        self.train_df = pd.read_csv(self.data_dir / "train.csv")
        print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.train_df)} –∑–∞–ø–∏—Å–µ–π")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞–∫–æ–≤
        with open(self.data_dir / "sign_to_prediction_index_map.json", 'r') as f:
            self.sign_mapping = json.load(f)
        print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.sign_mapping)} –∑–Ω–∞–∫–æ–≤")
    
    def analyze_dataset_statistics(self):
        """–ê–Ω–∞–ª–∏–∑ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        print("\nüìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–¢–ê–°–ï–¢–ê")
        print("=" * 50)
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(self.train_df):,}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤: {self.train_df['sign'].nunique()}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {self.train_df['participant_id'].nunique()}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {self.train_df['sequence_id'].nunique()}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–Ω–∞–∫–æ–≤
        sign_counts = self.train_df['sign'].value_counts()
        print(f"\nüéØ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–Ω–∞–∫–æ–≤:")
        print(f"   –°–∞–º—ã–π —á–∞—Å—Ç—ã–π –∑–Ω–∞–∫: {sign_counts.index[0]} ({sign_counts.iloc[0]} –∑–∞–ø–∏—Å–µ–π)")
        print(f"   –°–∞–º—ã–π —Ä–µ–¥–∫–∏–π –∑–Ω–∞–∫: {sign_counts.index[-1]} ({sign_counts.iloc[-1]} –∑–∞–ø–∏—Å–µ–π)")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –Ω–∞ –∑–Ω–∞–∫: {sign_counts.mean():.1f}")
        print(f"   –ú–µ–¥–∏–∞–Ω–∞ –∑–∞–ø–∏—Å–µ–π –Ω–∞ –∑–Ω–∞–∫: {sign_counts.median():.1f}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
        participant_counts = self.train_df['participant_id'].value_counts()
        print(f"\nüë• –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤:")
        print(f"   –°–∞–º—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π —É—á–∞—Å—Ç–Ω–∏–∫: {participant_counts.index[0]} ({participant_counts.iloc[0]} –∑–∞–ø–∏—Å–µ–π)")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞: {participant_counts.mean():.1f}")
        print(f"   –ú–µ–¥–∏–∞–Ω–∞ –∑–∞–ø–∏—Å–µ–π –Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞: {participant_counts.median():.1f}")
        
        return {
            'sign_counts': sign_counts,
            'participant_counts': participant_counts
        }
    
    def analyze_landmark_files(self, sample_size: int = 100):
        """–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ —Å landmarks"""
        print(f"\nüéØ –ê–Ω–∞–ª–∏–∑ landmark —Ñ–∞–π–ª–æ–≤ (–≤—ã–±–æ—Ä–∫–∞ –∏–∑ {sample_size} —Ñ–∞–π–ª–æ–≤)...")
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        sample_df = self.train_df.sample(n=min(sample_size, len(self.train_df)), random_state=42)
        
        frame_counts = []
        landmark_counts = []
        file_sizes = []
        missing_data_ratios = []
        landmark_types = []
        
        for idx, row in sample_df.iterrows():
            file_path = self.data_dir / row['path']
            
            if file_path.exists():
                # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                file_size = file_path.stat().st_size
                file_sizes.append(file_size)
                
                try:
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º parquet —Ñ–∞–π–ª
                    landmarks_df = pd.read_parquet(file_path)
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                    frame_counts.append(len(landmarks_df['frame'].unique()))
                    
                    # –¢–∏–ø—ã landmarks
                    types = landmarks_df['type'].unique()
                    landmark_types.extend(types)
                    
                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö landmarks
                    unique_landmarks = landmarks_df['landmark_index'].nunique()
                    landmark_counts.append(unique_landmarks)
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    missing_ratio = landmarks_df.isnull().sum().sum() / landmarks_df.size
                    missing_data_ratios.append(missing_ratio)
                    
                except Exception as e:
                    print(f"   –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {file_path}: {e}")
                    continue
        
        if frame_counts:
            print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–¥—Ä–æ–≤:")
            print(f"     –ú–∏–Ω–∏–º—É–º: {min(frame_counts)} –∫–∞–¥—Ä–æ–≤")
            print(f"     –ú–∞–∫—Å–∏–º—É–º: {max(frame_counts)} –∫–∞–¥—Ä–æ–≤")
            print(f"     –°—Ä–µ–¥–Ω–µ–µ: {np.mean(frame_counts):.1f} –∫–∞–¥—Ä–æ–≤")
            print(f"     –ú–µ–¥–∏–∞–Ω–∞: {np.median(frame_counts):.1f} –∫–∞–¥—Ä–æ–≤")
        
        if landmark_counts:
            print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ landmarks:")
            print(f"     –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫: {np.mean(landmark_counts):.1f}")
            print(f"     –ú–∏–Ω–∏–º—É–º —Ç–æ—á–µ–∫: {min(landmark_counts)}")
            print(f"     –ú–∞–∫—Å–∏–º—É–º —Ç–æ—á–µ–∫: {max(landmark_counts)}")
        
        if file_sizes:
            print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤:")
            print(f"     –ú–∏–Ω–∏–º—É–º: {min(file_sizes)} –±–∞–π—Ç")
            print(f"     –ú–∞–∫—Å–∏–º—É–º: {max(file_sizes)} –±–∞–π—Ç")
            print(f"     –°—Ä–µ–¥–Ω–µ–µ: {np.mean(file_sizes):.1f} –±–∞–π—Ç")
            print(f"     –ú–µ–¥–∏–∞–Ω–∞: {np.median(file_sizes):.1f} –±–∞–π—Ç")
        
        if missing_data_ratios:
            print(f"   –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:")
            print(f"     –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {np.mean(missing_data_ratios):.3%}")
            print(f"     –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {max(missing_data_ratios):.3%}")
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ landmarks
        type_counter = Counter(landmark_types)
        print(f"   –¢–∏–ø—ã landmarks:")
        for landmark_type, count in type_counter.most_common():
            print(f"     {landmark_type}: {count} —Ñ–∞–π–ª–æ–≤")
        
        return {
            'frame_counts': frame_counts,
            'landmark_counts': landmark_counts,
            'file_sizes': file_sizes,
            'missing_data_ratios': missing_data_ratios,
            'landmark_types': type_counter
        }
    
    def create_balanced_splits(self, 
                              train_ratio: float = 0.8,
                              val_ratio: float = 0.1,
                              test_ratio: float = 0.1,
                              min_samples_per_class: int = 5):
        """–°–æ–∑–¥–∞–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–ª–∏—Ç—ã train/val/test"""
        print(f"\n‚úÇÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ø–ª–∏—Ç–æ–≤...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "–°—É–º–º–∞ –¥–æ–ª–µ–π –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 1.0"
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–ª–∞—Å—Å—ã —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ–±—Ä–∞–∑—Ü–æ–≤
        sign_counts = self.train_df['sign'].value_counts()
        valid_signs = sign_counts[sign_counts >= min_samples_per_class].index
        
        print(f"   –ó–Ω–∞–∫–æ–≤ —Å >= {min_samples_per_class} –æ–±—Ä–∞–∑—Ü–∞–º–∏: {len(valid_signs)}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        filtered_df = self.train_df[self.train_df['sign'].isin(valid_signs)].copy()
        print(f"   –ó–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered_df)}")
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–ª–∏—Ç—ã
        splits = {}
        
        for sign in valid_signs:
            sign_data = filtered_df[filtered_df['sign'] == sign]
            n_samples = len(sign_data)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã —Å–ø–ª–∏—Ç–æ–≤
            n_train = int(n_samples * train_ratio)
            n_val = int(n_samples * val_ratio)
            n_test = n_samples - n_train - n_val
            
            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
            indices = sign_data.index.tolist()
            np.random.shuffle(indices)
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Å–ø–ª–∏—Ç—ã
            train_indices = indices[:n_train]
            val_indices = indices[n_train:n_train + n_val]
            test_indices = indices[n_train + n_val:]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å—ã
            if 'train' not in splits:
                splits['train'] = []
                splits['val'] = []
                splits['test'] = []
            
            splits['train'].extend(train_indices)
            splits['val'].extend(val_indices)
            splits['test'].extend(test_indices)
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ø–ª–∏—Ç–∞
        train_df = filtered_df.loc[splits['train']].reset_index(drop=True)
        val_df = filtered_df.loc[splits['val']].reset_index(drop=True)
        test_df = filtered_df.loc[splits['test']].reset_index(drop=True)
        
        print(f"   –†–∞–∑–º–µ—Ä—ã —Å–ø–ª–∏—Ç–æ–≤:")
        print(f"     Train: {len(train_df)} –∑–∞–ø–∏—Å–µ–π")
        print(f"     Val: {len(val_df)} –∑–∞–ø–∏—Å–µ–π")
        print(f"     Test: {len(test_df)} –∑–∞–ø–∏—Å–µ–π")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–ª–∏—Ç—ã
        splits_dir = self.data_dir / "splits"
        splits_dir.mkdir(exist_ok=True)
        
        train_df.to_csv(splits_dir / "train.csv", index=False)
        val_df.to_csv(splits_dir / "val.csv", index=False)
        test_df.to_csv(splits_dir / "test.csv", index=False)
        
        print(f"   –°–ø–ª–∏—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {splits_dir}")
        
        return train_df, val_df, test_df
    
    def get_class_weights(self, split_df: pd.DataFrame) -> torch.Tensor:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        sign_counts = split_df['sign'].value_counts()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ (–æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —á–∞—Å—Ç–æ—Ç–µ)
        total_samples = len(split_df)
        class_weights = total_samples / (len(sign_counts) * sign_counts)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º –∫–ª–∞—Å—Å–æ–≤
        class_weights = class_weights.sort_index()
        
        return torch.tensor(class_weights.values, dtype=torch.float32)
    
    def visualize_data_distribution(self, save_dir: str = "exploration_output"):
        """–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
        print(f"\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
        
        save_dir = Path(save_dir)
        save_dir.mkdir(exist_ok=True)
        
        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤
        sign_counts = self.train_df['sign'].value_counts()
        
        plt.figure(figsize=(15, 10))
        
        plt.subplot(2, 2, 1)
        sign_counts.head(20).plot(kind='bar')
        plt.title('–¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∑–Ω–∞–∫–æ–≤')
        plt.xlabel('–ó–Ω–∞–∫')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')
        plt.xticks(rotation=45)
        
        plt.subplot(2, 2, 2)
        plt.hist(sign_counts.values, bins=50, alpha=0.7, edgecolor='black')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –∑–Ω–∞–∫–æ–≤')
        plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –Ω–∞ –∑–Ω–∞–∫')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞–∫–æ–≤')
        
        plt.subplot(2, 2, 3)
        participant_counts = self.train_df['participant_id'].value_counts()
        plt.hist(participant_counts.values, bins=50, alpha=0.7, edgecolor='black')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞')
        plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤')
        
        plt.subplot(2, 2, 4)
        plt.boxplot([sign_counts.values, participant_counts.values], 
                   labels=['–ó–Ω–∞–∫–∏', '–£—á–∞—Å—Ç–Ω–∏–∫–∏'])
        plt.title('Box plot —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')
        
        plt.tight_layout()
        plt.savefig(save_dir / 'data_distribution.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: data_distribution.png")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
    print("üîç –ê–ù–ê–õ–ò–ó GOOGLE ASL SIGNS –î–ê–¢–ê–°–ï–¢–ê")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = ASLDataAnalyzer()
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    stats = analyzer.analyze_dataset_statistics()
    landmark_stats = analyzer.analyze_landmark_files(sample_size=200)
    
    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    analyzer.visualize_data_distribution()
    
    # –°–æ–∑–¥–∞–µ–º —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–ª–∏—Ç—ã
    train_df, val_df, test_df = analyzer.create_balanced_splits()
    
    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main() 
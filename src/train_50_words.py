# train_50_words.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.amp import autocast
from torch.cuda.amp import GradScaler
import numpy as np
from tqdm import tqdm
import time
import os
from pathlib import Path
import json
from datetime import datetime
import platform
import psutil
import pandas as pd
from typing import Optional, Tuple
from torch.utils.data import DataLoader

# ÐŸÐ¾Ð´Ð°Ð²Ð»ÑÐµÐ¼ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Triton Ð½Ð° Windows
if platform.system() == 'Windows':
    try:
        import torch._dynamo
        torch._dynamo.config.suppress_errors = True
        print("ðŸ”§ ÐŸÐ¾Ð´Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Triton Ð´Ð»Ñ Windows")
    except:
        pass

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° TensorFloat32 Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
if torch.cuda.is_available():
    torch.set_float32_matmul_precision('high')
    print("ðŸ”§ Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½ TensorFloat32 Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸")

from data_loader import ASLDataLoader
from preprocessing import ASLPreprocessor
from models import get_model, ASLEnsemble

class ASLDataset50Words:
    """Dataset Ð´Ð»Ñ Google ASL Signs Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð¾ 50 ÑÐ»Ð¾Ð²"""
    
    def __init__(self, 
                 data_dir: str,
                 split_file: str,
                 preprocessor: ASLPreprocessor,
                 max_len: int = 384,
                 augment: bool = False,
                 num_words: int = 50):
        """
        Args:
            data_dir: ÐŸÑƒÑ‚ÑŒ Ðº Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸ Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸
            split_file: Ð¤Ð°Ð¹Ð» ÑÐ¾ ÑÐ¿Ð»Ð¸Ñ‚Ð¾Ð¼ (train.csv, val.csv, test.csv)
            preprocessor: ÐŸÑ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€ landmarks
            max_len: ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ð½Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
            augment: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð»Ð¸ Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÑŽ
            num_words: ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ»Ð¾Ð² Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ 50)
        """
        self.data_dir = Path(data_dir)
        self.preprocessor = preprocessor
        self.max_len = max_len
        self.augment = augment
        self.num_words = num_words
        
        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
        if self.augment:
            from augmentations import ASLAugmentations
            self.augmenter = ASLAugmentations()
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¿Ð»Ð¸Ñ‚Ð°
        self.df = pd.read_csv(self.data_dir / split_file)
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³ Ð·Ð½Ð°ÐºÐ¾Ð²
        with open(self.data_dir / "sign_to_prediction_index_map.json", 'r') as f:
            self.sign_mapping = json.load(f)
        
        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿ÐµÑ€Ð²Ñ‹Ðµ num_words Ð·Ð½Ð°ÐºÐ¾Ð²
        all_signs = list(self.sign_mapping.keys())
        selected_signs = all_signs[:num_words]
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… Ð·Ð½Ð°ÐºÐ¾Ð²
        self.filtered_sign_mapping = {sign: idx for idx, sign in enumerate(selected_signs)}
        
        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… Ð·Ð½Ð°ÐºÐ¾Ð²
        self.df = self.df[self.df['sign'].isin(selected_signs)].copy()
        
        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¼ÐµÑ‚ÐºÐ¸ Ð² ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ð¸ Ñ Ð½Ð¾Ð²Ñ‹Ð¼ Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³Ð¾Ð¼
        self.df['label'] = self.df['sign'].map(self.filtered_sign_mapping)
        
        print(f"ðŸ“Š Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ {split_file} (Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½Ð¾ Ð´Ð¾ {num_words} ÑÐ»Ð¾Ð²): {len(self.df)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
        print(f"ðŸŽ¯ Ð’Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð½Ð°ÐºÐ¸: {', '.join(selected_signs[:10])}{'...' if len(selected_signs) > 10 else ''}")
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð¾Ð´Ð½Ñƒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ"""
        row = self.df.iloc[idx]
        file_path = row['path']
        sign = row['sign']
        label = row['label']  # Ð£Ð¶Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ð°Ñ Ð¼ÐµÑ‚ÐºÐ°
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ landmarks
        full_path = self.data_dir / file_path
        landmarks = self.preprocessor.load_landmark_file(str(full_path))
        
        # ÐžÐ±Ñ€ÐµÐ·ÐºÐ° Ð¿Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸
        if landmarks.shape[0] > self.max_len:
            landmarks = landmarks[:self.max_len]
        
        # ÐŸÑ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³ landmarks Ð² features
        landmarks_batch = landmarks.unsqueeze(0)  # (1, frames, landmarks, 3)
        features = self.preprocessor(landmarks_batch).squeeze(0)  # (frames, features)
        
        # ÐÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ (ÐµÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð°) - Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ÑÑ Ðº features
        if self.augment:
            features = self._augment_features(features)
        
        return {
            'landmarks': features,  # Ð¢ÐµÐ¿ÐµÑ€ÑŒ ÑÑ‚Ð¾ features, Ð° Ð½Ðµ landmarks
            'label': label,
            'sign': sign,
            'file_path': file_path
        }
    
    def _augment_features(self, features: torch.Tensor) -> torch.Tensor:
        """ÐÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ features Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ ASLAugmentations"""
        if not self.augment or not hasattr(self, 'augmenter'):
            return features
            
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ batch dimension Ð´Ð»Ñ Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¹
        features_batch = features.unsqueeze(0)  # (1, frames, features)
        
        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸
        augmented = self.augmenter(features_batch)
        
        # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ batch dimension
        return augmented.squeeze(0)

class ASLDataLoader50Words:
    """Ð—Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸Ðº Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Google ASL Signs Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð¾ 50 ÑÐ»Ð¾Ð²"""
    
    def __init__(self, 
                 data_dir: str = "../data/google_asl_signs",
                 batch_size: int = 32,
                 max_len: int = 384,
                 num_workers: int = 4,
                 preprocessor: Optional[ASLPreprocessor] = None,
                 num_words: int = 50):
        """
        Args:
            data_dir: ÐŸÑƒÑ‚ÑŒ Ðº Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸ Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸
            batch_size: Ð Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð°
            max_len: ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ð½Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
            num_workers: ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð²Ð¾Ñ€ÐºÐµÑ€Ð¾Ð² Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
            preprocessor: ÐŸÑ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€ (ÐµÑÐ»Ð¸ None, ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ÑÑ Ð½Ð¾Ð²Ñ‹Ð¹)
            num_words: ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ»Ð¾Ð² Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ 50)
        """
        self.data_dir = Path(data_dir)
        self.batch_size = batch_size
        self.max_len = max_len
        self.num_workers = num_workers
        self.num_words = num_words
        
        if preprocessor is None:
            self.preprocessor = ASLPreprocessor(max_len=max_len)
        else:
            self.preprocessor = preprocessor
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³ Ð·Ð½Ð°ÐºÐ¾Ð² Ð´Ð»Ñ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° ÐºÐ»Ð°ÑÑÐ¾Ð²
        with open(self.data_dir / "sign_to_prediction_index_map.json", 'r') as f:
            self.sign_mapping = json.load(f)
        
        # ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ»Ð°ÑÑÐ¾Ð² Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ñ€Ð°Ð²Ð½Ð¾ num_words
        self.num_classes = num_words
        
        print(f"ðŸŽ¯ ÐÐ°ÑÑ‚Ñ€Ð¾ÐµÐ½ Ð·Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸Ðº Ð´Ð°Ð½Ð½Ñ‹Ñ… (50 ÑÐ»Ð¾Ð²):")
        print(f"   ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ»Ð°ÑÑÐ¾Ð²: {self.num_classes}")
        print(f"   Ð Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð°: {batch_size}")
        print(f"   ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ð½Ð°: {max_len}")
    
    def get_dataloaders(self, 
                       train_ratio: float = 0.8,
                       val_ratio: float = 0.1,
                       test_ratio: float = 0.1,
                       augment_train: bool = True) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """
        Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ DataLoader'Ñ‹ Ð´Ð»Ñ train/val/test Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð¾ 50 ÑÐ»Ð¾Ð²
        
        Returns:
            train_loader, val_loader, test_loader
        """
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ ÑÐ¿Ð»Ð¸Ñ‚Ð¾Ð²
        splits_dir = self.data_dir / "splits"
        
        if not splits_dir.exists():
            print("âš ï¸ Ð¡Ð¿Ð»Ð¸Ñ‚Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ðµ...")
            from data_utils import ASLDataAnalyzer
            analyzer = ASLDataAnalyzer(str(self.data_dir))
            analyzer.create_balanced_splits(train_ratio, val_ratio, test_ratio)
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñ‹ Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð¾ 50 ÑÐ»Ð¾Ð²
        train_dataset = ASLDataset50Words(
            data_dir=self.data_dir,
            split_file="splits/train.csv",
            preprocessor=self.preprocessor,
            max_len=self.max_len,
            augment=augment_train,
            num_words=self.num_words
        )
        
        val_dataset = ASLDataset50Words(
            data_dir=self.data_dir,
            split_file="splits/val.csv",
            preprocessor=self.preprocessor,
            max_len=self.max_len,
            augment=False,
            num_words=self.num_words
        )
        
        test_dataset = ASLDataset50Words(
            data_dir=self.data_dir,
            split_file="splits/test.csv",
            preprocessor=self.preprocessor,
            max_len=self.max_len,
            augment=False,
            num_words=self.num_words
        )
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ DataLoader'Ñ‹
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
            collate_fn=self._collate_fn
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
            collate_fn=self._collate_fn
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
            collate_fn=self._collate_fn
        )
        
        print(f"ðŸ“Š Ð¡Ð¾Ð·Ð´Ð°Ð½Ñ‹ DataLoader'Ñ‹ (50 ÑÐ»Ð¾Ð²):")
        print(f"   Train: {len(train_loader)} Ð±Ð°Ñ‚Ñ‡ÐµÐ¹")
        print(f"   Val: {len(val_loader)} Ð±Ð°Ñ‚Ñ‡ÐµÐ¹")
        print(f"   Test: {len(test_loader)} Ð±Ð°Ñ‚Ñ‡ÐµÐ¹")
        
        return train_loader, val_loader, test_loader
    
    def _collate_fn(self, batch):
        """
        Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ Ð±Ð°Ñ‚Ñ‡Ð°
        ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ñ€Ð°Ð·Ð½Ð¾Ð¹ Ð´Ð»Ð¸Ð½Ñ‹
        """
        features_list = [item['landmarks'] for item in batch]  # Ð¢ÐµÐ¿ÐµÑ€ÑŒ ÑÑ‚Ð¾ features
        labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)
        
        # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð´Ð»Ð¸Ð½Ñƒ Ð² Ð±Ð°Ñ‚Ñ‡Ðµ
        max_frames = max(features.shape[0] for features in features_list)
        max_frames = min(max_frames, self.max_len)
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€ features Ð¸Ð· Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°
        feature_dim = features_list[0].shape[1]
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ñ‚ÐµÐ½Ð·Ð¾Ñ€ Ð´Ð»Ñ Ð±Ð°Ñ‚Ñ‡Ð°
        batch_size = len(features_list)
        batch_tensor = torch.zeros(batch_size, max_frames, feature_dim)
        
        # Ð—Ð°Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ñ‚ÐµÐ½Ð·Ð¾Ñ€
        for i, features in enumerate(features_list):
            frames = min(features.shape[0], max_frames)
            batch_tensor[i, :frames] = features[:frames]
        
        return {
            'features': batch_tensor,  # ÐŸÐµÑ€ÐµÐ¸Ð¼ÐµÐ½Ð¾Ð²Ñ‹Ð²Ð°ÐµÐ¼ Ð² features
            'labels': labels,
            'lengths': torch.tensor([min(features.shape[0], max_frames) 
                                   for features in features_list])
        }
    
    def get_class_weights(self, split: str = 'train') -> torch.Tensor:
        """Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÑ‚ Ð²ÐµÑÐ° ÐºÐ»Ð°ÑÑÐ¾Ð² Ð´Ð»Ñ ÑÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
        split_file = f"splits/{split}.csv"
        df = pd.read_csv(self.data_dir / split_file)
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³ Ð·Ð½Ð°ÐºÐ¾Ð²
        with open(self.data_dir / "sign_to_prediction_index_map.json", 'r') as f:
            sign_mapping = json.load(f)
        
        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿ÐµÑ€Ð²Ñ‹Ðµ num_words Ð·Ð½Ð°ÐºÐ¾Ð²
        all_signs = list(sign_mapping.keys())
        selected_signs = all_signs[:self.num_words]
        
        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… Ð·Ð½Ð°ÐºÐ¾Ð²
        df = df[df['sign'].isin(selected_signs)].copy()
        
        sign_counts = df['sign'].value_counts()
        
        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð²ÐµÑÐ° (Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð¿Ñ€Ð¾Ð¿Ð¾Ñ€Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ðµ)
        total_samples = len(df)
        class_weights = total_samples / (len(sign_counts) * sign_counts)
        
        # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ð¸Ð½Ð´ÐµÐºÑÐ°Ð¼ ÐºÐ»Ð°ÑÑÐ¾Ð²
        class_weights = class_weights.sort_index()
        
        return torch.tensor(class_weights.values, dtype=torch.float32)

class OptimizedASLTrainer50Words:
    """ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ‚Ñ€ÐµÐ½ÐµÑ€ Ð´Ð»Ñ ASL Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð¾ 50 ÑÐ»Ð¾Ð²"""
    
    def __init__(self, 
                 data_dir: str = "../data/google_asl_signs",
                 model_dir: str = "models",
                 max_len: int = 384,
                 batch_size: int = 16,
                 dim: int = 192,
                 lr: float = 5e-4,
                 epochs: int = 100,  # Ð£Ð¼ÐµÐ½ÑŒÑˆÐ¸Ð»Ð¸ Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
                 device: str = None,
                 use_augmentations: bool = True,
                 use_mixed_precision: bool = True,
                 gradient_clip_val: float = 1.0,
                 gradient_accumulation_steps: int = 3,
                 num_workers: int = 2,
                 pin_memory: bool = True,
                 num_words: int = 50):
        
        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        
        self.max_len = max_len
        self.batch_size = batch_size
        self.dim = dim
        self.lr = lr
        self.epochs = epochs
        self.use_augmentations = use_augmentations
        self.use_mixed_precision = use_mixed_precision
        self.gradient_clip_val = gradient_clip_val
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        self.num_words = num_words
        
        # Device
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # CUDA Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        if self.device.type == 'cuda':
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.cuda.empty_cache()
        
        print(f"ðŸŽ¯ ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ‚Ñ€ÐµÐ½ÐµÑ€ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½ (50 ÑÐ»Ð¾Ð²):")
        print(f"   Device: {self.device}")
        print(f"   ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ»Ð¾Ð²: {num_words}")
        print(f"   ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ð½Ð°: {max_len}")
        print(f"   Ð Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð°: {batch_size}")
        print(f"   Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ batch size: {batch_size * gradient_accumulation_steps}")
        print(f"   Ð Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {dim}")
        print(f"   Learning rate: {lr}")
        print(f"   Ð­Ð¿Ð¾Ñ…Ð¸: {epochs}")
        print(f"   ÐÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸: {'Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½Ñ‹' if use_augmentations else 'ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ñ‹'}")
        print(f"   Mixed Precision: {'Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½' if use_mixed_precision else 'ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½'}")
        print(f"   Gradient Clipping: {gradient_clip_val}")
        print(f"   Gradient Accumulation: {gradient_accumulation_steps} steps")
        print(f"   Num Workers: {num_workers}")
        print(f"   Pin Memory: {pin_memory}")
        
        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²
        self._setup_components()
        
    def _setup_components(self):
        """ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð² Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸"""
        print("\nðŸ“¦ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²...")
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€ Ñ ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð½Ñ‹Ð¼ ÐºÑÑˆÐµÐ¼
        self.preprocessor = ASLPreprocessor(max_len=self.max_len)
        self.preprocessor._max_cache_size = 2000  # Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÐºÑÑˆ
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸Ðº Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð¾ 50 ÑÐ»Ð¾Ð²
        self.dataloader = ASLDataLoader50Words(
            data_dir=str(self.data_dir),
            batch_size=self.batch_size,
            max_len=self.max_len,
            preprocessor=self.preprocessor,
            num_workers=self.num_workers,
            num_words=self.num_words
        )
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ DataLoader'Ñ‹
        print("ðŸ“‚ Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ DataLoader'Ñ‹...")
        self.train_loader, self.val_loader, self.test_loader = self.dataloader.get_dataloaders(
            augment_train=self.use_augmentations
        )
        
        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ñ„Ð¸Ñ‡ Ð¸Ð· Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ Ð±Ð°Ñ‚Ñ‡Ð°
        print("ðŸ” ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
        sample_batch = next(iter(self.train_loader))
        input_dim = sample_batch['features'].shape[-1]
        
        # ÐœÐ¾Ð´ÐµÐ»ÑŒ
        print("ðŸ¤– Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ...")
        self.model = get_model(
            input_dim=input_dim,
            num_classes=self.dataloader.num_classes,  # Ð¢ÐµÐ¿ÐµÑ€ÑŒ ÑÑ‚Ð¾ num_words
            max_len=self.max_len,
            dim=self.dim
        ).to(self.device)
        
        # PyTorch 2.0+ compile (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Linux/Mac)
        if hasattr(torch, 'compile') and self.device.type == 'cuda' and platform.system() != 'Windows':
            try:
                self.model = torch.compile(self.model, mode='reduce-overhead')
                print("   âœ… PyTorch 2.0+ compile Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½")
            except Exception as e:
                print(f"   âš ï¸ PyTorch compile Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½: {e}")
                if hasattr(self.model, '_orig_mod'):
                    self.model = self.model._orig_mod
        else:
            print("   â„¹ï¸ PyTorch compile Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½ (Windows Ð¸Ð»Ð¸ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½)")
        
        # Loss function
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # Optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # Scheduler
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=self.epochs,
            eta_min=1e-6
        )
        
        # Mixed precision scaler
        if self.use_mixed_precision:
            self.scaler = GradScaler()
        
        # Ð’ÐµÑÐ° ÐºÐ»Ð°ÑÑÐ¾Ð²
        self.class_weights = self.dataloader.get_class_weights('train').to(self.device)
        
        print(f"âœ… ÐšÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ñ‹:")
        print(f"   Ð Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ñ„Ð¸Ñ‡: {input_dim}")
        print(f"   ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ»Ð°ÑÑÐ¾Ð²: {self.dataloader.num_classes}")
        print(f"   ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÑÑˆÐ° Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°
        cache_stats = self.preprocessor.get_cache_stats()
        print(f"   ÐšÑÑˆ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°: {cache_stats['cache_size']}/{cache_stats['max_cache_size']} Ñ„Ð°Ð¹Ð»Ð¾Ð²")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ð°Ð¼ÑÑ‚Ð¸ GPU
        if self.device.type == 'cuda':
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   GPU Ð¿Ð°Ð¼ÑÑ‚ÑŒ: {gpu_memory:.1f} GB")
    
    def train_epoch(self, epoch: int):
        """ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ð´Ð½Ð¾Ð¹ ÑÐ¿Ð¾Ñ…Ð¸ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        # Ð˜Ð·Ð¼ÐµÑ€ÑÐµÐ¼ Ð²Ñ€ÐµÐ¼Ñ ÑÐ¿Ð¾Ñ…Ð¸
        epoch_start_time = time.time()
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{self.epochs}")
        
        for batch_idx, batch in enumerate(pbar):
            # ÐŸÐµÑ€ÐµÐ¼ÐµÑ‰Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð° device
            features = batch['features'].to(self.device, non_blocking=True)
            labels = batch['labels'].to(self.device, non_blocking=True)
            
            # Mixed precision forward pass
            if self.use_mixed_precision and self.device.type == 'cuda':
                with autocast(device_type='cuda', dtype=torch.float16):
                    outputs = self.model(features)
                    loss = self.criterion(outputs, labels)
                    loss = loss / self.gradient_accumulation_steps
            else:
                outputs = self.model(features)
                loss = self.criterion(outputs, labels)
                loss = loss / self.gradient_accumulation_steps
            
            # Mixed precision backward pass
            if self.use_mixed_precision and hasattr(self, 'scaler'):
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # Gradient accumulation
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                # Gradient clipping
                if self.use_mixed_precision and hasattr(self, 'scaler'):
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                    self.optimizer.step()
                
                self.optimizer.zero_grad()
            
            # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
            total_loss += loss.item() * self.gradient_accumulation_steps
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ progress bar Ñ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÐµÐ¹
            epoch_time = time.time() - epoch_start_time
            batches_per_sec = (batch_idx + 1) / epoch_time if epoch_time > 0 else 0
            
            pbar.set_postfix({
                'Loss': f'{loss.item() * self.gradient_accumulation_steps:.4f}',
                'Acc': f'{100.*correct/total:.2f}%',
                'LR': f'{self.scheduler.get_last_lr()[0]:.6f}',
                'Speed': f'{batches_per_sec:.1f} batch/s'
            })
            
            # ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð¿Ð°Ð¼ÑÑ‚Ð¸ ÐºÐ°Ð¶Ð´Ñ‹Ðµ 20 Ð±Ð°Ñ‚Ñ‡ÐµÐ¹
            if batch_idx % 20 == 0 and self.device.type == 'cuda':
                torch.cuda.empty_cache()
        
        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ scheduler
        self.scheduler.step()
        
        epoch_time = time.time() - epoch_start_time
        print(f"   â±ï¸ Ð’Ñ€ÐµÐ¼Ñ ÑÐ¿Ð¾Ñ…Ð¸: {epoch_time:.1f} ÑÐµÐº")
        
        return total_loss / len(self.train_loader), 100. * correct / total
    
    def validate(self, epoch: int):
        """Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        val_start_time = time.time()
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                # ÐŸÐµÑ€ÐµÐ¼ÐµÑ‰Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð° device
                features = batch['features'].to(self.device, non_blocking=True)
                labels = batch['labels'].to(self.device, non_blocking=True)
                
                # Forward pass
                if self.use_mixed_precision:
                    with autocast('cuda'):
                        outputs = self.model(features)
                        loss = self.criterion(outputs, labels)
                else:
                    outputs = self.model(features)
                    loss = self.criterion(outputs, labels)
                
                # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        val_time = time.time() - val_start_time
        print(f"   â±ï¸ Ð’Ñ€ÐµÐ¼Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸: {val_time:.1f} ÑÐµÐº")
        
        return total_loss / len(self.val_loader), 100. * correct / total
    
    def save_checkpoint(self, epoch: int, val_acc: float, is_best: bool = False):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚Ð°"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_acc': val_acc,
            'config': {
                'max_len': self.max_len,
                'batch_size': self.batch_size,
                'dim': self.dim,
                'lr': self.lr,
                'use_augmentations': self.use_augmentations,
                'use_mixed_precision': self.use_mixed_precision,
                'gradient_clip_val': self.gradient_clip_val,
                'gradient_accumulation_steps': self.gradient_accumulation_steps,
                'num_words': self.num_words
            }
        }
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ scaler Ð´Ð»Ñ mixed precision
        if self.use_mixed_precision:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚
        checkpoint_path = self.model_dir / f"checkpoint_50words_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚
        if is_best:
            best_path = self.model_dir / "best_model_50words.pth"
            torch.save(checkpoint, best_path)
            print(f"ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚ (50 ÑÐ»Ð¾Ð²): {best_path}")
    
    def train(self):
        """ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ†Ð¸ÐºÐ» Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸"""
        print(f"ðŸš€ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ (50 ÑÐ»Ð¾Ð²)...")
        print(f"   Ð’Ñ€ÐµÐ¼Ñ Ð½Ð°Ñ‡Ð°Ð»Ð°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        best_val_acc = 0
        train_losses = []
        train_accs = []
        val_losses = []
        val_accs = []
        
        total_start_time = time.time()
        
        for epoch in range(self.epochs):
            epoch_start_time = time.time()
            
            # ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
            train_loss, train_acc = self.train_epoch(epoch)
            train_losses.append(train_loss)
            train_accs.append(train_acc)
            
            # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ
            val_loss, val_acc = self.validate(epoch)
            val_losses.append(val_loss)
            val_accs.append(val_acc)
            
            epoch_time = time.time() - epoch_start_time
            
            # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
            print(f"Epoch {epoch+1}/{self.epochs} ({epoch_time:.1f} ÑÐµÐº):")
            print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            print(f"  LR: {self.scheduler.get_last_lr()[0]:.6f}")
            
            # Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸ GPU
            if self.device.type == 'cuda':
                gpu_memory_used = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_cached = torch.cuda.memory_reserved() / 1024**3
                print(f"  GPU Memory: {gpu_memory_used:.1f}GB used, {gpu_memory_cached:.1f}GB cached")
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚Ð°
            is_best = val_acc > best_val_acc
            if is_best:
                best_val_acc = val_acc
                print(f"ðŸŽ‰ ÐÐ¾Ð²Ñ‹Ð¹ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: {val_acc:.2f}%")
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ðµ 10 ÑÐ¿Ð¾Ñ… Ð¸Ð»Ð¸ ÐµÑÐ»Ð¸ ÑÑ‚Ð¾ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ (ÑƒÐ¼ÐµÐ½ÑŒÑˆÐ¸Ð»Ð¸ Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ)
            if (epoch + 1) % 10 == 0 or is_best:
                self.save_checkpoint(epoch, val_acc, is_best)
            
            # Ð Ð°Ð½Ð½ÑÑ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° (ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ð¹ 15 ÑÐ¿Ð¾Ñ…)
            if epoch > 15 and max(val_accs[-15:]) < best_val_acc:
                print(f"â¹ï¸ Ð Ð°Ð½Ð½ÑÑ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð½Ð° ÑÐ¿Ð¾Ñ…Ðµ {epoch+1}")
                break
        
        total_time = time.time() - total_start_time
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
        self.save_checkpoint(self.epochs-1, val_acc)
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
        history = {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'val_losses': val_losses,
            'val_accs': val_accs,
            'best_val_acc': best_val_acc,
            'total_training_time': total_time,
            'num_words': self.num_words
        }
        
        history_path = self.model_dir / "training_history_50words.json"
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2)
        
        print(f"âœ… ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾ (50 ÑÐ»Ð¾Ð²)!")
        print(f"   Ð›ÑƒÑ‡ÑˆÐ°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ: {best_val_acc:.2f}%")
        print(f"   ÐžÐ±Ñ‰ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ: {total_time/60:.1f} Ð¼Ð¸Ð½ÑƒÑ‚")
        print(f"   Ð’Ñ€ÐµÐ¼Ñ Ð¾ÐºÐ¾Ð½Ñ‡Ð°Ð½Ð¸Ñ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÑÑˆÐ° Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°
        cache_stats = self.preprocessor.get_cache_stats()
        print(f"ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÑÑˆÐ° Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°:")
        print(f"   Hit rate: {cache_stats['hit_rate']:.1%}")
        print(f"   Cache hits: {cache_stats['cache_hits']:,}")
        print(f"   Cache misses: {cache_stats['cache_misses']:,}")
        print(f"   Ð¤Ð°Ð¹Ð»Ð¾Ð² Ð² ÐºÑÑˆÐµ: {cache_stats['cache_size']}/{cache_stats['max_cache_size']}")
        
        return history

def main():
    """ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸ Ð´Ð»Ñ 50 ÑÐ»Ð¾Ð²"""
    print("ðŸ¤Ÿ ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—Ð˜Ð ÐžÐ’ÐÐÐÐžÐ• ÐžÐ‘Ð£Ð§Ð•ÐÐ˜Ð• ASL ÐœÐžÐ”Ð•Ð›Ð˜ (50 Ð¡Ð›ÐžÐ’)")
    print("=" * 70)
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ðµ Ñ€ÐµÑÑƒÑ€ÑÑ‹
    print("ðŸ” ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ñ… Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð²:")
    cpu_count = psutil.cpu_count()
    memory = psutil.virtual_memory()
    print(f"   CPU: {cpu_count} ÑÐ´ÐµÑ€")
    print(f"   RAM: {memory.total / 1024**3:.1f} GB")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   GPU: {gpu_name}")
        print(f"   GPU Memory: {gpu_memory:.1f} GB")
    
    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ‚Ñ€ÐµÐ½ÐµÑ€ Ð´Ð»Ñ 50 ÑÐ»Ð¾Ð²
    trainer = OptimizedASLTrainer50Words(
        data_dir="../data/google_asl_signs",
        model_dir="models",
        max_len=384,
        batch_size=16,
        dim=192,
        lr=5e-4,
        epochs=100,  # Ð£Ð¼ÐµÐ½ÑŒÑˆÐ¸Ð»Ð¸ Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
        use_augmentations=True,
        use_mixed_precision=True,
        gradient_clip_val=1.0,
        gradient_accumulation_steps=3,
        num_workers=2,
        pin_memory=True,
        num_words=50  # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð¾ 50 ÑÐ»Ð¾Ð²
    )
    
    # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
    history = trainer.train()
    
    print("ðŸŽ‰ ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ (50 ÑÐ»Ð¾Ð²) Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾!")

if __name__ == "__main__":
    main() 
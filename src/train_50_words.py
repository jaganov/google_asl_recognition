# train_50_words.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.amp import autocast
from torch.cuda.amp import GradScaler
import numpy as np
from tqdm import tqdm
import time
import os
from pathlib import Path
import json
from datetime import datetime
import platform
import psutil
import pandas as pd
from typing import Optional, Tuple
from torch.utils.data import DataLoader

# –ü–æ–¥–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏ Triton –Ω–∞ Windows
if platform.system() == 'Windows':
    try:
        import torch._dynamo
        torch._dynamo.config.suppress_errors = True
        print("üîß –ü–æ–¥–∞–≤–ª–µ–Ω—ã –æ—à–∏–±–∫–∏ Triton –¥–ª—è Windows")
    except:
        pass

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ TensorFloat32 –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
if torch.cuda.is_available():
    torch.set_float32_matmul_precision('high')
    print("üîß –í–∫–ª—é—á–µ–Ω TensorFloat32 –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")

from data_loader import ASLDataLoader
from preprocessing import ASLPreprocessor
from models import get_model, ASLEnsemble

class ASLDataset50Words:
    """Dataset –¥–ª—è Google ASL Signs —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–æ 50 —Å–ª–æ–≤"""
    
    def __init__(self, 
                 data_dir: str,
                 split_file: str,
                 preprocessor: ASLPreprocessor,
                 max_len: int = 384,
                 augment: bool = False,
                 num_words: int = 50):
        """
        Args:
            data_dir: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
            split_file: –§–∞–π–ª —Å–æ —Å–ø–ª–∏—Ç–æ–º (train.csv, val.csv, test.csv)
            preprocessor: –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä landmarks
            max_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            augment: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é
            num_words: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 50)
        """
        self.data_dir = Path(data_dir)
        self.preprocessor = preprocessor
        self.max_len = max_len
        self.augment = augment
        self.num_words = num_words
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self.augment:
            from augmentations import ASLAugmentations
            self.augmenter = ASLAugmentations()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–ø–ª–∏—Ç–∞
        self.df = pd.read_csv(self.data_dir / split_file)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞–∫–æ–≤
        with open(self.data_dir / "sign_to_prediction_index_map.json", 'r') as f:
            self.sign_mapping = json.load(f)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ num_words –∑–Ω–∞–∫–æ–≤
        all_signs = list(self.sign_mapping.keys())
        selected_signs = all_signs[:num_words]
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –º–∞–ø–ø–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤
        self.filtered_sign_mapping = {sign: idx for idx, sign in enumerate(selected_signs)}
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤
        self.df = self.df[self.df['sign'].isin(selected_signs)].copy()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –Ω–æ–≤—ã–º –º–∞–ø–ø–∏–Ω–≥–æ–º
        self.df['label'] = self.df['sign'].map(self.filtered_sign_mapping)
        
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω {split_file} (—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –¥–æ {num_words} —Å–ª–æ–≤): {len(self.df)} –∑–∞–ø–∏—Å–µ–π")
        print(f"üéØ –í—ã–±—Ä–∞–Ω–Ω—ã–µ –∑–Ω–∞–∫–∏: {', '.join(selected_signs[:10])}{'...' if len(selected_signs) > 10 else ''}")
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å"""
        row = self.df.iloc[idx]
        file_path = row['path']
        sign = row['sign']
        label = row['label']  # –£–∂–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º landmarks
        full_path = self.data_dir / file_path
        landmarks = self.preprocessor.load_landmark_file(str(full_path))
        
        # –û–±—Ä–µ–∑–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        if landmarks.shape[0] > self.max_len:
            landmarks = landmarks[:self.max_len]
        
        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ landmarks –≤ features
        landmarks_batch = landmarks.unsqueeze(0)  # (1, frames, landmarks, 3)
        features = self.preprocessor(landmarks_batch).squeeze(0)  # (frames, features)
        
        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞) - –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ features
        if self.augment:
            features = self._augment_features(features)
        
        return {
            'landmarks': features,  # –¢–µ–ø–µ—Ä—å —ç—Ç–æ features, –∞ –Ω–µ landmarks
            'label': label,
            'sign': sign,
            'file_path': file_path
        }
    
    def _augment_features(self, features: torch.Tensor) -> torch.Tensor:
        """–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è features —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ASLAugmentations"""
        if not self.augment or not hasattr(self, 'augmenter'):
            return features
            
        # –î–æ–±–∞–≤–ª—è–µ–º batch dimension –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π
        features_batch = features.unsqueeze(0)  # (1, frames, features)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        augmented = self.augmenter(features_batch)
        
        # –£–±–∏—Ä–∞–µ–º batch dimension
        return augmented.squeeze(0)

class ASLDataLoader50Words:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Google ASL Signs —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–æ 50 —Å–ª–æ–≤"""
    
    def __init__(self, 
                 data_dir: str = "../data/google_asl_signs",
                 batch_size: int = 32,
                 max_len: int = 384,
                 num_workers: int = 4,
                 preprocessor: Optional[ASLPreprocessor] = None,
                 num_words: int = 50):
        """
        Args:
            data_dir: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            max_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            num_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            preprocessor: –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
            num_words: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 50)
        """
        self.data_dir = Path(data_dir)
        self.batch_size = batch_size
        self.max_len = max_len
        self.num_workers = num_workers
        self.num_words = num_words
        
        if preprocessor is None:
            self.preprocessor = ASLPreprocessor(max_len=max_len)
        else:
            self.preprocessor = preprocessor
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–æ–≤
        with open(self.data_dir / "sign_to_prediction_index_map.json", 'r') as f:
            self.sign_mapping = json.load(f)
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ —Ç–µ–ø–µ—Ä—å —Ä–∞–≤–Ω–æ num_words
        self.num_classes = num_words
        
        print(f"üéØ –ù–∞—Å—Ç—Ä–æ–µ–Ω –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö (50 —Å–ª–æ–≤):")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {self.num_classes}")
        print(f"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {max_len}")
    
    def get_dataloaders(self, 
                       train_ratio: float = 0.8,
                       val_ratio: float = 0.1,
                       test_ratio: float = 0.1,
                       augment_train: bool = True) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """
        –°–æ–∑–¥–∞–µ—Ç DataLoader'—ã –¥–ª—è train/val/test —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–æ 50 —Å–ª–æ–≤
        
        Returns:
            train_loader, val_loader, test_loader
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ø–ª–∏—Ç–æ–≤
        splits_dir = self.data_dir / "splits"
        
        if not splits_dir.exists():
            print("‚ö†Ô∏è –°–ø–ª–∏—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ...")
            from data_utils import ASLDataAnalyzer
            analyzer = ASLDataAnalyzer(str(self.data_dir))
            analyzer.create_balanced_splits(train_ratio, val_ratio, test_ratio)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–æ 50 —Å–ª–æ–≤
        train_dataset = ASLDataset50Words(
            data_dir=self.data_dir,
            split_file="splits/train.csv",
            preprocessor=self.preprocessor,
            max_len=self.max_len,
            augment=augment_train,
            num_words=self.num_words
        )
        
        val_dataset = ASLDataset50Words(
            data_dir=self.data_dir,
            split_file="splits/val.csv",
            preprocessor=self.preprocessor,
            max_len=self.max_len,
            augment=False,
            num_words=self.num_words
        )
        
        test_dataset = ASLDataset50Words(
            data_dir=self.data_dir,
            split_file="splits/test.csv",
            preprocessor=self.preprocessor,
            max_len=self.max_len,
            augment=False,
            num_words=self.num_words
        )
        
        # –°–æ–∑–¥–∞–µ–º DataLoader'—ã
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
            collate_fn=self._collate_fn
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
            collate_fn=self._collate_fn
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
            collate_fn=self._collate_fn
        )
        
        print(f"üìä –°–æ–∑–¥–∞–Ω—ã DataLoader'—ã (50 —Å–ª–æ–≤):")
        print(f"   Train: {len(train_loader)} –±–∞—Ç—á–µ–π")
        print(f"   Val: {len(val_loader)} –±–∞—Ç—á–µ–π")
        print(f"   Test: {len(test_loader)} –±–∞—Ç—á–µ–π")
        
        return train_loader, val_loader, test_loader
    
    def _collate_fn(self, batch):
        """
        –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –±–∞—Ç—á–∞
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
        """
        features_list = [item['landmarks'] for item in batch]  # –¢–µ–ø–µ—Ä—å —ç—Ç–æ features
        labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)
        
        # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –≤ –±–∞—Ç—á–µ
        max_frames = max(features.shape[0] for features in features_list)
        max_frames = min(max_frames, self.max_len)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä features –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
        feature_dim = features_list[0].shape[1]
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä –¥–ª—è –±–∞—Ç—á–∞
        batch_size = len(features_list)
        batch_tensor = torch.zeros(batch_size, max_frames, feature_dim)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º —Ç–µ–Ω–∑–æ—Ä
        for i, features in enumerate(features_list):
            frames = min(features.shape[0], max_frames)
            batch_tensor[i, :frames] = features[:frames]
        
        return {
            'features': batch_tensor,  # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –≤ features
            'labels': labels,
            'lengths': torch.tensor([min(features.shape[0], max_frames) 
                                   for features in features_list])
        }
    
    def get_class_weights(self, split: str = 'train') -> torch.Tensor:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        split_file = f"splits/{split}.csv"
        df = pd.read_csv(self.data_dir / split_file)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞–∫–æ–≤
        with open(self.data_dir / "sign_to_prediction_index_map.json", 'r') as f:
            sign_mapping = json.load(f)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ num_words –∑–Ω–∞–∫–æ–≤
        all_signs = list(sign_mapping.keys())
        selected_signs = all_signs[:self.num_words]
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤
        df = df[df['sign'].isin(selected_signs)].copy()
        
        sign_counts = df['sign'].value_counts()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ (–æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —á–∞—Å—Ç–æ—Ç–µ)
        total_samples = len(df)
        class_weights = total_samples / (len(sign_counts) * sign_counts)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º –∫–ª–∞—Å—Å–æ–≤
        class_weights = class_weights.sort_index()
        
        return torch.tensor(class_weights.values, dtype=torch.float32)

class OptimizedASLTrainer50Words:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è ASL –º–æ–¥–µ–ª–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–æ 50 —Å–ª–æ–≤"""
    
    def __init__(self, 
                 data_dir: str = "../data/google_asl_signs",
                 model_dir: str = "models",
                 max_len: int = 384,
                 batch_size: int = 16,
                 dim: int = 192,
                 lr: float = 5e-4,
                 epochs: int = 100,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                 device: str = None,
                 use_augmentations: bool = True,
                 use_mixed_precision: bool = True,
                 gradient_clip_val: float = 1.0,
                 gradient_accumulation_steps: int = 3,
                 num_workers: int = 2,
                 pin_memory: bool = True,
                 num_words: int = 50):
        
        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        
        self.max_len = max_len
        self.batch_size = batch_size
        self.dim = dim
        self.lr = lr
        self.epochs = epochs
        self.use_augmentations = use_augmentations
        self.use_mixed_precision = use_mixed_precision
        self.gradient_clip_val = gradient_clip_val
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        self.num_words = num_words
        
        # Device
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if self.device.type == 'cuda':
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.cuda.empty_cache()
        
        print(f"üéØ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω (50 —Å–ª–æ–≤):")
        print(f"   Device: {self.device}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {num_words}")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {max_len}")
        print(f"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
        print(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size: {batch_size * gradient_accumulation_steps}")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {dim}")
        print(f"   Learning rate: {lr}")
        print(f"   –≠–ø–æ—Ö–∏: {epochs}")
        print(f"   –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {'–í–∫–ª—é—á–µ–Ω—ã' if use_augmentations else '–û—Ç–∫–ª—é—á–µ–Ω—ã'}")
        print(f"   Mixed Precision: {'–í–∫–ª—é—á–µ–Ω' if use_mixed_precision else '–û—Ç–∫–ª—é—á–µ–Ω'}")
        print(f"   Gradient Clipping: {gradient_clip_val}")
        print(f"   Gradient Accumulation: {gradient_accumulation_steps} steps")
        print(f"   Num Workers: {num_workers}")
        print(f"   Pin Memory: {pin_memory}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._setup_components()
        
    def _setup_components(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        print("\nüì¶ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –∫—ç—à–µ–º
        self.preprocessor = ASLPreprocessor(max_len=self.max_len)
        self.preprocessor._max_cache_size = 2000  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫—ç—à
        
        # –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–æ 50 —Å–ª–æ–≤
        self.dataloader = ASLDataLoader50Words(
            data_dir=str(self.data_dir),
            batch_size=self.batch_size,
            max_len=self.max_len,
            preprocessor=self.preprocessor,
            num_workers=self.num_workers,
            num_words=self.num_words
        )
        
        # –ü–æ–ª—É—á–∞–µ–º DataLoader'—ã
        print("üìÇ –°–æ–∑–¥–∞–µ–º DataLoader'—ã...")
        self.train_loader, self.val_loader, self.test_loader = self.dataloader.get_dataloaders(
            augment_train=self.use_augmentations
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∏—á –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞
        print("üîç –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        sample_batch = next(iter(self.train_loader))
        input_dim = sample_batch['features'].shape[-1]
        
        # –ú–æ–¥–µ–ª—å
        print("ü§ñ –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å...")
        self.model = get_model(
            input_dim=input_dim,
            num_classes=self.dataloader.num_classes,  # –¢–µ–ø–µ—Ä—å —ç—Ç–æ num_words
            max_len=self.max_len,
            dim=self.dim
        ).to(self.device)
        
        # PyTorch 2.0+ compile (—Ç–æ–ª—å–∫–æ –¥–ª—è Linux/Mac)
        if hasattr(torch, 'compile') and self.device.type == 'cuda' and platform.system() != 'Windows':
            try:
                self.model = torch.compile(self.model, mode='reduce-overhead')
                print("   ‚úÖ PyTorch 2.0+ compile –≤–∫–ª—é—á–µ–Ω")
            except Exception as e:
                print(f"   ‚ö†Ô∏è PyTorch compile –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                if hasattr(self.model, '_orig_mod'):
                    self.model = self.model._orig_mod
        else:
            print("   ‚ÑπÔ∏è PyTorch compile –æ—Ç–∫–ª—é—á–µ–Ω (Windows –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)")
        
        # Loss function
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # Optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # Scheduler
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=self.epochs,
            eta_min=1e-6
        )
        
        # Mixed precision scaler
        if self.use_mixed_precision:
            self.scaler = GradScaler()
        
        # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        self.class_weights = self.dataloader.get_class_weights('train').to(self.device)
        
        print(f"‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã:")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∏—á: {input_dim}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {self.dataloader.num_classes}")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        cache_stats = self.preprocessor.get_cache_stats()
        print(f"   –ö—ç—à –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞: {cache_stats['cache_size']}/{cache_stats['max_cache_size']} —Ñ–∞–π–ª–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ GPU
        if self.device.type == 'cuda':
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   GPU –ø–∞–º—è—Ç—å: {gpu_memory:.1f} GB")
    
    def train_epoch(self, epoch: int):
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è —ç–ø–æ—Ö–∏
        epoch_start_time = time.time()
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{self.epochs}")
        
        for batch_idx, batch in enumerate(pbar):
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ device
            features = batch['features'].to(self.device, non_blocking=True)
            labels = batch['labels'].to(self.device, non_blocking=True)
            
            # Mixed precision forward pass
            if self.use_mixed_precision and self.device.type == 'cuda':
                with autocast(device_type='cuda', dtype=torch.float16):
                    outputs = self.model(features)
                    loss = self.criterion(outputs, labels)
                    loss = loss / self.gradient_accumulation_steps
            else:
                outputs = self.model(features)
                loss = self.criterion(outputs, labels)
                loss = loss / self.gradient_accumulation_steps
            
            # Mixed precision backward pass
            if self.use_mixed_precision and hasattr(self, 'scaler'):
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # Gradient accumulation
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                # Gradient clipping
                if self.use_mixed_precision and hasattr(self, 'scaler'):
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                    self.optimizer.step()
                
                self.optimizer.zero_grad()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_loss += loss.item() * self.gradient_accumulation_steps
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º progress bar —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            epoch_time = time.time() - epoch_start_time
            batches_per_sec = (batch_idx + 1) / epoch_time if epoch_time > 0 else 0
            
            pbar.set_postfix({
                'Loss': f'{loss.item() * self.gradient_accumulation_steps:.4f}',
                'Acc': f'{100.*correct/total:.2f}%',
                'LR': f'{self.scheduler.get_last_lr()[0]:.6f}',
                'Speed': f'{batches_per_sec:.1f} batch/s'
            })
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 20 –±–∞—Ç—á–µ–π
            if batch_idx % 20 == 0 and self.device.type == 'cuda':
                torch.cuda.empty_cache()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º scheduler
        self.scheduler.step()
        
        epoch_time = time.time() - epoch_start_time
        print(f"   ‚è±Ô∏è –í—Ä–µ–º—è —ç–ø–æ—Ö–∏: {epoch_time:.1f} —Å–µ–∫")
        
        return total_loss / len(self.train_loader), 100. * correct / total
    
    def validate(self, epoch: int):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        val_start_time = time.time()
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ device
                features = batch['features'].to(self.device, non_blocking=True)
                labels = batch['labels'].to(self.device, non_blocking=True)
                
                # Forward pass
                if self.use_mixed_precision:
                    with autocast('cuda'):
                        outputs = self.model(features)
                        loss = self.criterion(outputs, labels)
                else:
                    outputs = self.model(features)
                    loss = self.criterion(outputs, labels)
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        val_time = time.time() - val_start_time
        print(f"   ‚è±Ô∏è –í—Ä–µ–º—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {val_time:.1f} —Å–µ–∫")
        
        return total_loss / len(self.val_loader), 100. * correct / total
    
    def save_checkpoint(self, epoch: int, val_acc: float, is_best: bool = False):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_acc': val_acc,
            'config': {
                'max_len': self.max_len,
                'batch_size': self.batch_size,
                'dim': self.dim,
                'lr': self.lr,
                'use_augmentations': self.use_augmentations,
                'use_mixed_precision': self.use_mixed_precision,
                'gradient_clip_val': self.gradient_clip_val,
                'gradient_accumulation_steps': self.gradient_accumulation_steps,
                'num_words': self.num_words
            }
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler –¥–ª—è mixed precision
        if self.use_mixed_precision:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        checkpoint_path = self.model_dir / f"checkpoint_50words_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        if is_best:
            best_path = self.model_dir / "best_model_50words.pth"
            torch.save(checkpoint, best_path)
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç (50 —Å–ª–æ–≤): {best_path}")
    
    def train(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (50 —Å–ª–æ–≤)...")
        print(f"   –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        best_val_acc = 0
        train_losses = []
        train_accs = []
        val_losses = []
        val_accs = []
        
        total_start_time = time.time()
        
        for epoch in range(self.epochs):
            epoch_start_time = time.time()
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_acc = self.train_epoch(epoch)
            train_losses.append(train_loss)
            train_accs.append(train_acc)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_acc = self.validate(epoch)
            val_losses.append(val_loss)
            val_accs.append(val_acc)
            
            epoch_time = time.time() - epoch_start_time
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            print(f"Epoch {epoch+1}/{self.epochs} ({epoch_time:.1f} —Å–µ–∫):")
            print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            print(f"  LR: {self.scheduler.get_last_lr()[0]:.6f}")
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏ GPU
            if self.device.type == 'cuda':
                gpu_memory_used = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_cached = torch.cuda.memory_reserved() / 1024**3
                print(f"  GPU Memory: {gpu_memory_used:.1f}GB used, {gpu_memory_cached:.1f}GB cached")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
            is_best = val_acc > best_val_acc
            if is_best:
                best_val_acc = val_acc
                print(f"üéâ –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {val_acc:.2f}%")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö –∏–ª–∏ –µ—Å–ª–∏ —ç—Ç–æ –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—É–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
            if (epoch + 1) % 10 == 0 or is_best:
                self.save_checkpoint(epoch, val_acc, is_best)
            
            # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ (–µ—Å–ª–∏ –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π 15 —ç–ø–æ—Ö)
            if epoch > 15 and max(val_accs[-15:]) < best_val_acc:
                print(f"‚èπÔ∏è –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                break
        
        total_time = time.time() - total_start_time
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        self.save_checkpoint(self.epochs-1, val_acc)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
        history = {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'val_losses': val_losses,
            'val_accs': val_accs,
            'best_val_acc': best_val_acc,
            'total_training_time': total_time,
            'num_words': self.num_words
        }
        
        history_path = self.model_dir / "training_history_50words.json"
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2)
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ (50 —Å–ª–æ–≤)!")
        print(f"   –õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.2f}%")
        print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
        print(f"   –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        cache_stats = self.preprocessor.get_cache_stats()
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞:")
        print(f"   Hit rate: {cache_stats['hit_rate']:.1%}")
        print(f"   Cache hits: {cache_stats['cache_hits']:,}")
        print(f"   Cache misses: {cache_stats['cache_misses']:,}")
        print(f"   –§–∞–π–ª–æ–≤ –≤ –∫—ç—à–µ: {cache_stats['cache_size']}/{cache_stats['max_cache_size']}")
        
        return history

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è 50 —Å–ª–æ–≤"""
    print("ü§ü –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï ASL –ú–û–î–ï–õ–ò (50 –°–õ–û–í)")
    print("=" * 70)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤:")
    cpu_count = psutil.cpu_count()
    memory = psutil.virtual_memory()
    print(f"   CPU: {cpu_count} —è–¥–µ—Ä")
    print(f"   RAM: {memory.total / 1024**3:.1f} GB")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   GPU: {gpu_name}")
        print(f"   GPU Memory: {gpu_memory:.1f} GB")
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è 50 —Å–ª–æ–≤
    trainer = OptimizedASLTrainer50Words(
        data_dir="../data/google_asl_signs",
        model_dir="models",
        max_len=384,
        batch_size=16,
        dim=192,
        lr=5e-4,
        epochs=100,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        use_augmentations=True,
        use_mixed_precision=True,
        gradient_clip_val=1.0,
        gradient_accumulation_steps=3,
        num_workers=2,
        pin_memory=True,
        num_words=50  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 50 —Å–ª–æ–≤
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    history = trainer.train()
    
    print("üéâ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (50 —Å–ª–æ–≤) –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")

if __name__ == "__main__":
    main() 
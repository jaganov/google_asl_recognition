# train.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.amp import autocast
from torch.cuda.amp import GradScaler
import numpy as np
from tqdm import tqdm
import time
import os
from pathlib import Path
import json
from datetime import datetime
import platform

# –ü–æ–¥–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏ Triton –Ω–∞ Windows
if platform.system() == 'Windows':
    try:
        import torch._dynamo
        torch._dynamo.config.suppress_errors = True
        print("üîß –ü–æ–¥–∞–≤–ª–µ–Ω—ã –æ—à–∏–±–∫–∏ Triton –¥–ª—è Windows")
    except:
        pass

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ TensorFloat32 –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
if torch.cuda.is_available():
    torch.set_float32_matmul_precision('high')
    print("üîß –í–∫–ª—é—á–µ–Ω TensorFloat32 –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")

from data_loader import ASLDataLoader
from preprocessing import ASLPreprocessor
from models import get_model, ASLEnsemble

class ASLTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è ASL –º–æ–¥–µ–ª–∏ (–∫–∞–∫ —É –ø–æ–±–µ–¥–∏—Ç–µ–ª—è) —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è RTX 4070"""
    
    def __init__(self, 
                 data_dir: str = "../data/google_asl_signs",
                 model_dir: str = "models",
                 max_len: int = 384,
                 batch_size: int = 12,  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è RTX 4070 12GB VRAM
                 dim: int = 192,
                 lr: float = 5e-4,
                 epochs: int = 400,
                 device: str = None,
                 use_augmentations: bool = True,
                 use_mixed_precision: bool = True,  # Mixed precision –¥–ª—è RTX 4070
                 gradient_clip_val: float = 1.0,   # Gradient clipping
                 gradient_accumulation_steps: int = 4):  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size = 48
        
        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        
        self.max_len = max_len
        self.batch_size = batch_size
        self.dim = dim
        self.lr = lr
        self.epochs = epochs
        self.use_augmentations = use_augmentations
        self.use_mixed_precision = use_mixed_precision
        self.gradient_clip_val = gradient_clip_val
        self.gradient_accumulation_steps = gradient_accumulation_steps
        
        # Device
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è RTX 4070
        if self.device.type == 'cuda':
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
            torch.cuda.empty_cache()
        
        print(f"üéØ –¢—Ä–µ–Ω–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è RTX 4070):")
        print(f"   Device: {self.device}")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {max_len}")
        print(f"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
        print(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size: {batch_size * gradient_accumulation_steps}")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {dim}")
        print(f"   Learning rate: {lr}")
        print(f"   –≠–ø–æ—Ö–∏: {epochs}")
        print(f"   –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {'–í–∫–ª—é—á–µ–Ω—ã' if use_augmentations else '–û—Ç–∫–ª—é—á–µ–Ω—ã'}")
        print(f"   Mixed Precision: {'–í–∫–ª—é—á–µ–Ω' if use_mixed_precision else '–û—Ç–∫–ª—é—á–µ–Ω'}")
        print(f"   Gradient Clipping: {gradient_clip_val}")
        print(f"   Gradient Accumulation: {gradient_accumulation_steps} steps")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._setup_components()
        
    def _setup_components(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        # –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è–º–∏ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ø–∞–º—è—Ç–∏)
        self.dataloader = ASLDataLoader(
            data_dir=str(self.data_dir),
            batch_size=self.batch_size,
            max_len=self.max_len,
            preprocessor=None,  # DataLoader —Å–æ–∑–¥–∞—Å—Ç –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å–∞–º
            num_workers=4  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è RTX 4070
        )
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∏–∑ DataLoader
        self.preprocessor = self.dataloader.preprocessor
        
        # –ü–æ–ª—É—á–∞–µ–º DataLoader'—ã —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è–º–∏ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        self.train_loader, self.val_loader, self.test_loader = self.dataloader.get_dataloaders(
            augment_train=self.use_augmentations
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∏—á –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞
        sample_batch = next(iter(self.train_loader))
        input_dim = sample_batch['features'].shape[-1]  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å features
        
        # –ú–æ–¥–µ–ª—å
        self.model = get_model(
            input_dim=input_dim,
            num_classes=self.dataloader.num_classes,
            max_len=self.max_len,
            dim=self.dim
        ).to(self.device)
        
        # PyTorch 2.0+ compile –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (–æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è Windows)
        import platform
        if hasattr(torch, 'compile') and self.device.type == 'cuda' and platform.system() != 'Windows':
            try:
                # –ü—Ä–æ–±—É–µ–º –±–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è Linux/Mac
                self.model = torch.compile(self.model, mode='reduce-overhead')
                print("   ‚úÖ PyTorch 2.0+ compile –≤–∫–ª—é—á–µ–Ω (reduce-overhead mode)")
            except Exception as e:
                print(f"   ‚ö†Ô∏è PyTorch compile –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                print("   ‚ÑπÔ∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ compile")
                # –£–±–∏—Ä–∞–µ–º compile –µ—Å–ª–∏ –æ–Ω –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
                if hasattr(self.model, '_orig_mod'):
                    self.model = self.model._orig_mod
        else:
            if platform.system() == 'Windows':
                print("   ‚ÑπÔ∏è PyTorch compile –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è Windows (–∏–∑–±–µ–≥–∞–µ–º –ø—Ä–æ–±–ª–µ–º —Å Triton)")
            else:
                print("   ‚ÑπÔ∏è PyTorch compile –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –Ω–µ–≥–æ)")
        
        # Loss function (–∫–∞–∫ —É –ø–æ–±–µ–¥–∏—Ç–µ–ª—è)
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # Optimizer (–∫–∞–∫ —É –ø–æ–±–µ–¥–∏—Ç–µ–ª—è: AdamW)
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # Scheduler (–∫–∞–∫ —É –ø–æ–±–µ–¥–∏—Ç–µ–ª—è: CosineDecay)
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=self.epochs,
            eta_min=1e-6
        )
        
        # Mixed precision scaler
        if self.use_mixed_precision:
            self.scaler = GradScaler()
        
        # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        self.class_weights = self.dataloader.get_class_weights('train').to(self.device)
        
        print(f"‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã:")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∏—á: {input_dim}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {self.dataloader.num_classes}")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        cache_stats = self.preprocessor.get_cache_stats()
        print(f"   –ö—ç—à –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞: {cache_stats['cache_size']}/{cache_stats['max_cache_size']} —Ñ–∞–π–ª–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ GPU
        if self.device.type == 'cuda':
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   GPU –ø–∞–º—è—Ç—å: {gpu_memory:.1f} GB")
    
    def train_epoch(self, epoch: int):
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{self.epochs}")
        
        for batch_idx, batch in enumerate(pbar):
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ device
            features = batch['features'].to(self.device, non_blocking=True)
            labels = batch['labels'].to(self.device, non_blocking=True)
            
            # Mixed precision forward pass
            if self.use_mixed_precision and self.device.type == 'cuda':
                try:
                    with autocast(device_type='cuda', dtype=torch.float16):
                        outputs = self.model(features)
                        loss = self.criterion(outputs, labels)
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º loss –¥–ª—è gradient accumulation
                        loss = loss / self.gradient_accumulation_steps
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Mixed precision –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                    print("   ‚ÑπÔ∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ mixed precision")
                    self.use_mixed_precision = False
                    outputs = self.model(features)
                    loss = self.criterion(outputs, labels)
                    loss = loss / self.gradient_accumulation_steps
            else:
                outputs = self.model(features)
                loss = self.criterion(outputs, labels)
                loss = loss / self.gradient_accumulation_steps
            
            # Mixed precision backward pass
            if self.use_mixed_precision and hasattr(self, 'scaler'):
                try:
                    self.scaler.scale(loss).backward()
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Mixed precision backward –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                    print("   ‚ÑπÔ∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ mixed precision")
                    self.use_mixed_precision = False
                    loss.backward()
            else:
                loss.backward()
            
            # Gradient accumulation
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                # Gradient clipping
                if self.use_mixed_precision and hasattr(self, 'scaler'):
                    try:
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è Mixed precision optimizer –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                        print("   ‚ÑπÔ∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ mixed precision")
                        self.use_mixed_precision = False
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                        self.optimizer.step()
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                    self.optimizer.step()
                
                self.optimizer.zero_grad()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_loss += loss.item() * self.gradient_accumulation_steps
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º progress bar
            pbar.set_postfix({
                'Loss': f'{loss.item() * self.gradient_accumulation_steps:.4f}',
                'Acc': f'{100.*correct/total:.2f}%',
                'LR': f'{self.scheduler.get_last_lr()[0]:.6f}'
            })
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 50 –±–∞—Ç—á–µ–π
            if batch_idx % 50 == 0 and self.device.type == 'cuda':
                torch.cuda.empty_cache()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º scheduler
        self.scheduler.step()
        
        return total_loss / len(self.train_loader), 100. * correct / total
    
    def validate(self, epoch: int):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ device
                features = batch['features'].to(self.device, non_blocking=True)
                labels = batch['labels'].to(self.device, non_blocking=True)
                
                # Forward pass (—Å mixed precision –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
                if self.use_mixed_precision:
                    with autocast('cuda'):
                        outputs = self.model(features)
                        loss = self.criterion(outputs, labels)
                else:
                    outputs = self.model(features)
                    loss = self.criterion(outputs, labels)
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        return total_loss / len(self.val_loader), 100. * correct / total
    
    def save_checkpoint(self, epoch: int, val_acc: float, is_best: bool = False):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_acc': val_acc,
            'config': {
                'max_len': self.max_len,
                'batch_size': self.batch_size,
                'dim': self.dim,
                'lr': self.lr,
                'use_augmentations': self.use_augmentations,
                'use_mixed_precision': self.use_mixed_precision,
                'gradient_clip_val': self.gradient_clip_val,
                'gradient_accumulation_steps': self.gradient_accumulation_steps
            }
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler –¥–ª—è mixed precision
        if self.use_mixed_precision:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        checkpoint_path = self.model_dir / f"checkpoint_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        if is_best:
            best_path = self.model_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç: {best_path}")
    
    def train(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è RTX 4070)...")
        print(f"   –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        best_val_acc = 0
        train_losses = []
        train_accs = []
        val_losses = []
        val_accs = []
        
        for epoch in range(self.epochs):
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_acc = self.train_epoch(epoch)
            train_losses.append(train_loss)
            train_accs.append(train_acc)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_acc = self.validate(epoch)
            val_losses.append(val_loss)
            val_accs.append(val_acc)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            print(f"Epoch {epoch+1}/{self.epochs}:")
            print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            print(f"  LR: {self.scheduler.get_last_lr()[0]:.6f}")
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏ GPU
            if self.device.type == 'cuda':
                gpu_memory_used = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_cached = torch.cuda.memory_reserved() / 1024**3
                print(f"  GPU Memory: {gpu_memory_used:.1f}GB used, {gpu_memory_cached:.1f}GB cached")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
            is_best = val_acc > best_val_acc
            if is_best:
                best_val_acc = val_acc
                print(f"üéâ –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {val_acc:.2f}%")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 50 —ç–ø–æ—Ö –∏–ª–∏ –µ—Å–ª–∏ —ç—Ç–æ –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if (epoch + 1) % 50 == 0 or is_best:
                self.save_checkpoint(epoch, val_acc, is_best)
            
            # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ (–µ—Å–ª–∏ –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π 50 —ç–ø–æ—Ö)
            if epoch > 50 and max(val_accs[-50:]) < best_val_acc:
                print(f"‚èπÔ∏è –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                break
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        self.save_checkpoint(self.epochs-1, val_acc)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
        history = {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'val_losses': val_losses,
            'val_accs': val_accs,
            'best_val_acc': best_val_acc
        }
        
        history_path = self.model_dir / "training_history.json"
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2)
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"   –õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.2f}%")
        print(f"   –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        cache_stats = self.preprocessor.get_cache_stats()
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞:")
        print(f"   Hit rate: {cache_stats['hit_rate']:.1%}")
        print(f"   Cache hits: {cache_stats['cache_hits']:,}")
        print(f"   Cache misses: {cache_stats['cache_misses']:,}")
        print(f"   –§–∞–π–ª–æ–≤ –≤ –∫—ç—à–µ: {cache_stats['cache_size']}/{cache_stats['max_cache_size']}")
        
        return history

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è RTX 4070"""
    print("ü§ü –û–ë–£–ß–ï–ù–ò–ï ASL –ú–û–î–ï–õ–ò (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è RTX 4070)")
    print("=" * 70)
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è RTX 4070
    trainer = ASLTrainer(
        data_dir="../data/google_asl_signs",
        model_dir="models",
        max_len=384,
        batch_size=12,  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è RTX 4070 12GB VRAM
        dim=192,  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 384 –¥–ª—è –±–æ–ª—å—à–µ–π –º–æ–¥–µ–ª–∏
        lr=5e-4,
        epochs=400,
        use_augmentations=True,  # –í–∫–ª—é—á–∞–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–∞–∫ —É –ø–æ–±–µ–¥–∏—Ç–µ–ª—è
        use_mixed_precision=True,  # Mixed precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        gradient_clip_val=1.0,  # Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        gradient_accumulation_steps=4  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size = 48
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    history = trainer.train()
    
    print("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")

if __name__ == "__main__":
    main() 
# train.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.amp import autocast
from torch.cuda.amp import GradScaler
import numpy as np
from tqdm import tqdm
import time
import os
from pathlib import Path
import json
from datetime import datetime
import platform

# ÐŸÐ¾Ð´Ð°Ð²Ð»ÑÐµÐ¼ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Triton Ð½Ð° Windows
if platform.system() == 'Windows':
    try:
        import torch._dynamo
        torch._dynamo.config.suppress_errors = True
        print("ðŸ”§ ÐŸÐ¾Ð´Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Triton Ð´Ð»Ñ Windows")
    except:
        pass

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° TensorFloat32 Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
if torch.cuda.is_available():
    torch.set_float32_matmul_precision('high')
    print("ðŸ”§ Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½ TensorFloat32 Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸")

from data_loader import ASLDataLoader
from preprocessing import ASLPreprocessor
from models import get_model, ASLEnsemble

class ASLTrainer:
    """Ð¢Ñ€ÐµÐ½ÐµÑ€ Ð´Ð»Ñ ASL Ð¼Ð¾Ð´ÐµÐ»Ð¸ (ÐºÐ°Ðº Ñƒ Ð¿Ð¾Ð±ÐµÐ´Ð¸Ñ‚ÐµÐ»Ñ) Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸ Ð´Ð»Ñ RTX 4070"""
    
    def __init__(self, 
                 data_dir: str = "../data/google_asl_signs",
                 model_dir: str = "models",
                 max_len: int = 384,
                 batch_size: int = 12,  # ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ Ð´Ð»Ñ RTX 4070 12GB VRAM
                 dim: int = 192,
                 lr: float = 5e-4,
                 epochs: int = 400,
                 device: str = None,
                 use_augmentations: bool = True,
                 use_mixed_precision: bool = True,  # Mixed precision Ð´Ð»Ñ RTX 4070
                 gradient_clip_val: float = 1.0,   # Gradient clipping
                 gradient_accumulation_steps: int = 4):  # Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ batch size = 48
        
        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        
        self.max_len = max_len
        self.batch_size = batch_size
        self.dim = dim
        self.lr = lr
        self.epochs = epochs
        self.use_augmentations = use_augmentations
        self.use_mixed_precision = use_mixed_precision
        self.gradient_clip_val = gradient_clip_val
        self.gradient_accumulation_steps = gradient_accumulation_steps
        
        # Device
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # CUDA Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ RTX 4070
        if self.device.type == 'cuda':
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            # ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð°Ð¼ÑÑ‚Ð¸
            torch.cuda.empty_cache()
        
        print(f"ðŸŽ¯ Ð¢Ñ€ÐµÐ½ÐµÑ€ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½ (Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð´Ð»Ñ RTX 4070):")
        print(f"   Device: {self.device}")
        print(f"   ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ð½Ð°: {max_len}")
        print(f"   Ð Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð°: {batch_size}")
        print(f"   Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ batch size: {batch_size * gradient_accumulation_steps}")
        print(f"   Ð Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {dim}")
        print(f"   Learning rate: {lr}")
        print(f"   Ð­Ð¿Ð¾Ñ…Ð¸: {epochs}")
        print(f"   ÐÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸: {'Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½Ñ‹' if use_augmentations else 'ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ñ‹'}")
        print(f"   Mixed Precision: {'Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½' if use_mixed_precision else 'ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½'}")
        print(f"   Gradient Clipping: {gradient_clip_val}")
        print(f"   Gradient Accumulation: {gradient_accumulation_steps} steps")
        
        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²
        self._setup_components()
        
    def _setup_components(self):
        """ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²"""
        # Ð—Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸Ðº Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÑÐ¼Ð¸ (Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð´Ð»Ñ Ð¿Ð°Ð¼ÑÑ‚Ð¸)
        self.dataloader = ASLDataLoader(
            data_dir=str(self.data_dir),
            batch_size=self.batch_size,
            max_len=self.max_len,
            preprocessor=None,  # DataLoader ÑÐ¾Ð·Ð´Ð°ÑÑ‚ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€ ÑÐ°Ð¼
            num_workers=4  # ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ Ð´Ð»Ñ RTX 4070
        )
        
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€ Ð¸Ð· DataLoader
        self.preprocessor = self.dataloader.preprocessor
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ DataLoader'Ñ‹ Ñ Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÑÐ¼Ð¸ Ð´Ð»Ñ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸
        self.train_loader, self.val_loader, self.test_loader = self.dataloader.get_dataloaders(
            augment_train=self.use_augmentations
        )
        
        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ñ„Ð¸Ñ‡ Ð¸Ð· Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ Ð±Ð°Ñ‚Ñ‡Ð°
        sample_batch = next(iter(self.train_loader))
        input_dim = sample_batch['features'].shape[-1]  # Ð Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ features
        
        # ÐœÐ¾Ð´ÐµÐ»ÑŒ
        self.model = get_model(
            input_dim=input_dim,
            num_classes=self.dataloader.num_classes,
            max_len=self.max_len,
            dim=self.dim
        ).to(self.device)
        
        # PyTorch 2.0+ compile Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ (Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½ Ð´Ð»Ñ Windows)
        import platform
        if hasattr(torch, 'compile') and self.device.type == 'cuda' and platform.system() != 'Windows':
            try:
                # ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ð±Ð¾Ð»ÐµÐµ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼ Ð´Ð»Ñ Linux/Mac
                self.model = torch.compile(self.model, mode='reduce-overhead')
                print("   âœ… PyTorch 2.0+ compile Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½ (reduce-overhead mode)")
            except Exception as e:
                print(f"   âš ï¸ PyTorch compile Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½: {e}")
                print("   â„¹ï¸ ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ð±ÐµÐ· compile")
                # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ compile ÐµÑÐ»Ð¸ Ð¾Ð½ Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚
                if hasattr(self.model, '_orig_mod'):
                    self.model = self.model._orig_mod
        else:
            if platform.system() == 'Windows':
                print("   â„¹ï¸ PyTorch compile Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½ Ð´Ð»Ñ Windows (Ð¸Ð·Ð±ÐµÐ³Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼ Ñ Triton)")
            else:
                print("   â„¹ï¸ PyTorch compile Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ (Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ð±ÐµÐ· Ð½ÐµÐ³Ð¾)")
        
        # Loss function (ÐºÐ°Ðº Ñƒ Ð¿Ð¾Ð±ÐµÐ´Ð¸Ñ‚ÐµÐ»Ñ)
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # Optimizer (ÐºÐ°Ðº Ñƒ Ð¿Ð¾Ð±ÐµÐ´Ð¸Ñ‚ÐµÐ»Ñ: AdamW)
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # Scheduler (ÐºÐ°Ðº Ñƒ Ð¿Ð¾Ð±ÐµÐ´Ð¸Ñ‚ÐµÐ»Ñ: CosineDecay)
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=self.epochs,
            eta_min=1e-6
        )
        
        # Mixed precision scaler
        if self.use_mixed_precision:
            self.scaler = GradScaler()
        
        # Ð’ÐµÑÐ° ÐºÐ»Ð°ÑÑÐ¾Ð² Ð´Ð»Ñ ÑÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
        self.class_weights = self.dataloader.get_class_weights('train').to(self.device)
        
        print(f"âœ… ÐšÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ñ‹:")
        print(f"   Ð Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ñ„Ð¸Ñ‡: {input_dim}")
        print(f"   ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ»Ð°ÑÑÐ¾Ð²: {self.dataloader.num_classes}")
        print(f"   ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÑÑˆÐ° Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°
        cache_stats = self.preprocessor.get_cache_stats()
        print(f"   ÐšÑÑˆ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°: {cache_stats['cache_size']}/{cache_stats['max_cache_size']} Ñ„Ð°Ð¹Ð»Ð¾Ð²")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ð°Ð¼ÑÑ‚Ð¸ GPU
        if self.device.type == 'cuda':
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   GPU Ð¿Ð°Ð¼ÑÑ‚ÑŒ: {gpu_memory:.1f} GB")
    
    def train_epoch(self, epoch: int):
        """ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ð´Ð½Ð¾Ð¹ ÑÐ¿Ð¾Ñ…Ð¸ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{self.epochs}")
        
        for batch_idx, batch in enumerate(pbar):
            # ÐŸÐµÑ€ÐµÐ¼ÐµÑ‰Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð° device
            features = batch['features'].to(self.device, non_blocking=True)
            labels = batch['labels'].to(self.device, non_blocking=True)
            
            # Mixed precision forward pass
            if self.use_mixed_precision and self.device.type == 'cuda':
                try:
                    with autocast(device_type='cuda', dtype=torch.float16):
                        outputs = self.model(features)
                        loss = self.criterion(outputs, labels)
                        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ loss Ð´Ð»Ñ gradient accumulation
                        loss = loss / self.gradient_accumulation_steps
                except Exception as e:
                    print(f"   âš ï¸ Mixed precision Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½: {e}")
                    print("   â„¹ï¸ ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ð±ÐµÐ· mixed precision")
                    self.use_mixed_precision = False
                    outputs = self.model(features)
                    loss = self.criterion(outputs, labels)
                    loss = loss / self.gradient_accumulation_steps
            else:
                outputs = self.model(features)
                loss = self.criterion(outputs, labels)
                loss = loss / self.gradient_accumulation_steps
            
            # Mixed precision backward pass
            if self.use_mixed_precision and hasattr(self, 'scaler'):
                try:
                    self.scaler.scale(loss).backward()
                except Exception as e:
                    print(f"   âš ï¸ Mixed precision backward Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½: {e}")
                    print("   â„¹ï¸ ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ð±ÐµÐ· mixed precision")
                    self.use_mixed_precision = False
                    loss.backward()
            else:
                loss.backward()
            
            # Gradient accumulation
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                # Gradient clipping
                if self.use_mixed_precision and hasattr(self, 'scaler'):
                    try:
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    except Exception as e:
                        print(f"   âš ï¸ Mixed precision optimizer Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½: {e}")
                        print("   â„¹ï¸ ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ð±ÐµÐ· mixed precision")
                        self.use_mixed_precision = False
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                        self.optimizer.step()
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                    self.optimizer.step()
                
                self.optimizer.zero_grad()
            
            # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
            total_loss += loss.item() * self.gradient_accumulation_steps
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ progress bar
            pbar.set_postfix({
                'Loss': f'{loss.item() * self.gradient_accumulation_steps:.4f}',
                'Acc': f'{100.*correct/total:.2f}%',
                'LR': f'{self.scheduler.get_last_lr()[0]:.6f}'
            })
            
            # ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð¿Ð°Ð¼ÑÑ‚Ð¸ ÐºÐ°Ð¶Ð´Ñ‹Ðµ 50 Ð±Ð°Ñ‚Ñ‡ÐµÐ¹
            if batch_idx % 50 == 0 and self.device.type == 'cuda':
                torch.cuda.empty_cache()
        
        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ scheduler
        self.scheduler.step()
        
        return total_loss / len(self.train_loader), 100. * correct / total
    
    def validate(self, epoch: int):
        """Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                # ÐŸÐµÑ€ÐµÐ¼ÐµÑ‰Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð° device
                features = batch['features'].to(self.device, non_blocking=True)
                labels = batch['labels'].to(self.device, non_blocking=True)
                
                # Forward pass (Ñ mixed precision ÐµÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½)
                if self.use_mixed_precision:
                    with autocast('cuda'):
                        outputs = self.model(features)
                        loss = self.criterion(outputs, labels)
                else:
                    outputs = self.model(features)
                    loss = self.criterion(outputs, labels)
                
                # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        return total_loss / len(self.val_loader), 100. * correct / total
    
    def save_checkpoint(self, epoch: int, val_acc: float, is_best: bool = False):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚Ð°"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_acc': val_acc,
            'config': {
                'max_len': self.max_len,
                'batch_size': self.batch_size,
                'dim': self.dim,
                'lr': self.lr,
                'use_augmentations': self.use_augmentations,
                'use_mixed_precision': self.use_mixed_precision,
                'gradient_clip_val': self.gradient_clip_val,
                'gradient_accumulation_steps': self.gradient_accumulation_steps
            }
        }
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ scaler Ð´Ð»Ñ mixed precision
        if self.use_mixed_precision:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚
        checkpoint_path = self.model_dir / f"checkpoint_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚
        if is_best:
            best_path = self.model_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚: {best_path}")
    
    def train(self):
        """ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ†Ð¸ÐºÐ» Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸"""
        print(f"ðŸš€ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ (Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ Ð´Ð»Ñ RTX 4070)...")
        print(f"   Ð’Ñ€ÐµÐ¼Ñ Ð½Ð°Ñ‡Ð°Ð»Ð°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        best_val_acc = 0
        train_losses = []
        train_accs = []
        val_losses = []
        val_accs = []
        
        for epoch in range(self.epochs):
            # ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
            train_loss, train_acc = self.train_epoch(epoch)
            train_losses.append(train_loss)
            train_accs.append(train_acc)
            
            # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ
            val_loss, val_acc = self.validate(epoch)
            val_losses.append(val_loss)
            val_accs.append(val_acc)
            
            # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
            print(f"Epoch {epoch+1}/{self.epochs}:")
            print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            print(f"  LR: {self.scheduler.get_last_lr()[0]:.6f}")
            
            # Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸ GPU
            if self.device.type == 'cuda':
                gpu_memory_used = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_cached = torch.cuda.memory_reserved() / 1024**3
                print(f"  GPU Memory: {gpu_memory_used:.1f}GB used, {gpu_memory_cached:.1f}GB cached")
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚Ð°
            is_best = val_acc > best_val_acc
            if is_best:
                best_val_acc = val_acc
                print(f"ðŸŽ‰ ÐÐ¾Ð²Ñ‹Ð¹ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: {val_acc:.2f}%")
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ðµ 50 ÑÐ¿Ð¾Ñ… Ð¸Ð»Ð¸ ÐµÑÐ»Ð¸ ÑÑ‚Ð¾ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
            if (epoch + 1) % 50 == 0 or is_best:
                self.save_checkpoint(epoch, val_acc, is_best)
            
            # Ð Ð°Ð½Ð½ÑÑ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° (ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ð¹ 50 ÑÐ¿Ð¾Ñ…)
            if epoch > 50 and max(val_accs[-50:]) < best_val_acc:
                print(f"â¹ï¸ Ð Ð°Ð½Ð½ÑÑ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð½Ð° ÑÐ¿Ð¾Ñ…Ðµ {epoch+1}")
                break
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
        self.save_checkpoint(self.epochs-1, val_acc)
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
        history = {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'val_losses': val_losses,
            'val_accs': val_accs,
            'best_val_acc': best_val_acc
        }
        
        history_path = self.model_dir / "training_history.json"
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2)
        
        print(f"âœ… ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾!")
        print(f"   Ð›ÑƒÑ‡ÑˆÐ°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ: {best_val_acc:.2f}%")
        print(f"   Ð’Ñ€ÐµÐ¼Ñ Ð¾ÐºÐ¾Ð½Ñ‡Ð°Ð½Ð¸Ñ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÑÑˆÐ° Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°
        cache_stats = self.preprocessor.get_cache_stats()
        print(f"ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÑÑˆÐ° Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°:")
        print(f"   Hit rate: {cache_stats['hit_rate']:.1%}")
        print(f"   Cache hits: {cache_stats['cache_hits']:,}")
        print(f"   Cache misses: {cache_stats['cache_misses']:,}")
        print(f"   Ð¤Ð°Ð¹Ð»Ð¾Ð² Ð² ÐºÑÑˆÐµ: {cache_stats['cache_size']}/{cache_stats['max_cache_size']}")
        
        return history

def main():
    """ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸ Ð´Ð»Ñ RTX 4070"""
    print("ðŸ¤Ÿ ÐžÐ‘Ð£Ð§Ð•ÐÐ˜Ð• ASL ÐœÐžÐ”Ð•Ð›Ð˜ (Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ Ð´Ð»Ñ RTX 4070)")
    print("=" * 70)
    
    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ñ‚Ñ€ÐµÐ½ÐµÑ€ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸ Ð´Ð»Ñ RTX 4070
    trainer = ASLTrainer(
        data_dir="../data/google_asl_signs",
        model_dir="models",
        max_len=384,
        batch_size=12,  # ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ Ð´Ð»Ñ RTX 4070 12GB VRAM
        dim=192,  # ÐœÐ¾Ð¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ 384 Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        lr=5e-4,
        epochs=400,
        use_augmentations=True,  # Ð’ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ ÐºÐ°Ðº Ñƒ Ð¿Ð¾Ð±ÐµÐ´Ð¸Ñ‚ÐµÐ»Ñ
        use_mixed_precision=True,  # Mixed precision Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ
        gradient_clip_val=1.0,  # Gradient clipping Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸
        gradient_accumulation_steps=4  # Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ batch size = 48
    )
    
    # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
    history = trainer.train()
    
    print("ðŸŽ‰ ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾!")

if __name__ == "__main__":
    main() 
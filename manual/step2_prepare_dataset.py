import torch
from torch import nn
import numpy as np
import math
import pandas as pd
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

np.random.seed(1)
torch.manual_seed(2)  # we set up a seed so that your output matches ours although the initialization is random.
dtype = torch.float
dtype_long = torch.long
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

print(f"Using device: {device}")

def load_dataset(data_dir: str = "dataset25_split", max_len: int = 384, max_samples: Optional[int] = None):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç Google ASL Signs dataset –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    
    Args:
        data_dir: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏ (dataset25_split)
        max_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–¥—Ä–æ–≤
        max_samples: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (None = –≤—Å–µ)
    
    Returns:
        train_data: –°–ø–∏—Å–æ–∫ —Ç–µ–Ω–∑–æ—Ä–æ–≤ landmarks –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        train_labels: –°–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏  
        test_data: –°–ø–∏—Å–æ–∫ —Ç–µ–Ω–∑–æ—Ä–æ–≤ landmarks –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_labels: –°–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        sign_mapping: –°–ª–æ–≤–∞—Ä—å –º–∞–ø–ø–∏–Ω–≥–∞ –∑–Ω–∞–∫–æ–≤ –∫ –∏–Ω–¥–µ–∫—Å–∞–º
        classes: –°–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ (–∑–Ω–∞–∫–æ–≤)
    """
    print("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ Google ASL Signs dataset (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)...")
    
    data_path = Path(data_dir)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    if not (data_path / "train").exists():
        raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {data_path / 'train'} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    if not (data_path / "test").exists():
        raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {data_path / 'test'} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞–∫–æ–≤
    sign_mapping_path = data_path / "train" / "sign_to_prediction_index_map.json"
    if not sign_mapping_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª {sign_mapping_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    with open(sign_mapping_path, 'r') as f:
        sign_mapping = json.load(f)
    
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sign_mapping)} –∑–Ω–∞–∫–æ–≤")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º train –¥–∞–Ω–Ω—ã–µ
    print("\nüìÅ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    train_csv_path = data_path / "train" / "train.csv"
    if not train_csv_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª {train_csv_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    train_df = pd.read_csv(train_csv_path)
    print(f"   –ù–∞–π–¥–µ–Ω–æ {len(train_df)} –∑–∞–ø–∏—Å–µ–π –≤ train.csv")
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
    if max_samples and len(train_df) > max_samples:
        train_df = train_df.sample(n=max_samples, random_state=42).reset_index(drop=True)
        print(f"   –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {max_samples} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    train_data, train_labels = _load_sequences_optimized(train_df, data_path / "train", sign_mapping, max_len)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º test –¥–∞–Ω–Ω—ã–µ
    print("\nüìÅ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_csv_path = data_path / "test" / "train.csv"
    if not test_csv_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª {test_csv_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    test_df = pd.read_csv(test_csv_path)
    print(f"   –ù–∞–π–¥–µ–Ω–æ {len(test_df)} –∑–∞–ø–∏—Å–µ–π –≤ test/train.csv")
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
    if max_samples and len(test_df) > max_samples:
        test_df = test_df.sample(n=max_samples, random_state=42).reset_index(drop=True)
        print(f"   –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {max_samples} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    test_data, test_labels = _load_sequences_optimized(test_df, data_path / "test", sign_mapping, max_len)
    
    # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤
    classes = list(sign_mapping.keys())
    
    print(f"\n‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"   –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(train_data)}")
    print(f"   –¢–µ—Å—Ç–æ–≤—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(test_data)}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(classes)}")
    print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_len}")
    
    return train_data, train_labels, test_data, test_labels, sign_mapping, classes

def _load_sequences_optimized(df: pd.DataFrame, data_dir: Path, sign_mapping: Dict[str, int], max_len: int) -> Tuple[List[torch.Tensor], List[int]]:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π landmarks
    """
    sequences = []
    labels = []
    
    total_files = len(df)
    successful_loads = 0
    failed_loads = 0
    
    print(f"   –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {total_files} —Ñ–∞–π–ª–æ–≤...")
    
    for idx, row in df.iterrows():
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 —Ñ–∞–π–ª–æ–≤
        if idx % 50 == 0:
            print(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {idx}/{total_files} (—É—Å–ø–µ—à–Ω–æ: {successful_loads}, –æ—à–∏–±–æ–∫: {failed_loads})")
        
        file_path = data_dir / row['path']
        sign = row['sign']
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        if not file_path.exists():
            print(f"   ‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            failed_loads += 1
            continue
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º parquet —Ñ–∞–π–ª —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞–º—è—Ç–∏
            landmarks_df = pd.read_parquet(file_path, engine='pyarrow')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –ø—É—Å—Ç–æ–π
            if len(landmarks_df) == 0:
                print(f"   ‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ñ–∞–π–ª: {file_path}")
                failed_loads += 1
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            required_columns = ['frame', 'landmark_index', 'x', 'y', 'z']
            if not all(col in landmarks_df.columns for col in required_columns):
                print(f"   ‚ö†Ô∏è –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–∞: {file_path}")
                failed_loads += 1
                continue
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä landmarks
            sequence_tensor = _parquet_to_tensor_optimized(landmarks_df, max_len)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–Ω–∑–æ—Ä –Ω–µ –ø—É—Å—Ç–æ–π
            if sequence_tensor.shape[0] == 0:
                print(f"   ‚ö†Ô∏è –ü—É—Å—Ç–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {file_path}")
                failed_loads += 1
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∫—É
            label = sign_mapping[sign]
            
            sequences.append(sequence_tensor)
            labels.append(label)
            successful_loads += 1
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {str(e)[:100]}...")
            failed_loads += 1
            continue
    
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: —É—Å–ø–µ—à–Ω–æ {successful_loads}, –æ—à–∏–±–æ–∫ {failed_loads}")
    
    return sequences, labels

def _parquet_to_tensor_optimized(landmarks_df: pd.DataFrame, max_len: int) -> torch.Tensor:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ parquet DataFrame –≤ —Ç–µ–Ω–∑–æ—Ä landmarks
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–∞–¥—Ä—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Ö
        frames = sorted(landmarks_df['frame'].unique())
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤
        frames = frames[:max_len]
        
        if len(frames) == 0:
            return torch.zeros((0, 0, 3), dtype=torch.float32)
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É landmarks —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–æ–≤
        # Face: 468 landmarks (0-467)
        # Pose: 33 landmarks (468-500)
        # Left Hand: 21 landmarks (501-521)
        # Right Hand: 21 landmarks (522-542)
        # –í—Å–µ–≥–æ: 543 landmarks
        
        total_landmarks = 543
        landmarks_tensor = torch.zeros((len(frames), total_landmarks, 3), dtype=torch.float32)
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–∞
        for frame_idx, frame in enumerate(frames):
            # –î–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∫–∞–¥—Ä–∞
            frame_data = landmarks_df[landmarks_df['frame'] == frame]
            
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–æ–≤
            frame_dict = {}
            for _, row in frame_data.iterrows():
                landmark_idx = row['landmark_index']
                landmark_type = row['type']
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤ —Ç–µ–Ω–∑–æ—Ä–µ
                if landmark_type == 'face':
                    tensor_idx = landmark_idx  # 0-467
                elif landmark_type == 'pose':
                    tensor_idx = 468 + landmark_idx  # 468-500
                elif landmark_type == 'left_hand':
                    tensor_idx = 501 + landmark_idx  # 501-521
                elif landmark_type == 'right_hand':
                    tensor_idx = 522 + landmark_idx  # 522-542
                else:
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ç–∏–ø—ã
                
                frame_dict[tensor_idx] = [row['x'], row['y'], row['z']]
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º —Ç–µ–Ω–∑–æ—Ä
            for tensor_idx in range(total_landmarks):
                if tensor_idx in frame_dict:
                    coords = frame_dict[tensor_idx]
                    landmarks_tensor[frame_idx, tensor_idx] = torch.tensor(coords, dtype=torch.float32)
        
        return landmarks_tensor
        
    except Exception as e:
        print(f"   –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ –≤ —Ç–µ–Ω–∑–æ—Ä: {str(e)[:100]}...")
        return torch.zeros((0, 0, 3), dtype=torch.float32)

def get_dataset_statistics(train_data: List[torch.Tensor], test_data: List[torch.Tensor], sign_mapping: Dict[str, int]):
    """
    –í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    """
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
    train_lengths = [seq.shape[0] for seq in train_data]
    test_lengths = [seq.shape[0] for seq in test_data]
    
    print(f"   –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
    print(f"     –ú–∏–Ω–∏–º—É–º –∫–∞–¥—Ä–æ–≤: {min(train_lengths)}")
    print(f"     –ú–∞–∫—Å–∏–º—É–º –∫–∞–¥—Ä–æ–≤: {max(train_lengths)}")
    print(f"     –°—Ä–µ–¥–Ω–µ–µ –∫–∞–¥—Ä–æ–≤: {np.mean(train_lengths):.1f}")
    print(f"     –ú–µ–¥–∏–∞–Ω–∞ –∫–∞–¥—Ä–æ–≤: {np.median(train_lengths):.1f}")
    
    print(f"   –¢–µ—Å—Ç–æ–≤—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
    print(f"     –ú–∏–Ω–∏–º—É–º –∫–∞–¥—Ä–æ–≤: {min(test_lengths)}")
    print(f"     –ú–∞–∫—Å–∏–º—É–º –∫–∞–¥—Ä–æ–≤: {max(test_lengths)}")
    print(f"     –°—Ä–µ–¥–Ω–µ–µ –∫–∞–¥—Ä–æ–≤: {np.mean(test_lengths):.1f}")
    print(f"     –ú–µ–¥–∏–∞–Ω–∞ –∫–∞–¥—Ä–æ–≤: {np.median(test_lengths):.1f}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ landmarks
    if train_data:
        sample_seq = train_data[0]
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å landmarks: {sample_seq.shape[1]} —Ç–æ—á–µ–∫")
        print(f"   –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã: x, y, z (3 –∏–∑–º–µ—Ä–µ–Ω–∏—è)")
        print(f"   –°—Ç—Ä—É–∫—Ç—É—Ä–∞ landmarks:")
        print(f"     - Face: 468 —Ç–æ—á–µ–∫ (0-467)")
        print(f"     - Pose: 33 —Ç–æ—á–∫–∏ (468-500)")
        print(f"     - Left Hand: 21 —Ç–æ—á–∫–∞ (501-521)")
        print(f"     - Right Hand: 21 —Ç–æ—á–∫–∞ (522-542)")
        print(f"     - –í—Å–µ–≥–æ: 543 —Ç–æ—á–∫–∏")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(sign_mapping)}")
    print(f"   –ö–ª–∞—Å—Å—ã: {list(sign_mapping.keys())}")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ–±—Ä–∞–∑—Ü–æ–≤...")
    train_data, train_labels, test_data, test_labels, sign_mapping, classes = load_dataset(max_samples=100)
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    get_dataset_statistics(train_data, test_data, sign_mapping)
    
    print(f"\nüéØ –ì–æ—Ç–æ–≤–æ –∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ!")
    print(f"   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ train_data –∏ train_labels –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏")
    print(f"   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ test_data –∏ test_labels –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
    print(f"   –ö–∞–∂–¥–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å - —ç—Ç–æ –∞–Ω–∏–º–∞—Ü–∏—è –∂–µ—Å—Ç–∞ MediaPipe")
    print(f"\nüí° –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: load_dataset(max_samples=None)")






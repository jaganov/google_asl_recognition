import cv2
import mediapipe as mp
import numpy as np
import torch
import json
import time
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import math
import os
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –∏ –∫–ª–∞—Å—Å—ã –∏–∑ step3
from step3_prepare_train import (
    ASLModel, 
    PreprocessingLayer, 
    AdaptiveDropout,
    TemporalConvBlock,
    BidirectionalLSTM,
    TemporalAttention,
    Conv1DBlock,
    TransformerBlock
)
from step2_prepare_dataset import load_dataset

class LiveASLRecognition:
    """–ñ–∏–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ASL –∂–µ—Å—Ç–æ–≤"""
    
    def __init__(self, 
                 model_path: str = "models/asl_model_v20250720_080209.pth",
                 camera_id: int = 0,
                 target_frames: int = 16,
                 use_only_face: bool = False,
                 confidence_threshold: float = 0.8,
                 save_screenshots: bool = True):
        """
        Args:
            model_path: –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            camera_id: ID –∫–∞–º–µ—Ä—ã
            target_frames: –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤
            use_only_face: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ face landmarks
            confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–∞ (0.8 = 80%)
            save_screenshots: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        """
        self.camera_id = camera_id
        self.target_frames = target_frames
        self.use_only_face = use_only_face
        self.confidence_threshold = confidence_threshold
        self.save_screenshots = save_screenshots
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
        self.screenshots_dir = Path("screenshots")
        self.screenshots_dir.mkdir(exist_ok=True)
        
        # –°—á–µ—Ç—á–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∂–µ—Å—Ç–∞
        self.screenshot_counters = {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MediaPipe
        self.mp_holistic = mp.solutions.holistic
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        self.holistic = self.mp_holistic.Holistic(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5,
            model_complexity=1
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model, self.sign_mapping, self.classes = self._load_model(model_path)
        
        # –ë—É—Ñ–µ—Ä –¥–ª—è –∫–∞–¥—Ä–æ–≤
        self.frame_buffer = []
        self.max_buffer_size = 30
        
        print(f"üéØ Live ASL Recognition –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        print(f"   –ú–æ–¥–µ–ª—å: {model_path}")
        print(f"   –ö–ª–∞—Å—Å–æ–≤: {len(self.classes)}")
        print(f"   –¶–µ–ª–µ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤: {target_frames}")
        print(f"   –¢–æ–ª—å–∫–æ face landmarks: {use_only_face}")
        print(f"   –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {confidence_threshold:.1%}")
        print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤: {save_screenshots}")
        if save_screenshots:
            print(f"   –ü–∞–ø–∫–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤: {self.screenshots_dir.absolute()}")
    
    def _load_model(self, model_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –º–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞–∫–æ–≤"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞–∫–æ–≤
        sign_mapping_path = "dataset25/sign_to_prediction_index_map.json"
        if Path(sign_mapping_path).exists():
            with open(sign_mapping_path, 'r') as f:
                sign_mapping = json.load(f)
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
            classes = [""] * len(sign_mapping)
            for sign, idx in sign_mapping.items():
                classes[idx] = sign
        else:
            print(f"‚ö†Ô∏è –ú–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {sign_mapping_path}")
            sign_mapping = {}
            classes = [f"class_{i}" for i in range(25)]
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–∞
        try:
            train_data, train_labels, test_data, test_labels, _, _ = load_dataset(max_samples=1)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞ (–ø–æ—Å–ª–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ preprocessing)
            sample_sequence = train_data[0]
            sample_preprocessed = PreprocessingLayer(max_len=384)(sample_sequence.unsqueeze(0))
            input_dim = sample_preprocessed.shape[-1]
            
            print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞ –ø–æ—Å–ª–µ preprocessing: {input_dim}")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            print("   –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 744")
            input_dim = 744
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º checkpoint –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        if Path(model_path).exists():
            checkpoint = torch.load(model_path, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            else:
                state_dict = checkpoint
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏
            print("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
            model_keys = list(state_dict.keys())
            print(f"   –ö–ª—é—á–∏ –≤ –º–æ–¥–µ–ª–∏: {model_keys[:10]}...")  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 –∫–ª—é—á–µ–π
            
            # –°–æ–∑–¥–∞–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
            model = self._create_compatible_model_from_state_dict(input_dim, len(classes), state_dict)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
            model.load_state_dict(state_dict)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
        else:
            print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
            return None, sign_mapping, classes
        
        model.eval()
        return model, sign_mapping, classes
    
    def _create_compatible_model_from_state_dict(self, input_dim: int, num_classes: int, state_dict: dict):
        """–°–æ–∑–¥–∞–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ state_dict"""
        class CompatibleASLModel(nn.Module):
            def __init__(self, input_dim, num_classes, max_len=384, dim=192):
                super().__init__()
                self.max_len = max_len
                self.dim = dim
                
                # Preprocessing
                self.preprocessing = PreprocessingLayer(max_len)
                
                # Stem with improved initialization
                self.stem_conv = nn.Linear(input_dim, dim, bias=False)
                self.stem_bn = nn.BatchNorm1d(dim, momentum=0.95)
                self.stem_dropout = AdaptiveDropout(initial_p=0.1, final_p=0.3, warmup_epochs=20)
                
                # Temporal Convolutional Network (TCN) - 3 blocks with different dilations
                self.tcn1 = TemporalConvBlock(dim, kernel_size=17, dilation=1, drop_rate=0.15)
                self.tcn2 = TemporalConvBlock(dim, kernel_size=17, dilation=2, drop_rate=0.2)
                self.tcn3 = TemporalConvBlock(dim, kernel_size=17, dilation=4, drop_rate=0.25)
                
                # Projection for attention pooling
                self.attention_projection = nn.Linear(dim, dim)
                
                # Bidirectional LSTM - 2 layers for better dependency capture
                self.lstm = BidirectionalLSTM(dim, hidden_dim=dim//2, num_layers=2, drop_rate=0.15)
                
                # Temporal Attention - more heads for better attention
                self.temporal_attention = TemporalAttention(dim, num_heads=8, drop_rate=0.15)
                
                # 1D CNN + Transformer blocks - 3 conv blocks
                ksize = 17
                self.conv1 = Conv1DBlock(dim, ksize, drop_rate=0.15)
                self.conv2 = Conv1DBlock(dim, ksize, drop_rate=0.2)
                self.conv3 = Conv1DBlock(dim, ksize, drop_rate=0.25)
                self.transformer1 = TransformerBlock(dim, expand=2, drop_rate=0.15)
                
                # Top layers with improved pooling
                self.top_conv = nn.Linear(dim, dim)
                self.top_bn = nn.BatchNorm1d(dim, momentum=0.95)
                self.adaptive_dropout = AdaptiveDropout(initial_p=0.2, final_p=0.5, warmup_epochs=25)
                
                # Improved pooling - add attention pooling
                self.temporal_pool1 = nn.AdaptiveAvgPool1d(1)
                self.temporal_pool2 = nn.AdaptiveMaxPool1d(1)
                self.attention_pool = nn.MultiheadAttention(dim, num_heads=4, batch_first=True, dropout=0.1)
                
                # Final classifier with improved architecture
                self.classifier = nn.Sequential(
                    nn.Linear(dim * 3, dim),  # dim*3 for avg + max + attention pooling
                    nn.BatchNorm1d(dim),
                    nn.SiLU(),
                    nn.Dropout(0.3),
                    nn.Linear(dim, num_classes)
                )
                
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
                self._init_weights()
            
            def _init_weights(self):
                for m in self.modules():
                    if isinstance(m, nn.Linear):
                        init.xavier_uniform_(m.weight)
                        if m.bias is not None:
                            init.zeros_(m.bias)
                    elif isinstance(m, nn.Conv1d):
                        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        if m.bias is not None:
                            init.zeros_(m.bias)
                    elif isinstance(m, nn.BatchNorm1d):
                        init.ones_(m.weight)
                        init.zeros_(m.bias)
                    elif isinstance(m, nn.LSTM):
                        for name, param in m.named_parameters():
                            if 'weight' in name:
                                init.orthogonal_(param)
                            elif 'bias' in name:
                                init.zeros_(param)
            
            def forward(self, x):
                # x: (batch, seq, landmarks, 3)
                
                # Preprocessing
                x = self.preprocessing(x)  # (batch, seq, features)
                
                # Stem with adaptive dropout
                x = self.stem_conv(x)
                x = self.stem_bn(x.transpose(1, 2)).transpose(1, 2)
                x = self.stem_dropout(x)
                
                # TCN blocks - 3 blocks with different dilations
                x = self.tcn1(x)
                x = self.tcn2(x)
                x = self.tcn3(x)
                
                # Bidirectional LSTM - 2 layers
                x = self.lstm(x)
                
                # Temporal Attention - more heads
                x = self.temporal_attention(x)
                
                # 1D CNN + Transformer - 3 conv blocks
                x = self.conv1(x)
                x = self.conv2(x)
                x = self.conv3(x)
                x = self.transformer1(x)
                
                # Top layers with improved pooling
                x = self.top_conv(x)  # (batch, seq, dim)
                x = self.top_bn(x.transpose(1, 2)).transpose(1, 2)
                x = self.adaptive_dropout(x)
                
                # Improved temporal pooling
                x_transposed = x.transpose(1, 2)  # (batch, dim, seq)
                
                # Global average pooling
                global_avg = self.temporal_pool1(x_transposed).squeeze(-1)  # (batch, dim)
                
                # Global max pooling
                global_max = self.temporal_pool2(x_transposed).squeeze(-1)  # (batch, dim)
                
                # Attention pooling - use projection for stability
                x_for_attn = self.attention_projection(x)  # x already has dimension (batch, seq, dim)
                
                attn_out, _ = self.attention_pool(x_for_attn, x_for_attn, x_for_attn)
                global_attn = torch.mean(attn_out, dim=1)  # (batch, dim)
                
                # Combine pooling results
                x_pooled = torch.cat([global_avg, global_max, global_attn], dim=1)  # (batch, dim*3)
                
                x = self.classifier(x_pooled)
                
                return x
        
        return CompatibleASLModel(input_dim, num_classes, max_len=384, dim=192).to(self.device)
    
    def _extract_landmarks(self, results) -> Optional[np.ndarray]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç landmarks –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ MediaPipe"""
        landmarks = []
        
        if self.use_only_face:
            if results.face_landmarks:
                for landmark in results.face_landmarks.landmark:
                    landmarks.extend([landmark.x, landmark.y, landmark.z])
        else:
            # Face landmarks (468 —Ç–æ—á–µ–∫)
            if results.face_landmarks:
                for landmark in results.face_landmarks.landmark:
                    landmarks.extend([landmark.x, landmark.y, landmark.z])
            
            # Pose landmarks (33 —Ç–æ—á–∫–∏)
            if results.pose_landmarks:
                for landmark in results.pose_landmarks.landmark:
                    landmarks.extend([landmark.x, landmark.y, landmark.z])
            
            # Left hand landmarks (21 —Ç–æ—á–∫–∞)
            if results.left_hand_landmarks:
                for landmark in results.left_hand_landmarks.landmark:
                    landmarks.extend([landmark.x, landmark.y, landmark.z])
            
            # Right hand landmarks (21 —Ç–æ—á–∫–∞)
            if results.right_hand_landmarks:
                for landmark in results.right_hand_landmarks.landmark:
                    landmarks.extend([landmark.x, landmark.y, landmark.z])
        
        if landmarks:
            return np.array(landmarks).reshape(-1, 3)
        return None
    
    def _draw_landmarks(self, frame: np.ndarray, results) -> np.ndarray:
        """–†–∏—Å—É–µ—Ç landmarks –Ω–∞ –∫–∞–¥—Ä–µ"""
        annotated_frame = frame.copy()
        
        # –†–∏—Å—É–µ–º face landmarks
        if results.face_landmarks:
            self.mp_drawing.draw_landmarks(
                annotated_frame, results.face_landmarks, self.mp_holistic.FACEMESH_CONTOURS,
                landmark_drawing_spec=None,
                connection_drawing_spec=self.mp_drawing_styles.get_default_face_mesh_contours_style()
            )
        
        # –†–∏—Å—É–µ–º pose landmarks (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ face)
        if not self.use_only_face and results.pose_landmarks:
            self.mp_drawing.draw_landmarks(
                annotated_frame, results.pose_landmarks, self.mp_holistic.POSE_CONNECTIONS,
                landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style()
            )
        
        # –†–∏—Å—É–µ–º left hand landmarks (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ face)
        if not self.use_only_face and results.left_hand_landmarks:
            self.mp_drawing.draw_landmarks(
                annotated_frame, results.left_hand_landmarks, self.mp_holistic.HAND_CONNECTIONS,
                landmark_drawing_spec=self.mp_drawing_styles.get_default_hand_landmarks_style(),
                connection_drawing_spec=self.mp_drawing_styles.get_default_hand_connections_style()
            )
        
        # –†–∏—Å—É–µ–º right hand landmarks (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ face)
        if not self.use_only_face and results.right_hand_landmarks:
            self.mp_drawing.draw_landmarks(
                annotated_frame, results.right_hand_landmarks, self.mp_holistic.HAND_CONNECTIONS,
                landmark_drawing_spec=self.mp_drawing_styles.get_default_hand_landmarks_style(),
                connection_drawing_spec=self.mp_drawing_styles.get_default_hand_connections_style()
            )
        
        return annotated_frame
    
    def _normalize_landmarks(self, landmarks_data: List[np.ndarray]) -> List[np.ndarray]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç landmarks"""
        if not landmarks_data:
            return []
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤
        if len(landmarks_data) > self.target_frames:
            indices = np.linspace(0, len(landmarks_data) - 1, self.target_frames, dtype=int)
            landmarks_data = [landmarks_data[i] for i in indices]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ landmarks
        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç 543 landmarks (468 face + 33 pose + 21 left hand + 21 right hand)
        expected_landmarks = 543
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É landmarks
        normalized_data = []
        for frame in landmarks_data:
            if len(frame) >= expected_landmarks:
                normalized_frame = frame[:expected_landmarks]
            else:
                # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç landmarks
                normalized_frame = np.zeros((expected_landmarks, 3))
                normalized_frame[:len(frame)] = frame
            
            normalized_data.append(normalized_frame)
        
        return normalized_data
    
    def _predict_gesture(self, landmarks_data: List[np.ndarray]) -> List[Tuple[str, float]]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∂–µ—Å—Ç"""
        if not landmarks_data or len(landmarks_data) < 5:
            return []
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        normalized_data = self._normalize_landmarks(landmarks_data)
        
        if not normalized_data:
            return []
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        # –§–æ—Ä–º–∞—Ç: (batch, seq, landmarks, 3)
        tensor = torch.tensor(normalized_data, dtype=torch.float32).unsqueeze(0)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º
        with torch.no_grad():
            predictions = self.model(tensor.to(self.device))
            probabilities = torch.softmax(predictions, dim=1)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        top_probs, top_indices = torch.topk(probabilities, 3, dim=1)
        
        results = []
        for i in range(3):
            idx = top_indices[0][i].item()
            prob = top_probs[0][i].item()
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∑–Ω–∞–∫–∞
            if idx < len(self.classes) and self.classes[idx]:
                sign_name = self.classes[idx]
            else:
                sign_name = f"Unknown_{idx}"
            
            results.append((sign_name, prob))
        
        return results
    
    def _save_screenshot(self, frame: np.ndarray, predictions: List[Tuple[str, float]]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏"""
        if not self.save_screenshots or not predictions:
            return
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        top_prediction = predictions[0]
        sign_name, confidence = top_prediction
        
        if confidence < self.confidence_threshold:
            return
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –∫–∞–¥—Ä–∞ –¥–ª—è —Å–∫—Ä–∏–Ω—à–æ—Ç–∞
        screenshot = frame.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –Ω–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç
        y_offset = 30
        cv2.putText(screenshot, "ASL Recognition Results:", 
                   (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        y_offset += 40
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        cv2.putText(screenshot, f"Time: {timestamp}", 
                   (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        y_offset += 30
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        cv2.putText(screenshot, "Top 3 Predictions:", 
                   (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        y_offset += 30
        
        for i, (sign, prob) in enumerate(predictions):
            # –¶–≤–µ—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            if prob >= self.confidence_threshold:
                color = (0, 255, 0)  # –ó–µ–ª–µ–Ω—ã–π –¥–ª—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö
            elif i == 0:
                color = (255, 255, 0)  # –ñ–µ–ª—Ç—ã–π –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ
            else:
                color = (0, 255, 255)  # –ì–æ–ª—É–±–æ–π –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
            
            text = f"{i+1}. {sign}: {prob:.3f} ({prob:.1%})"
            cv2.putText(screenshot, text, 
                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
            y_offset += 25
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö
        y_offset += 10
        cv2.putText(screenshot, f"Confidence Threshold: {self.confidence_threshold:.1%}", 
                   (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        y_offset += 20
        cv2.putText(screenshot, f"Frames: {len(self.frame_buffer)}", 
                   (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        if sign_name not in self.screenshot_counters:
            self.screenshot_counters[sign_name] = 0
        self.screenshot_counters[sign_name] += 1
        
        # –û—á–∏—â–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        safe_sign_name = "".join(c for c in sign_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_sign_name = safe_sign_name.replace(' ', '_')
        
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{safe_sign_name}_{timestamp_str}_{self.screenshot_counters[sign_name]:03d}.jpg"
        filepath = self.screenshots_dir / filename
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç
        success = cv2.imwrite(str(filepath), screenshot)
        if success:
            print(f"üì∏ –°–æ—Ö—Ä–∞–Ω–µ–Ω —Å–∫—Ä–∏–Ω—à–æ—Ç: {filename} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–∞: {filename}")
    
    def start_recognition(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∂–∏–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ"""
        print("\nüé¨ –ù–∞—á–∏–Ω–∞–µ–º –∂–∏–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ASL –∂–µ—Å—Ç–æ–≤...")
        print("   –ù–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞")
        
        cap = cv2.VideoCapture(self.camera_id)
        
        if not cap.isOpened():
            print("‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É")
            return
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–∞–º–µ—Ä—ã
        cap.set(cv2.CAP_PROP_FPS, 30)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        self.frame_buffer = []
        last_prediction_time = time.time()
        prediction_interval = 0.5  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 0.5 —Å–µ–∫—É–Ω–¥
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    print("‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–∞–¥—Ä–∞")
                    break
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR –≤ RGB
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–¥—Ä —á–µ—Ä–µ–∑ MediaPipe
                results = self.holistic.process(rgb_frame)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º landmarks
                frame_landmarks = self._extract_landmarks(results)
                
                if frame_landmarks is not None:
                    self.frame_buffer.append(frame_landmarks)
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
                    if len(self.frame_buffer) > self.max_buffer_size:
                        self.frame_buffer.pop(0)
                
                # –†–∏—Å—É–µ–º landmarks –Ω–∞ –∫–∞–¥—Ä–µ
                annotated_frame = self._draw_landmarks(frame, results)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∂–µ—Å—Ç –∫–∞–∂–¥—ã–µ 0.5 —Å–µ–∫—É–Ω–¥
                current_time = time.time()
                if current_time - last_prediction_time > prediction_interval and len(self.frame_buffer) >= 5:
                    predictions = self._predict_gesture(self.frame_buffer)
                    last_prediction_time = current_time
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç –µ—Å–ª–∏ –µ—Å—Ç—å –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                    if predictions and self.save_screenshots:
                        self._save_screenshot(annotated_frame, predictions)
                    
                    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –∫–∞–¥—Ä–µ
                    y_offset = 30
                    cv2.putText(annotated_frame, "Top 3 Predictions:", 
                               (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    y_offset += 30
                    
                    for i, (sign, prob) in enumerate(predictions):
                        # –¶–≤–µ—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                        if prob >= self.confidence_threshold:
                            color = (0, 255, 0)  # –ó–µ–ª–µ–Ω—ã–π –¥–ª—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö
                            thickness = 3  # –¢–æ–ª—â–µ –¥–ª—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö
                        elif i == 0:
                            color = (255, 255, 0)  # –ñ–µ–ª—Ç—ã–π –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ
                            thickness = 2
                        else:
                            color = (0, 255, 255)  # –ì–æ–ª—É–±–æ–π –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
                            thickness = 2
                        
                        text = f"{i+1}. {sign}: {prob:.3f} ({prob:.1%})"
                        cv2.putText(annotated_frame, text, 
                                   (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, thickness)
                        y_offset += 25
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
                    if self.save_screenshots and predictions:
                        top_confidence = predictions[0][1]
                        if top_confidence >= self.confidence_threshold:
                            cv2.putText(annotated_frame, "üì∏ Screenshot saved!", 
                                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                        else:
                            cv2.putText(annotated_frame, f"Waiting for {self.confidence_threshold:.1%} confidence...", 
                                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ –∫–∞–¥—Ä–µ
                y_bottom = annotated_frame.shape[0] - 80
                cv2.putText(annotated_frame, f"Frames: {len(self.frame_buffer)}", 
                           (10, y_bottom), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                y_bottom += 20
                cv2.putText(annotated_frame, f"Confidence Threshold: {self.confidence_threshold:.1%}", 
                           (10, y_bottom), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                y_bottom += 20
                cv2.putText(annotated_frame, "Press 'q' to quit", 
                           (10, y_bottom), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                cv2.imshow('Live ASL Recognition', annotated_frame)
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞–≤–∏—à
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("   –í—ã—Ö–æ–¥...")
                    break
        
        finally:
            cap.release()
            cv2.destroyAllWindows()
        
        print("‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    print("üé• –ñ–∏–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ASL –∂–µ—Å—Ç–æ–≤")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å
    recognizer = LiveASLRecognition(
        model_path="models/asl_model_v20250720_080209.pth",
        camera_id=0,
        target_frames=16,
        use_only_face=False,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ landmarks –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        confidence_threshold=0.8,  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –ø—Ä–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ >= 80%
        save_screenshots=True  # –í–∫–ª—é—á–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
    recognizer.start_recognition() 
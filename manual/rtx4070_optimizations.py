"""
–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è RTX4070 –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ ASL
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
import numpy as np
from typing import Optional, Tuple

# ============================================================================
# –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –î–õ–Ø RTX4070
# ============================================================================

def setup_rtx4070_optimizations():
    """
    –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –¥–ª—è RTX4070
    """
    print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –¥–ª—è RTX4070...")
    
    # –í–∫–ª—é—á–∞–µ–º TF32 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ Ampere –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ cuDNN
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è convolutions
    torch.backends.cudnn.enabled = True
    
    print("   ‚úÖ TF32 –≤–∫–ª—é—á–µ–Ω")
    print("   ‚úÖ cuDNN benchmark –≤–∫–ª—é—á–µ–Ω")
    print("   ‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")

class MixedPrecisionTrainer:
    """
    –¢—Ä–µ–Ω–µ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π mixed precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ RTX4070
    """
    def __init__(self, model, optimizer, device):
        self.model = model
        self.optimizer = optimizer
        self.device = device
        self.scaler = GradScaler()
    
    def train_step(self, sequences, labels, criterion):
        """
        –û–¥–∏–Ω —à–∞–≥ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —Å mixed precision
        """
        self.optimizer.zero_grad()
        
        # Forward pass —Å autocast
        with autocast():
            outputs = self.model(sequences)
            loss = criterion(outputs, labels)
        
        # Backward pass —Å scaler
        self.scaler.scale(loss).backward()
        self.scaler.step(self.optimizer)
        self.scaler.update()
        
        return loss.item(), outputs

# ============================================================================
# –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø –ú–û–î–ï–õ–ò
# ============================================================================

class DropPath(nn.Module):
    """
    DropPath (Stochastic Depth) –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    """
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        if self.drop_prob == 0. or not self.training:
            return x
        keep_prob = 1 - self.drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()
        output = x.div(keep_prob) * random_tensor
        return output

class ImprovedConv1DBlock(nn.Module):
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π 1D CNN –±–ª–æ–∫ —Å DropPath
    """
    def __init__(self, dim, kernel_size=17, drop_rate=0.2, drop_path_rate=0.2):
        super().__init__()
        self.kernel_size = kernel_size
        self.padding = kernel_size - 1
        
        # Depthwise convolution
        self.depthwise = nn.Conv1d(dim, dim, kernel_size, padding=self.padding, groups=dim)
        self.pointwise = nn.Conv1d(dim, dim, 1)
        
        # BatchNorm + Swish
        self.bn = nn.BatchNorm1d(dim, momentum=0.95)
        self.dropout = nn.Dropout(drop_rate)
        self.drop_path = DropPath(drop_path_rate)
        
        # Residual connection
        self.residual = nn.Conv1d(dim, dim, 1) if dim != dim else nn.Identity()
    
    def forward(self, x):
        residual = self.residual(x.transpose(1, 2))
        
        # Causal convolution
        x = x.transpose(1, 2)
        x = self.depthwise(x)
        x = self.pointwise(x)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º causal padding
        x = x[:, :, :-self.padding] if self.padding > 0 else x
        
        x = self.bn(x)
        x = F.silu(x)
        x = self.dropout(x)
        x = self.drop_path(x)
        
        x = x.transpose(1, 2)
        x = x + residual.transpose(1, 2)
        
        return x

class AWP(nn.Module):
    """
    Adversarial Weight Perturbation –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    """
    def __init__(self, model, epsilon=0.2, alpha=0.2):
        super().__init__()
        self.model = model
        self.epsilon = epsilon
        self.alpha = alpha
        self.backup = {}
        self.backup_eps = {}

    def attack_backward(self, x, y, criterion, optimizer):
        """
        –ê—Ç–∞–∫–∞ –Ω–∞ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
        """
        self._save()
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                eps = self.epsilon * param.abs().detach()
                param.data.add_(eps)
                self.backup_eps[name] = eps.data.clone()
        
        optimizer.zero_grad()
        outputs = self.model(x)
        loss = criterion(outputs, y)
        loss.backward()
        
        self._restore()
        return loss

    def _save(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data.clone()

    def _restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]

class ImprovedASLModel(nn.Module):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å ASL —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
    """
    def __init__(self, input_dim, num_classes, max_len=384, dim=192, dropout_step=0):
        super().__init__()
        self.max_len = max_len
        self.dim = dim
        
        # Preprocessing
        self.preprocessing = PreprocessingLayer(max_len)
        
        # Stem
        self.stem_conv = nn.Linear(input_dim, dim, bias=False)
        self.stem_bn = nn.BatchNorm1d(dim, momentum=0.95)
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ 1D CNN + Transformer blocks
        ksize = 17
        
        # –ü–µ—Ä–≤–∞—è –≥—Ä—É–ø–ø–∞
        self.conv1 = ImprovedConv1DBlock(dim, ksize, drop_rate=0.2, drop_path_rate=0.2)
        self.conv2 = ImprovedConv1DBlock(dim, ksize, drop_rate=0.2, drop_path_rate=0.2)
        self.conv3 = ImprovedConv1DBlock(dim, ksize, drop_rate=0.2, drop_path_rate=0.2)
        self.transformer1 = TransformerBlock(dim, expand=2)
        
        # –í—Ç–æ—Ä–∞—è –≥—Ä—É–ø–ø–∞
        self.conv4 = ImprovedConv1DBlock(dim, ksize, drop_rate=0.2, drop_path_rate=0.2)
        self.conv5 = ImprovedConv1DBlock(dim, ksize, drop_rate=0.2, drop_path_rate=0.2)
        self.conv6 = ImprovedConv1DBlock(dim, ksize, drop_rate=0.2, drop_path_rate=0.2)
        self.transformer2 = TransformerBlock(dim, expand=2)
        
        # Top layers
        self.top_conv = nn.Linear(dim, dim * 2)
        self.late_dropout = LateDropout(0.8, start_step=dropout_step)
        self.classifier = nn.Linear(dim * 2, num_classes)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
    
    def forward(self, x):
        # x: (batch, seq, landmarks, 3)
        
        # Preprocessing
        x = self.preprocessing(x)
        
        # Stem
        x = self.stem_conv(x)
        x = self.stem_bn(x.transpose(1, 2)).transpose(1, 2)
        
        # 1D CNN + Transformer blocks
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.transformer1(x)
        
        x = self.conv4(x)
        x = self.conv5(x)
        x = self.conv6(x)
        x = self.transformer2(x)
        
        # Top layers
        x = self.top_conv(x)
        x = torch.mean(x, dim=1)
        x = self.late_dropout(x)
        x = self.classifier(x)
        
        return x

# ============================================================================
# –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –¢–†–ï–ù–ò–†–û–í–©–ò–ö
# ============================================================================

class OptimizedTrainer:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è RTX4070
    """
    def __init__(self, model, device, use_mixed_precision=True):
        self.model = model
        self.device = device
        self.use_mixed_precision = use_mixed_precision
        
        if use_mixed_precision:
            self.scaler = GradScaler()
        
        # AWP
        self.awp = AWP(model, epsilon=0.2, alpha=0.2)
    
    def train_step(self, sequences, labels, criterion, optimizer, epoch):
        """
        –û–¥–∏–Ω —à–∞–≥ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
        """
        sequences = sequences.to(self.device)
        labels = labels.to(self.device)
        
        optimizer.zero_grad()
        
        # Forward pass
        if self.use_mixed_precision:
            with autocast():
                outputs = self.model(sequences)
                loss = criterion(outputs, labels)
            
            # Backward pass —Å scaler
            self.scaler.scale(loss).backward()
            
            # AWP –ø–æ—Å–ª–µ 15 —ç–ø–æ—Ö–∏
            if epoch >= 15:
                self.scaler.unscale_(optimizer)
                self.awp.attack_backward(sequences, labels, criterion, optimizer)
            
            self.scaler.step(optimizer)
            self.scaler.update()
        else:
            outputs = self.model(sequences)
            loss = criterion(outputs, labels)
            
            loss.backward()
            
            # AWP –ø–æ—Å–ª–µ 15 —ç–ø–æ—Ö–∏
            if epoch >= 15:
                self.awp.attack_backward(sequences, labels, criterion, optimizer)
            
            optimizer.step()
        
        return loss.item(), outputs

# ============================================================================
# –£–¢–ò–õ–ò–¢–´ –î–õ–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –ü–ê–ú–Ø–¢–ò
# ============================================================================

def optimize_memory_usage():
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ GPU
    """
    print("üíæ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏...")
    
    # –û—á–∏—â–∞–µ–º –∫—ç—à CUDA
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞–º—è—Ç–∏
    torch.cuda.set_per_process_memory_fraction(0.9)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 90% –ø–∞–º—è—Ç–∏
    
    print("   ‚úÖ –ö—ç—à CUDA –æ—á–∏—â–µ–Ω")
    print("   ‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞–º—è—Ç–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")

def get_optimal_batch_size(model, input_shape, device, max_memory_gb=12):
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –¥–ª—è RTX4070
    """
    print(f"üîç –ü–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞...")
    
    # –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω–µ–±–æ–ª—å—à–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    batch_size = 8
    max_batch_size = 64
    
    while batch_size <= max_batch_size:
        try:
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å —Ç–µ–∫—É—â–∏–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞
            test_input = torch.randn(batch_size, *input_shape, device=device)
            
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
            torch.cuda.empty_cache()
            
            # –ü—Ä–æ–±—É–µ–º forward pass
            with torch.no_grad():
                _ = model(test_input)
            
            # –ï—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
            batch_size *= 2
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                # –ï—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ä–∞–∑–º–µ—Ä
                optimal_batch_size = batch_size // 2
                print(f"   ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {optimal_batch_size}")
                return optimal_batch_size
            else:
                raise e
    
    print(f"   ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size // 2}")
    return batch_size // 2

# ============================================================================
# –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ò
# ============================================================================

class AdvancedAugmentation:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ—à–µ–Ω–∏—è –ø–æ–±–µ–¥–∏—Ç–µ–ª—è
    """
    def __init__(self, p=0.5):
        self.p = p
    
    def temporal_cutout(self, x, cutout_ratio=0.1):
        """
        –í—Ä–µ–º–µ–Ω–Ω–æ–π cutout - –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
        """
        if random.random() > self.p:
            return x
        
        seq_len = x.shape[0]
        cutout_len = int(seq_len * cutout_ratio)
        
        if cutout_len > 0:
            start_idx = random.randint(0, seq_len - cutout_len)
            x[start_idx:start_idx + cutout_len] = 0
        
        return x
    
    def spatial_cutout(self, x, cutout_ratio=0.1):
        """
        –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π cutout - –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö landmarks
        """
        if random.random() > self.p:
            return x
        
        num_landmarks = x.shape[1]
        cutout_count = int(num_landmarks * cutout_ratio)
        
        if cutout_count > 0:
            cutout_indices = random.sample(range(num_landmarks), cutout_count)
            x[:, cutout_indices] = 0
        
        return x
    
    def random_noise(self, x, noise_std=0.01):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —à—É–º–∞
        """
        if random.random() > self.p:
            return x
        
        noise = torch.randn_like(x) * noise_std
        return x + noise
    
    def apply_all_augmentations(self, x):
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π
        """
        x = self.temporal_resample(x)
        x = self.random_masking(x)
        x = self.horizontal_flip(x)
        x = self.random_affine(x)
        x = self.temporal_cutout(x)
        x = self.spatial_cutout(x)
        x = self.random_noise(x)
        
        return x

# ============================================================================
# –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò
# ============================================================================

def benchmark_model(model, input_shape, device, num_runs=100):
    """
    –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
    """
    print(f"‚ö° –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏...")
    
    model.eval()
    input_tensor = torch.randn(1, *input_shape, device=device)
    
    # –†–∞–∑–æ–≥—Ä–µ–≤
    with torch.no_grad():
        for _ in range(10):
            _ = model(input_tensor)
    
    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º GPU
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è
    start_time = torch.cuda.Event(enable_timing=True)
    end_time = torch.cuda.Event(enable_timing=True)
    
    start_time.record()
    
    with torch.no_grad():
        for _ in range(num_runs):
            _ = model(input_tensor)
    
    end_time.record()
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    avg_time = start_time.elapsed_time(end_time) / num_runs
    fps = 1000 / avg_time  # FPS
    
    print(f"   ‚è±Ô∏è  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è inference: {avg_time:.2f} ms")
    print(f"   üéØ FPS: {fps:.1f}")
    
    return avg_time, fps

def monitor_gpu_usage():
    """
    –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU
    """
    if torch.cuda.is_available():
        memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
        memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"üíæ GPU Memory:")
        print(f"   –í—ã–¥–µ–ª–µ–Ω–æ: {memory_allocated:.2f} GB")
        print(f"   –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {memory_reserved:.2f} GB")
        print(f"   –í—Å–µ–≥–æ: {memory_total:.2f} GB")
        print(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {memory_allocated/memory_total*100:.1f}%")

# ============================================================================
# –≠–ö–°–ü–û–†–¢ –ú–û–î–ï–õ–ò
# ============================================================================

def export_model_for_inference(model, input_shape, output_path="models/asl_model_optimized.pt"):
    """
    –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ inference
    """
    print(f"üì¶ –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è inference...")
    
    model.eval()
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    example_input = torch.randn(1, *input_shape)
    
    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å TorchScript
    try:
        traced_model = torch.jit.trace(model, example_input)
        torch.jit.save(traced_model, output_path)
        print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤: {output_path}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        loaded_model = torch.jit.load(output_path)
        with torch.no_grad():
            output = loaded_model(example_input)
        print(f"   ‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ: {e}")
        print(f"   üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ã—á–Ω—É—é –º–æ–¥–µ–ª—å...")
        torch.save(model.state_dict(), output_path.replace('.pt', '_state_dict.pth'))

if __name__ == "__main__":
    print("üîß RTX4070 –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ ASL")
    print("=" * 60)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
    setup_rtx4070_optimizations()
    optimize_memory_usage()
    
    print("\n‚úÖ –í—Å–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã!")
    print("üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–¥–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏") 
import cv2
import mediapipe as mp
import numpy as np
import torch
import json
import time
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import math
import os
import sys
import platform
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –∏ –∫–ª–∞—Å—Å—ã –∏–∑ step3
from step3_prepare_train import (
    ASLModel, 
    PreprocessingLayer, 
    AdaptiveDropout,
    TemporalConvBlock,
    BidirectionalLSTM,
    TemporalAttention,
    Conv1DBlock,
    TransformerBlock
)
from step2_prepare_dataset import load_dataset

class MacLiveASLRecognition:
    """–ñ–∏–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ASL –∂–µ—Å—Ç–æ–≤ –¥–ª—è macOS"""
    
    def __init__(self, 
                 model_path: str = None,
                 camera_id: int = 0,
                 target_frames: int = 16,
                 use_only_face: bool = False,
                 confidence_threshold: float = 0.8,
                 save_screenshots: bool = True):
        """
        Args:
            model_path: –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—â–µ—Ç –µ—Å–ª–∏ None)
            camera_id: ID –∫–∞–º–µ—Ä—ã
            target_frames: –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤
            use_only_face: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ face landmarks
            confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–∞ (0.8 = 80%)
            save_screenshots: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        """
        print("üçé –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ASL Recognition –¥–ª—è macOS...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–ª–∞—Ç—Ñ–æ—Ä–º—É
        self.is_mac = platform.system() == "Darwin"
        if self.is_mac:
            print("   –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ macOS")
        
        self.camera_id = camera_id
        self.target_frames = target_frames
        self.use_only_face = use_only_face
        self.confidence_threshold = confidence_threshold
        self.save_screenshots = save_screenshots
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é (–¥–ª—è standalone –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è)
        if getattr(sys, 'frozen', False):
            # –ó–∞–ø—É—â–µ–Ω–æ –∫–∞–∫ standalone –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
            self.base_dir = Path(sys._MEIPASS)
        else:
            # –ó–∞–ø—É—â–µ–Ω–æ –∫–∞–∫ Python —Å–∫—Ä–∏–ø—Ç
            self.base_dir = Path(__file__).parent
        
        print(f"   –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.base_dir}")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
        if model_path is None:
            model_path = self._find_latest_model()
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ –≤ –¥–æ–º–∞—à–Ω–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        self.screenshots_dir = Path.home() / "ASL_Screenshots"
        self.screenshots_dir.mkdir(exist_ok=True)
        
        # –°—á–µ—Ç—á–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∂–µ—Å—Ç–∞
        self.screenshot_counters = {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MediaPipe
        self.mp_holistic = mp.solutions.holistic
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        self.holistic = self.mp_holistic.Holistic(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5,
            model_complexity=1
        )
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è Mac (–±–µ–∑ CUDA)
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self.model, self.sign_mapping, self.classes = self._load_model(model_path)
        
        # –ë—É—Ñ–µ—Ä –¥–ª—è –∫–∞–¥—Ä–æ–≤
        self.frame_buffer = []
        self.max_buffer_size = 30
        
        print(f"üéØ Live ASL Recognition –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        print(f"   –ú–æ–¥–µ–ª—å: {model_path}")
        print(f"   –ö–ª–∞—Å—Å–æ–≤: {len(self.classes) if self.classes else 0}")
        print(f"   –¶–µ–ª–µ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤: {target_frames}")
        print(f"   –¢–æ–ª—å–∫–æ face landmarks: {use_only_face}")
        print(f"   –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {confidence_threshold:.1%}")
        print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤: {save_screenshots}")
        if save_screenshots:
            print(f"   –ü–∞–ø–∫–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤: {self.screenshots_dir}")
    
    def _find_latest_model(self) -> str:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å"""
        models_dirs = [
            self.base_dir / "models",
            self.base_dir / "manual" / "models",
            Path("models"),
            Path("manual/models")
        ]
        
        latest_model = None
        latest_time = 0
        
        for models_dir in models_dirs:
            if models_dir.exists():
                print(f"   –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π –≤: {models_dir}")
                for model_file in models_dir.glob("*.pth"):
                    try:
                        file_time = model_file.stat().st_mtime
                        if file_time > latest_time:
                            latest_time = file_time
                            latest_model = str(model_file)
                            print(f"   –ù–∞–π–¥–µ–Ω–∞ –º–æ–¥–µ–ª—å: {model_file.name}")
                    except Exception as e:
                        print(f"   –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {model_file}: {e}")
        
        if latest_model:
            print(f"   –í—ã–±—Ä–∞–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –º–æ–¥–µ–ª—å: {latest_model}")
            return latest_model
        else:
            print("   ‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
            return "models/asl_model_v20250720_080209.pth"
    
    def _load_model(self, model_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –º–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞–∫–æ–≤"""
        # –ü–æ–∏—Å–∫ –º–∞–ø–ø–∏–Ω–≥–∞ –∑–Ω–∞–∫–æ–≤
        mapping_paths = [
            self.base_dir / "dataset25" / "sign_to_prediction_index_map.json",
            self.base_dir / "manual" / "dataset25" / "sign_to_prediction_index_map.json",
            Path("dataset25/sign_to_prediction_index_map.json"),
            Path("manual/dataset25/sign_to_prediction_index_map.json")
        ]
        
        sign_mapping = {}
        classes = []
        
        for mapping_path in mapping_paths:
            if mapping_path.exists():
                print(f"   –ù–∞–π–¥–µ–Ω –º–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞–∫–æ–≤: {mapping_path}")
                try:
                    with open(mapping_path, 'r') as f:
                        sign_mapping = json.load(f)
                    # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
                    classes = [""] * len(sign_mapping)
                    for sign, idx in sign_mapping.items():
                        classes[idx] = sign
                    break
                except Exception as e:
                    print(f"   –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–∞–ø–ø–∏–Ω–≥–∞: {e}")
        
        if not sign_mapping:
            print(f"   ‚ö†Ô∏è –ú–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
            classes = [f"class_{i}" for i in range(25)]
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        input_dim = 744  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        try:
            train_data, train_labels, test_data, test_labels, _, _ = load_dataset(max_samples=1)
            sample_sequence = train_data[0]
            sample_preprocessed = PreprocessingLayer(max_len=384)(sample_sequence.unsqueeze(0))
            input_dim = sample_preprocessed.shape[-1]
            print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞ –ø–æ—Å–ª–µ preprocessing: {input_dim}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {input_dim}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model_paths = [
            Path(model_path),
            self.base_dir / model_path,
            self.base_dir / "models" / Path(model_path).name,
            self.base_dir / "manual" / "models" / Path(model_path).name
        ]
        
        model = None
        for path in model_paths:
            if path.exists():
                print(f"   –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑: {path}")
                try:
                    checkpoint = torch.load(path, map_location=self.device)
                    if 'model_state_dict' in checkpoint:
                        state_dict = checkpoint['model_state_dict']
                    else:
                        state_dict = checkpoint
                    
                    # –°–æ–∑–¥–∞–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º—É—é –º–æ–¥–µ–ª—å
                    model = self._create_compatible_model_from_state_dict(input_dim, len(classes), state_dict)
                    model.load_state_dict(state_dict)
                    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                    break
                except Exception as e:
                    print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        
        if model is None:
            print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")
            return None, sign_mapping, classes
        
        model.eval()
        return model, sign_mapping, classes
    
    def _create_compatible_model_from_state_dict(self, input_dim: int, num_classes: int, state_dict: dict):
        """–°–æ–∑–¥–∞–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ state_dict"""
        class CompatibleASLModel(nn.Module):
            def __init__(self, input_dim, num_classes, max_len=384, dim=192):
                super().__init__()
                self.max_len = max_len
                self.dim = dim
                
                # Preprocessing
                self.preprocessing = PreprocessingLayer(max_len)
                
                # Stem with improved initialization
                self.stem_conv = nn.Linear(input_dim, dim, bias=False)
                self.stem_bn = nn.BatchNorm1d(dim, momentum=0.95)
                self.stem_dropout = AdaptiveDropout(initial_p=0.1, final_p=0.3, warmup_epochs=20)
                
                # Temporal Convolutional Network (TCN) - 3 blocks with different dilations
                self.tcn1 = TemporalConvBlock(dim, kernel_size=17, dilation=1, drop_rate=0.15)
                self.tcn2 = TemporalConvBlock(dim, kernel_size=17, dilation=2, drop_rate=0.2)
                self.tcn3 = TemporalConvBlock(dim, kernel_size=17, dilation=4, drop_rate=0.25)
                
                # Projection for attention pooling
                self.attention_projection = nn.Linear(dim, dim)
                
                # Bidirectional LSTM - 2 layers for better dependency capture
                self.lstm = BidirectionalLSTM(dim, hidden_dim=dim//2, num_layers=2, drop_rate=0.15)
                
                # Temporal Attention - more heads for better attention
                self.temporal_attention = TemporalAttention(dim, num_heads=8, drop_rate=0.15)
                
                # 1D CNN + Transformer blocks - 3 conv blocks
                ksize = 17
                self.conv1 = Conv1DBlock(dim, ksize, drop_rate=0.15)
                self.conv2 = Conv1DBlock(dim, ksize, drop_rate=0.2)
                self.conv3 = Conv1DBlock(dim, ksize, drop_rate=0.25)
                self.transformer1 = TransformerBlock(dim, expand=2, drop_rate=0.15)
                
                # Top layers with improved pooling
                self.top_conv = nn.Linear(dim, dim)
                self.top_bn = nn.BatchNorm1d(dim, momentum=0.95)
                self.adaptive_dropout = AdaptiveDropout(initial_p=0.2, final_p=0.5, warmup_epochs=25)
                
                # Improved pooling - add attention pooling
                self.temporal_pool1 = nn.AdaptiveAvgPool1d(1)
                self.temporal_pool2 = nn.AdaptiveMaxPool1d(1)
                self.attention_pool = nn.MultiheadAttention(dim, num_heads=4, batch_first=True, dropout=0.1)
                
                # Final classifier with improved architecture
                self.classifier = nn.Sequential(
                    nn.Linear(dim * 3, dim),  # dim*3 for avg + max + attention pooling
                    nn.BatchNorm1d(dim),
                    nn.SiLU(),
                    nn.Dropout(0.3),
                    nn.Linear(dim, num_classes)
                )
                
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
                self._init_weights()
            
            def _init_weights(self):
                for m in self.modules():
                    if isinstance(m, nn.Linear):
                        init.xavier_uniform_(m.weight)
                        if m.bias is not None:
                            init.zeros_(m.bias)
                    elif isinstance(m, nn.Conv1d):
                        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        if m.bias is not None:
                            init.zeros_(m.bias)
                    elif isinstance(m, nn.BatchNorm1d):
                        init.ones_(m.weight)
                        init.zeros_(m.bias)
                    elif isinstance(m, nn.LSTM):
                        for name, param in m.named_parameters():
                            if 'weight' in name:
                                init.orthogonal_(param)
                            elif 'bias' in name:
                                init.zeros_(param)
            
            def forward(self, x):
                # x: (batch, seq, landmarks, 3)
                
                # Preprocessing
                x = self.preprocessing(x)  # (batch, seq, features)
                
                # Stem with adaptive dropout
                x = self.stem_conv(x)
                x = self.stem_bn(x.transpose(1, 2)).transpose(1, 2)
                x = self.stem_dropout(x)
                
                # TCN blocks - 3 blocks with different dilations
                x = self.tcn1(x)
                x = self.tcn2(x)
                x = self.tcn3(x)
                
                # Bidirectional LSTM - 2 layers
                x = self.lstm(x)
                
                # Temporal Attention - more heads
                x = self.temporal_attention(x)
                
                # 1D CNN + Transformer - 3 conv blocks
                x = self.conv1(x)
                x = self.conv2(x)
                x = self.conv3(x)
                x = self.transformer1(x)
                
                # Top layers with improved pooling
                x = self.top_conv(x)  # (batch, seq, dim)
                x = self.top_bn(x.transpose(1, 2)).transpose(1, 2)
                x = self.adaptive_dropout(x)
                
                # Improved temporal pooling
                x_transposed = x.transpose(1, 2)  # (batch, dim, seq)
                
                # Global average pooling
                global_avg = self.temporal_pool1(x_transposed).squeeze(-1)  # (batch, dim)
                
                # Global max pooling
                global_max = self.temporal_pool2(x_transposed).squeeze(-1)  # (batch, dim)
                
                # Attention pooling - use projection for stability
                x_for_attn = self.attention_projection(x)  # x already has dimension (batch, seq, dim)
                
                attn_out, _ = self.attention_pool(x_for_attn, x_for_attn, x_for_attn)
                global_attn = torch.mean(attn_out, dim=1)  # (batch, dim)
                
                # Combine pooling results
                x_pooled = torch.cat([global_avg, global_max, global_attn], dim=1)  # (batch, dim*3)
                
                x = self.classifier(x_pooled)
                
                return x
        
        return CompatibleASLModel(input_dim, num_classes, max_len=384, dim=192).to(self.device)
    
    def _extract_landmarks(self, results) -> Optional[np.ndarray]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç landmarks –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ MediaPipe"""
        landmarks = []
        
        if self.use_only_face:
            if results.face_landmarks:
                for landmark in results.face_landmarks.landmark:
                    landmarks.extend([landmark.x, landmark.y, landmark.z])
        else:
            # Face landmarks (468 —Ç–æ—á–µ–∫)
            if results.face_landmarks:
                for landmark in results.face_landmarks.landmark:
                    landmarks.extend([landmark.x, landmark.y, landmark.z])
            
            # Pose landmarks (33 —Ç–æ—á–∫–∏)
            if results.pose_landmarks:
                for landmark in results.pose_landmarks.landmark:
                    landmarks.extend([landmark.x, landmark.y, landmark.z])
            
            # Left hand landmarks (21 —Ç–æ—á–∫–∞)
            if results.left_hand_landmarks:
                for landmark in results.left_hand_landmarks.landmark:
                    landmarks.extend([landmark.x, landmark.y, landmark.z])
            
            # Right hand landmarks (21 —Ç–æ—á–∫–∞)
            if results.right_hand_landmarks:
                for landmark in results.right_hand_landmarks.landmark:
                    landmarks.extend([landmark.x, landmark.y, landmark.z])
        
        if landmarks:
            return np.array(landmarks).reshape(-1, 3)
        return None
    
    def _draw_landmarks(self, frame: np.ndarray, results) -> np.ndarray:
        """–†–∏—Å—É–µ—Ç landmarks –Ω–∞ –∫–∞–¥—Ä–µ"""
        annotated_frame = frame.copy()
        
        # –†–∏—Å—É–µ–º face landmarks
        if results.face_landmarks:
            self.mp_drawing.draw_landmarks(
                annotated_frame, results.face_landmarks, self.mp_holistic.FACEMESH_CONTOURS,
                landmark_drawing_spec=None,
                connection_drawing_spec=self.mp_drawing_styles.get_default_face_mesh_contours_style()
            )
        
        # –†–∏—Å—É–µ–º pose landmarks (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ face)
        if not self.use_only_face and results.pose_landmarks:
            self.mp_drawing.draw_landmarks(
                annotated_frame, results.pose_landmarks, self.mp_holistic.POSE_CONNECTIONS,
                landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style()
            )
        
        # –†–∏—Å—É–µ–º left hand landmarks (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ face)
        if not self.use_only_face and results.left_hand_landmarks:
            self.mp_drawing.draw_landmarks(
                annotated_frame, results.left_hand_landmarks, self.mp_holistic.HAND_CONNECTIONS,
                landmark_drawing_spec=self.mp_drawing_styles.get_default_hand_landmarks_style(),
                connection_drawing_spec=self.mp_drawing_styles.get_default_hand_connections_style()
            )
        
        # –†–∏—Å—É–µ–º right hand landmarks (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ face)
        if not self.use_only_face and results.right_hand_landmarks:
            self.mp_drawing.draw_landmarks(
                annotated_frame, results.right_hand_landmarks, self.mp_holistic.HAND_CONNECTIONS,
                landmark_drawing_spec=self.mp_drawing_styles.get_default_hand_landmarks_style(),
                connection_drawing_spec=self.mp_drawing_styles.get_default_hand_connections_style()
            )
        
        return annotated_frame
    
    def _normalize_landmarks(self, landmarks_data: List[np.ndarray]) -> List[np.ndarray]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç landmarks"""
        if not landmarks_data:
            return []
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤
        if len(landmarks_data) > self.target_frames:
            indices = np.linspace(0, len(landmarks_data) - 1, self.target_frames, dtype=int)
            landmarks_data = [landmarks_data[i] for i in indices]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ landmarks
        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç 543 landmarks (468 face + 33 pose + 21 left hand + 21 right hand)
        expected_landmarks = 543
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É landmarks
        normalized_data = []
        for frame in landmarks_data:
            if len(frame) >= expected_landmarks:
                normalized_frame = frame[:expected_landmarks]
            else:
                # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç landmarks
                normalized_frame = np.zeros((expected_landmarks, 3))
                normalized_frame[:len(frame)] = frame
            
            normalized_data.append(normalized_frame)
        
        return normalized_data
    
    def _predict_gesture(self, landmarks_data: List[np.ndarray]) -> List[Tuple[str, float]]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∂–µ—Å—Ç"""
        if not landmarks_data or len(landmarks_data) < 5 or self.model is None:
            return []
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        normalized_data = self._normalize_landmarks(landmarks_data)
        
        if not normalized_data:
            return []
        
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            # –§–æ—Ä–º–∞—Ç: (batch, seq, landmarks, 3)
            tensor = torch.tensor(normalized_data, dtype=torch.float32).unsqueeze(0)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º
            with torch.no_grad():
                predictions = self.model(tensor.to(self.device))
                probabilities = torch.softmax(predictions, dim=1)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            top_probs, top_indices = torch.topk(probabilities, min(3, len(self.classes)), dim=1)
            
            results = []
            for i in range(min(3, len(self.classes))):
                idx = top_indices[0][i].item()
                prob = top_probs[0][i].item()
                
                # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∑–Ω–∞–∫–∞
                if idx < len(self.classes) and self.classes[idx]:
                    sign_name = self.classes[idx]
                else:
                    sign_name = f"Unknown_{idx}"
                
                results.append((sign_name, prob))
            
            return results
        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
            return []
    
    def _save_screenshot(self, frame: np.ndarray, predictions: List[Tuple[str, float]]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏"""
        if not self.save_screenshots or not predictions:
            return
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        top_prediction = predictions[0]
        sign_name, confidence = top_prediction
        
        if confidence < self.confidence_threshold:
            return
        
        try:
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –∫–∞–¥—Ä–∞ –¥–ª—è —Å–∫—Ä–∏–Ω—à–æ—Ç–∞
            screenshot = frame.copy()
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –Ω–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç
            y_offset = 30
            cv2.putText(screenshot, "ASL Recognition Results:", 
                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
            y_offset += 40
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            cv2.putText(screenshot, f"Time: {timestamp}", 
                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            y_offset += 30
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            cv2.putText(screenshot, "Top 3 Predictions:", 
                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            y_offset += 30
            
            for i, (sign, prob) in enumerate(predictions):
                # –¶–≤–µ—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                if prob >= self.confidence_threshold:
                    color = (0, 255, 0)  # –ó–µ–ª–µ–Ω—ã–π –¥–ª—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö
                elif i == 0:
                    color = (255, 255, 0)  # –ñ–µ–ª—Ç—ã–π –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ
                else:
                    color = (0, 255, 255)  # –ì–æ–ª—É–±–æ–π –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
                
                text = f"{i+1}. {sign}: {prob:.3f} ({prob:.1%})"
                cv2.putText(screenshot, text, 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                y_offset += 25
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö
            y_offset += 10
            cv2.putText(screenshot, f"Confidence Threshold: {self.confidence_threshold:.1%}", 
                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            y_offset += 20
            cv2.putText(screenshot, f"Frames: {len(self.frame_buffer)}", 
                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
            if sign_name not in self.screenshot_counters:
                self.screenshot_counters[sign_name] = 0
            self.screenshot_counters[sign_name] += 1
            
            # –û—á–∏—â–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            safe_sign_name = "".join(c for c in sign_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_sign_name = safe_sign_name.replace(' ', '_')
            
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{safe_sign_name}_{timestamp_str}_{self.screenshot_counters[sign_name]:03d}.jpg"
            filepath = self.screenshots_dir / filename
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç
            success = cv2.imwrite(str(filepath), screenshot)
            if success:
                print(f"üì∏ –°–æ—Ö—Ä–∞–Ω–µ–Ω —Å–∫—Ä–∏–Ω—à–æ—Ç: {filename} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})")
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–∞: {filename}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞: {e}")
    
    def start_recognition(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∂–∏–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ"""
        print("\nüé¨ –ù–∞—á–∏–Ω–∞–µ–º –∂–∏–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ASL –∂–µ—Å—Ç–æ–≤ –Ω–∞ macOS...")
        print("   –ù–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞")
        
        if self.model is None:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –Ω–∞—á–∞—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ.")
            return
        
        cap = cv2.VideoCapture(self.camera_id)
        
        if not cap.isOpened():
            print("‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É")
            print("   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å camera_id (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1 –≤–º–µ—Å—Ç–æ 0)")
            return
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–∞–º–µ—Ä—ã
        cap.set(cv2.CAP_PROP_FPS, 30)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        self.frame_buffer = []
        last_prediction_time = time.time()
        prediction_interval = 0.5  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 0.5 —Å–µ–∫—É–Ω–¥
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    print("‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–∞–¥—Ä–∞")
                    break
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR –≤ RGB
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–¥—Ä —á–µ—Ä–µ–∑ MediaPipe
                results = self.holistic.process(rgb_frame)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º landmarks
                frame_landmarks = self._extract_landmarks(results)
                
                if frame_landmarks is not None:
                    self.frame_buffer.append(frame_landmarks)
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
                    if len(self.frame_buffer) > self.max_buffer_size:
                        self.frame_buffer.pop(0)
                
                # –†–∏—Å—É–µ–º landmarks –Ω–∞ –∫–∞–¥—Ä–µ
                annotated_frame = self._draw_landmarks(frame, results)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∂–µ—Å—Ç –∫–∞–∂–¥—ã–µ 0.5 —Å–µ–∫—É–Ω–¥
                current_time = time.time()
                if current_time - last_prediction_time > prediction_interval and len(self.frame_buffer) >= 5:
                    predictions = self._predict_gesture(self.frame_buffer)
                    last_prediction_time = current_time
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç –µ—Å–ª–∏ –µ—Å—Ç—å –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                    if predictions and self.save_screenshots:
                        self._save_screenshot(annotated_frame, predictions)
                    
                    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –∫–∞–¥—Ä–µ
                    if predictions:
                        y_offset = 30
                        cv2.putText(annotated_frame, "Top 3 Predictions:", 
                                   (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                        y_offset += 30
                        
                        for i, (sign, prob) in enumerate(predictions):
                            # –¶–≤–µ—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                            if prob >= self.confidence_threshold:
                                color = (0, 255, 0)  # –ó–µ–ª–µ–Ω—ã–π –¥–ª—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö
                                thickness = 3  # –¢–æ–ª—â–µ –¥–ª—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö
                            elif i == 0:
                                color = (255, 255, 0)  # –ñ–µ–ª—Ç—ã–π –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ
                                thickness = 2
                            else:
                                color = (0, 255, 255)  # –ì–æ–ª—É–±–æ–π –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
                                thickness = 2
                            
                            text = f"{i+1}. {sign}: {prob:.3f} ({prob:.1%})"
                            cv2.putText(annotated_frame, text, 
                                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, thickness)
                            y_offset += 25
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
                        if self.save_screenshots:
                            top_confidence = predictions[0][1]
                            if top_confidence >= self.confidence_threshold:
                                cv2.putText(annotated_frame, "üì∏ Screenshot saved!", 
                                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                            else:
                                cv2.putText(annotated_frame, f"Waiting for {self.confidence_threshold:.1%} confidence...", 
                                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ –∫–∞–¥—Ä–µ
                y_bottom = annotated_frame.shape[0] - 100
                cv2.putText(annotated_frame, f"Device: {self.device}", 
                           (10, y_bottom), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                y_bottom += 20
                cv2.putText(annotated_frame, f"Frames: {len(self.frame_buffer)}", 
                           (10, y_bottom), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                y_bottom += 20
                cv2.putText(annotated_frame, f"Confidence Threshold: {self.confidence_threshold:.1%}", 
                           (10, y_bottom), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                y_bottom += 20
                cv2.putText(annotated_frame, "Press 'q' to quit", 
                           (10, y_bottom), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                cv2.imshow('ASL Recognition for macOS', annotated_frame)
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞–≤–∏—à
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("   –í—ã—Ö–æ–¥...")
                    break
        
        except KeyboardInterrupt:
            print("   –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º...")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {e}")
        finally:
            cap.release()
            cv2.destroyAllWindows()
        
        print("‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    print("üé• –ñ–∏–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ASL –∂–µ—Å—Ç–æ–≤ –¥–ª—è macOS")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å
    recognizer = MacLiveASLRecognition(
        model_path=None,  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π –º–æ–¥–µ–ª–∏
        camera_id=0,
        target_frames=16,
        use_only_face=False,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ landmarks –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        confidence_threshold=0.8,  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –ø—Ä–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ >= 80%
        save_screenshots=True  # –í–∫–ª—é—á–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
    recognizer.start_recognition() 
import pandas as pd
import json
import os
import shutil
from collections import Counter
from tqdm import tqdm

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
INPUT_CSV = 'data/google_asl_signs/train.csv'
INPUT_MAP = 'data/google_asl_signs/sign_to_prediction_index_map.json'
INPUT_LANDMARKS = 'data/google_asl_signs/train_landmark_files'
OUTPUT_DIR = 'manual/dataset25'
OUTPUT_CSV = os.path.join(OUTPUT_DIR, 'train.csv')
OUTPUT_MAP = os.path.join(OUTPUT_DIR, 'sign_to_prediction_index_map.json')
OUTPUT_LANDMARKS = os.path.join(OUTPUT_DIR, 'train_landmark_files')

# –°–ø–∏—Å–æ–∫ –±–∞–∑–æ–≤—ã—Ö —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–µ–Ω –∑–Ω–∞—Ç—å –∫–∞–∂–¥—ã–π (–≤ –ø–æ—Ä—è–¥–∫–µ –≤–∞–∂–Ω–æ—Å—Ç–∏)
BASIC_WORDS = [
    # –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è –∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–≤–∞
    'hello', 'yes,no', 'please', 'thankyou', 'sorry', 'goodbye', 'bye',
    
    # –õ–∏—á–Ω—ã–µ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è
    'i', 'you', 'he', 'she', 'we', 'they', 'me', 'my', 'your', 'his,her',
    
    # –°–µ–º—å—è
    'mom', 'dad', 'baby', 'boy', 'girl', 'man', 'woman', 'child', 'family',
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
    'eat', 'drink', 'sleep', 'walk', 'un', 'sit', 'stand', 'come', 'go', 'stop',
    
    # –≠–º–æ—Ü–∏–∏ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    'happy', 'sad', 'angry', 'tired', 'hungry', 'thirsty', 'sick', 'ood', 'bad',
    
    # –¶–≤–µ—Ç–∞
    'red', 'blue', 'green', 'yellow', 'black', 'white', 'brown', 'pink',
    
    # –ß–∏—Å–ª–∞
    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten',
    
    # –í—Ä–µ–º—è
    'today', 'tomorrow', 'yesterday', 'morning', 'afternoon', 'night', 'time',
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–¥–º–µ—Ç—ã
    'water', 'food', 'house', 'car', 'book', 'phone', 'money', 'help'
]

def get_available_basic_words(df, basic_words_list):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –±–∞–∑–æ–≤—ã—Ö —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    available_words = []
    missing_words = []
    
    for word in basic_words_list:
        if word in df['sign'].values:
            available_words.append(word)
        else:
            missing_words.append(word)
    
    print(f"–ù–∞–π–¥–µ–Ω–æ –±–∞–∑–æ–≤—ã—Ö —Å–ª–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {len(available_words)}")
    if missing_words:
        print(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {missing_words}")
    return available_words

def get_top_words_by_importance(df, basic_words, target_count=50):
    """–í—ã–±–∏—Ä–∞–µ—Ç —Å–ª–æ–≤–∞ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏: —Å–Ω–∞—á–∞–ª–∞ –±–∞–∑–æ–≤—ã–µ, –ø–æ—Ç–æ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ"""
    available_basic = get_available_basic_words(df, basic_words)
    
    # –ë–µ—Ä–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ —Å–ª–æ–≤–∞
    selected_words = available_basic[:target_count]
    
    # –ï—Å–ª–∏ –±–∞–∑–æ–≤—ã—Ö —Å–ª–æ–≤ –º–µ–Ω—å—à–µ 50, –¥–æ–±–∞–≤–ª—è–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∏–∑ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è
    if len(selected_words) < target_count:
        remaining_words = df[~df['sign'].isin(selected_words)]['sign'].value_counts()
        additional_words = remaining_words.head(target_count - len(selected_words)).index.tolist()
        selected_words.extend(additional_words)
    
    return selected_words[:target_count]

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
print('–ó–∞–≥—Ä—É–∂–∞–µ–º train.csv...')
df = pd.read_csv(INPUT_CSV)
print(f'–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}')
print(f'–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {df["sign"].nunique()}')

# 2. –í—ã–±–∏—Ä–∞–µ–º 25 –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
print('–í—ã–±–∏—Ä–∞–µ–º 25 –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤...')
top_25 = get_top_words_by_importance(df, BASIC_WORDS, 25)
print(f'–í—ã–±—Ä–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ ({len(top_25)}):')
for i, word in enumerate(top_25):
    count = len(df[df['sign'] == word])
    print(f'{i:2d} {word:15s} ({count:5d} –∑–∞–ø–∏—Å–µ–π)')

# 3. –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏ —Ç–æ–ª—å–∫–æ —Å —ç—Ç–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
filtered_df = df[df['sign'].isin(top_25)].copy()
print(f'\n–°—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered_df)}')

# 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π train.csv
os.makedirs(OUTPUT_DIR, exist_ok=True)
filtered_df.to_csv(OUTPUT_CSV, index=False)
print(f'–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {OUTPUT_CSV}')

# 5. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π sign_to_prediction_index_map.json
with open(INPUT_MAP, 'r') as f:
    full_map = json.load(f)

new_map = {sign: i for i, sign in enumerate(top_25)}
with open(OUTPUT_MAP, 'w') as f:
    json.dump(new_map, f, indent=2)
print(f'–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {OUTPUT_MAP}')

# 6. –ö–æ–ø–∏—Ä—É–µ–º parquet-—Ñ–∞–π–ª—ã —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
os.makedirs(OUTPUT_LANDMARKS, exist_ok=True)
print('\n–ö–æ–ø–∏—Ä—É–µ–º parquet-—Ñ–∞–π–ª—ã...')

# –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è
unique_paths = filtered_df['path'].unique()
copied_count = 0
skipped_count = 0

with tqdm(total=len(unique_paths), desc="–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤") as pbar:
    for rel_path in unique_paths:
        src = os.path.join('data/google_asl_signs', rel_path)
        dst = os.path.join(OUTPUT_DIR, rel_path)
        dst_dir = os.path.dirname(dst)
        
        try:
            os.makedirs(dst_dir, exist_ok=True)
            if not os.path.exists(dst):
                shutil.copy2(src, dst)
                copied_count += 1
            else:
                skipped_count += 1
        except Exception as e:
            print(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–∏ {rel_path}: {e}')
            skipped_count += 1        
        pbar.update(1)

print('\n–ì–æ—Ç–æ–≤–æ!')
print(f'–°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {copied_count}')
print(f'–ü—Ä–æ–ø—É—â–µ–Ω–æ (—É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç): {skipped_count}')
print(f'–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(unique_paths)}')

def create_train_test_split():
    """–°–æ–∑–¥–∞–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train –∏ test –¥–∞—Ç–∞—Å–µ—Ç—ã"""
    print('\n' + '='*60)
    print('üöÄ –°–û–ó–î–ê–ù–ò–ï –†–ê–ó–î–ï–õ–ï–ù–ò–Ø –ù–ê TRAIN –ò TEST')
    print('='*60)
    
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–æ–¥—É–ª–∏
    import numpy as np
    import random
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    SPLIT_OUTPUT_BASE = 'manual/dataset25_split'
    TRAIN_DIR = os.path.join(SPLIT_OUTPUT_BASE, 'train')
    TEST_DIR = os.path.join(SPLIT_OUTPUT_BASE, 'test')
    TEST_SIZE = 0.2  # 20% –¥–ª—è —Ç–µ—Å—Ç–∞
    RANDOM_STATE = 42
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print('–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è...')
    df = pd.read_csv(OUTPUT_CSV)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {df['participant_id'].nunique()}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {df['sign'].nunique()}")
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º
    unique_participants = df['participant_id'].unique()
    participant_word_counts = df.groupby('participant_id')['sign'].nunique()
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤ –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
    word_count_bins = pd.cut(participant_word_counts, bins=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])
    
    split_df = pd.DataFrame({
        'participant_id': unique_participants,
        'word_count': participant_word_counts.values,
        'word_count_bin': word_count_bins.values
    })
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    random.seed(RANDOM_STATE)
    np.random.seed(RANDOM_STATE)
    
    # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –Ω–∞ train –∏ test
    participants_list = unique_participants.tolist()
    random.shuffle(participants_list)
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏
    split_idx = int(len(participants_list) * (1 - TEST_SIZE))
    train_participants = participants_list[:split_idx]
    test_participants = participants_list[split_idx:]
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    train_df = df[df['participant_id'].isin(train_participants)].copy()
    test_df = df[df['participant_id'].isin(test_participants)].copy()
    
    print(f"–£—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –≤ train: {len(train_participants)}")
    print(f"–£—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –≤ test: {len(test_participants)}")
    print(f"–ó–∞–ø–∏—Å–µ–π –≤ train: {len(train_df)}")
    print(f"–ó–∞–ø–∏—Å–µ–π –≤ test: {len(test_df)}")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    os.makedirs(SPLIT_OUTPUT_BASE, exist_ok=True)
    os.makedirs(TRAIN_DIR, exist_ok=True)
    os.makedirs(TEST_DIR, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV —Ñ–∞–π–ª—ã
    train_csv = os.path.join(TRAIN_DIR, 'train.csv')
    test_csv = os.path.join(TEST_DIR, 'train.csv')
    
    train_df.to_csv(train_csv, index=False)
    test_df.to_csv(test_csv, index=False)
    
    # –ö–æ–ø–∏—Ä—É–µ–º sign_to_prediction_index_map.json
    for output_dir in [TRAIN_DIR, TEST_DIR]:
        shutil.copy2(OUTPUT_MAP, os.path.join(output_dir, 'sign_to_prediction_index_map.json'))
    
    # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö
    def copy_files_for_split(split_df, output_dir, split_name):
        unique_paths = split_df['path'].unique()
        copied_count = 0
        
        print(f'\n–ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –¥–ª—è {split_name}...')
        with tqdm(total=len(unique_paths), desc=f"–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ {split_name}") as pbar:
            for rel_path in unique_paths:
                src = os.path.join(OUTPUT_DIR, rel_path)
                dst = os.path.join(output_dir, rel_path)
                dst_dir = os.path.dirname(dst)
                
                try:
                    os.makedirs(dst_dir, exist_ok=True)
                    if not os.path.exists(dst):
                        shutil.copy2(src, dst)
                        copied_count += 1
                except Exception as e:
                    print(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–∏ {rel_path}: {e}')
                pbar.update(1)
        
        print(f'–°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è {split_name}: {copied_count}')
        return copied_count
    
    copy_files_for_split(train_df, TRAIN_DIR, 'train')
    copy_files_for_split(test_df, TEST_DIR, 'test')
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å
    print('\n=== –ü–†–û–í–ï–†–ö–ê –ë–ê–õ–ê–ù–°–ê ===')
    train_signs = train_df['sign'].value_counts()
    test_signs = test_df['sign'].value_counts()
    
    all_signs = set(df['sign'].unique())
    train_sign_set = set(train_signs.index)
    test_sign_set = set(test_signs.index)
    
    missing_in_train = all_signs - train_sign_set
    missing_in_test = all_signs - test_sign_set
    
    if not missing_in_train and not missing_in_test:
        print('‚úÖ –í—Å–µ —Å–ª–æ–≤–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ train –∏ test')
    else:
        if missing_in_train:
            print(f'‚ö†Ô∏è  –°–ª–æ–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ train: {missing_in_train}')
        if missing_in_test:
            print(f'‚ö†Ô∏è  –°–ª–æ–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ test: {missing_in_test}')
    
    print(f'\n‚úÖ –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!')
    print(f'üìÅ Train –¥–∞–Ω–Ω—ã–µ: {TRAIN_DIR}')
    print(f'üìÅ Test –¥–∞–Ω–Ω—ã–µ: {TEST_DIR}')
    print(f'üìä Train: {len(train_df)} –∑–∞–ø–∏—Å–µ–π ({len(train_df)/len(df)*100:.1f}%)')
    print(f'üìä Test: {len(test_df)} –∑–∞–ø–∏—Å–µ–π ({len(test_df)/len(df)*100:.1f}%)')

# –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ, —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É:
# create_train_test_split() 
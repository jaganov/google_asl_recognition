{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google ASL Recognition Project Implementation Report\n",
    "\n",
    "**Author:** JAGANOV Timur / m5281502\n",
    "\n",
    "**Date:** July 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project implements a deep learning model for American Sign Language (ASL) recognition using Google's ASL dataset. \n",
    "The implementation focuses on temporal sequence modeling with a hybrid architecture combining Temporal Convolutional Networks (TCN), LSTM, and Transformer components.\n",
    "\n",
    "American Sign Language (ASL) is the primary language of the deaf community in North America, with over 500,000 native users.\n",
    "Automatic ASL recognition presents unique challenges due to the complex spatio-temporal nature of sign gestures, requiring both precise hand shape recognition and temporal motion understanding.\n",
    "\n",
    "This work addresses ASL recognition using the Google ASL dataset from Kaggle competition, which provides pre-computed MediaPipe landmarks (50GB of parquet files) instead of raw video data. We focus on developing a robust recognition system that overcomes the common challenge of overfitting in deep learning models.\n",
    "Our approach combines multiple temporal modeling techniques with novel adaptive regularization strategies, processing the ready-to-use landmark sequences.\n",
    "\n",
    "**GitHub Repository:** [https://github.com/jaganov/google_asl_recognition](https://github.com/jaganov/google_asl_recognition)\n",
    "\n",
    "The overall approach involves:\n",
    "1. **Data Processing**: Loading and preprocessing ASL sign data from Google's parquet files containing MediaPipe landmarks\n",
    "2. **Feature Engineering**: Processing pre-computed MediaPipe landmarks (543 points per frame) into rich temporal features\n",
    "3. **Model Training**: Developing and training deep learning models for sign classification\n",
    "4. **Real-time Recognition**: Implementing a live recognition system that uses MediaPipe for webcam input\n",
    "5. **Performance Optimization**: Optimizing the system for GPU acceleration (RTX 4070)\n",
    "\n",
    "The project aims to bridge communication gaps for the deaf and hard-of-hearing community by providing an accessible, accurate, and real-time ASL recognition tool.\n",
    "\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "The project consists of several key scripts:\n",
    "- **Data Preparation**: `step1_extract_words.py`, `step1.2_split_train_test.py`, `step2_prepare_dataset.py`\n",
    "- **Model Training**: `step3_prepare_train.py` \n",
    "- **Live Recognition**: `step5_live_recognition.py`\n",
    "- **Testing & Utilities**: Various testing and analysis scripts\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Advanced Preprocessing**: Multi-feature extraction including velocity, acceleration, and temporal consistency\n",
    "- **Hybrid Architecture**: TCN + LSTM + Transformer for comprehensive temporal modeling\n",
    "- **Adaptive Regularization**: Dynamic dropout and advanced augmentation strategies\n",
    "- **Live Recognition**: Real-time webcam testing capabilities\n",
    "- **Comprehensive Monitoring**: Detailed training metrics and model versioning\n",
    "\n",
    "### Recognized Gestures (25 ASL Signs)\n",
    "\n",
    "**Greetings & Courtesy**: hello, please, thankyou, bye  \n",
    "**Family**: mom, dad, boy, girl, man, child  \n",
    "**Actions**: drink, sleep, go  \n",
    "**Emotions**: happy, sad, hungry, thirsty, sick, bad  \n",
    "**Colors**: red, blue, green, yellow, black, white  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "### Problem Definition\n",
    "\n",
    "**Objective**: Develop an automated system for recognizing American Sign Language (ASL) signs from video input with high accuracy and real-time performance.\n",
    "\n",
    "The primary challenges in ASL recognition include:\n",
    "- **Temporal Dependencies**: Signs involve complex temporal patterns that simple CNNs cannot capture\n",
    "- **Overfitting**: Limited training data leads to models memorizing rather than generalizing\n",
    "- **Hardware Constraints**: Training large models on consumer GPUs requires careful optimization\n",
    "- **Class Imbalance**: Some signs have fewer examples than others\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- **Primary Dataset**: Google ASL Signs Dataset\n",
    "- **Data Structure**: Pre-computed MediaPipe landmarks stored in parquet files (543 landmarks per frame)\n",
    "- **Sign Categories**: 250 distinct ASL signs (we focus on 25 most common signs)\n",
    "- **Data Split**: Training/Testing split for model validation\n",
    "- **Data Size**: ~50GB of landmark data (no video processing required)\n",
    "\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- **GPU**: NVIDIA RTX 4070 (12GB VRAM)\n",
    "- **CPU**: Multi-core processor (Intel i5+ or AMD Ryzen 5+)\n",
    "- **RAM**: 16GB system memory\n",
    "- **Storage**: 50GB available space for datasets and models\n",
    "\n",
    "**Software Environment:**\n",
    "- **OS**: Windows 10/11, Linux Ubuntu 18.04+\n",
    "- **Python**: 3.8+ with virtual environment support\n",
    "- **CUDA**: 11.8+ for GPU acceleration\n",
    "- **Key Libraries**: PyTorch, MediaPipe (for live recognition), OpenCV, NumPy, Pandas\n",
    "\n",
    "\n",
    "**Google ASL Signs Dataset:**\n",
    "- **Total Samples**: 25,000+ sign instances\n",
    "- **Participants**: 100+ diverse signers\n",
    "- **Sign Categories**: 250 ASL words (project focuses on 25 common signs)\n",
    "- **Data Format**: Parquet files with pre-computed MediaPipe landmarks\n",
    "  - Face: 468 points, Pose: 33 points, Hands: 42 points (21 per hand)\n",
    "- **Temporal Structure**: Variable-length sequences with frame-level coordinates\n",
    "\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "**Primary Performance Targets:**\n",
    "- **Validation Accuracy**: 75-78% (significant improvement over baseline 65%)\n",
    "- **Inference Time**: <100ms per sequence (real-time capability)\n",
    "- **Training Stability**: Smooth convergence without sharp accuracy drops\n",
    "- **Overfitting Reduction**: Training-validation gap <12% (vs baseline 17.4%)\n",
    "- **Model Size**: <15MB for deployment efficiency\n",
    "\n",
    "**Technical Improvements:**\n",
    "- **Temporal Modeling**: Enhanced capture of sign dynamics through hybrid architecture\n",
    "- **Generalization**: Better performance across different signers and conditions\n",
    "- **Convergence**: More stable training reaching 100-150 epochs vs baseline 55\n",
    "- **Memory Efficiency**: Optimized for RTX 4070 (12GB VRAM) with batch size 32\n",
    "\n",
    "**Real-world Performance:**\n",
    "- **Live Recognition**: Smooth real-time processing via webcam\n",
    "- **Robustness**: Consistent performance across lighting conditions\n",
    "- **Scalability**: Architecture supports easy extension to more sign classes\n",
    "\n",
    "### Evaluation Criteria\n",
    "\n",
    "**Classification Metrics:**\n",
    "- **Training Accuracy**: Accuracy on training dataset during training process\n",
    "- **Validation Accuracy**: Primary metric for model comparison and early stopping\n",
    "- **Loss Values**: Training and validation loss for convergence analysis\n",
    "- **Best Model Selection**: Highest validation accuracy achieved during training\n",
    "\n",
    "**Training Metrics:**\n",
    "- **Loss Convergence**: Training and validation loss curves\n",
    "- **Learning Stability**: Absence of sharp accuracy drops or oscillations\n",
    "- **Overfitting Analysis**: Gap between training and validation performance\n",
    "- **Epoch Efficiency**: Number of epochs to reach optimal performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Results\n",
    "\n",
    "### Training Progress\n",
    "![Training History](./training_history.png)\n",
    "\n",
    "### Live Recognition Demo\n",
    "![Live Recognition Screenshot](./happy_20250721_134139_002.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Approach and Models\n",
    "\n",
    "### Overview\n",
    "\n",
    "The ASL recognition model employs a sophisticated hybrid architecture that combines multiple deep learning techniques to effectively capture temporal dependencies in sign language sequences:\n",
    "\n",
    "**Architecture Pipeline:**\n",
    "```\n",
    "Input (MediaPipe landmarks) â†’ Preprocessing â†’ TCN â†’ LSTM â†’ Attention â†’ CNN+Transformer â†’ Pooling â†’ Classification\n",
    "```\n",
    "\n",
    "### Core Components\n",
    "\n",
    "#### 1. Advanced Preprocessing Layer\n",
    "- **Input**: MediaPipe hand and face landmarks (543 points total)\n",
    "- **Landmark Selection**: Focus on key landmarks (hands, eyes, nose, lips) - 62 selected points\n",
    "- **Feature Engineering**:\n",
    "  - **Velocity**: Frame-to-frame motion (dx)\n",
    "  - **Acceleration**: Second-order motion (dx2) \n",
    "  - **Relative Motion**: Inter-landmark movement patterns\n",
    "  - **Temporal Consistency**: Motion coherence across frames\n",
    "  - **Motion Magnitude**: Movement intensity\n",
    "  - **Motion Direction**: Movement orientation\n",
    "- **Normalization**: Relative to nose position for translation invariance\n",
    "- **Output**: 434 features per frame (62 landmarks Ã— 7 feature types)\n",
    "\n",
    "#### 2. Temporal Convolutional Network (TCN)\n",
    "- **3 Sequential Blocks** with increasing dilation rates (1, 2, 4)\n",
    "- **Kernel Size**: 17 for extended temporal receptive field\n",
    "- **Causal Convolutions**: Maintain temporal order\n",
    "- **Gated Activation**: Sigmoid gates control information flow\n",
    "- **Purpose**: Capture local temporal patterns at different scales\n",
    "\n",
    "#### 3. Bidirectional LSTM\n",
    "- **2 Layers** for deep temporal modeling\n",
    "- **Bidirectional**: Processes sequences forward and backward\n",
    "- **Hidden Dimension**: 96 (dim/2) per direction\n",
    "- **Purpose**: Capture long-term dependencies in both directions\n",
    "\n",
    "#### 4. Temporal Attention\n",
    "- **8 Attention Heads** for multi-aspect focus\n",
    "- **Positional Encoding**: Learned temporal positions up to 1000 frames\n",
    "- **Self-Attention**: Focuses on important time steps\n",
    "- **Purpose**: Highlight critical frames for classification\n",
    "\n",
    "#### 5. CNN + Transformer Blocks\n",
    "- **3 Conv1D Blocks**: Depthwise separable convolutions\n",
    "- **1 Transformer Block**: Multi-head self-attention + FFN\n",
    "- **Activation**: Swish (SiLU) for smooth gradients\n",
    "- **Purpose**: Refine feature representations\n",
    "\n",
    "#### 6. Multi-Scale Pooling\n",
    "- **Global Average Pooling**: Overall sequence representation\n",
    "- **Global Max Pooling**: Peak activation capture  \n",
    "- **Attention Pooling**: Learned importance weighting\n",
    "- **Concatenation**: Combines all pooling results (3Ã—dim features)\n",
    "\n",
    "#### 7. Classification Head\n",
    "- **Input**: 576 features (192Ã—3 from pooling)\n",
    "- **Hidden Layer**: 192 units with BatchNorm + SiLU + Dropout(0.3)\n",
    "- **Output**: 250 classes (ASL signs)\n",
    "\n",
    "### Regularization Strategy\n",
    "\n",
    "#### Adaptive Dropout\n",
    "- **Dynamic Rates**: Gradually increases from 0.1 to 0.6 over training\n",
    "- **Warmup Period**: 20-30 epochs for smooth adaptation\n",
    "- **Purpose**: Prevents overfitting while maintaining learning capacity\n",
    "\n",
    "#### Data Augmentation\n",
    "- **Temporal Resampling**: Scale sequences (0.8-1.2x)\n",
    "- **Random Masking**: Hide 5% of frames\n",
    "- **Spatial Affine**: Small rotations, translations, scaling\n",
    "- **Temporal Distortion**: Add temporal noise\n",
    "\n",
    "### Technical Specifications\n",
    "\n",
    "- **Total Parameters**: ~2.1M (optimized for RTX 4070)\n",
    "- **Input Sequence Length**: Up to 384 frames\n",
    "- **Feature Dimension**: 192 throughout the network\n",
    "- **Batch Size**: 32 (memory optimized)\n",
    "- **Training Strategy**: AdamW + CosineAnnealingWarmRestarts\n",
    "- **Early Stopping**: Patience of 20 epochs after minimum 80 epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments & Results\n",
    "\n",
    "This section contains the complete implementation of our ASL recognition model training pipeline. The code is structured as modular components that can be executed step-by-step in a Jupyter notebook environment.\n",
    "\n",
    "### Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÐŸÐ°Ð¼ÑÑ‚ÑŒ kernel'Ð° Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð°\n"
     ]
    }
   ],
   "source": [
    "# ÐŸÐµÑ€Ð²Ð°Ñ ÑÑ‡ÐµÐ¹ÐºÐ° - ÐžÐ‘Ð¯Ð—ÐÐ¢Ð•Ð›Ð¬ÐÐž Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚Ðµ Ð¿ÐµÑ€Ð²Ð¾Ð¹!\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° pandas Ð¸Ð· Ð¿Ð°Ð¼ÑÑ‚Ð¸ kernel'Ð°\n",
    "pandas_modules = [mod for mod in sys.modules.keys() if 'pandas' in mod]\n",
    "for mod in pandas_modules:\n",
    "    if mod in sys.modules:\n",
    "        del sys.modules[mod]\n",
    "\n",
    "# ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° pyarrow\n",
    "pyarrow_modules = [mod for mod in sys.modules.keys() if 'pyarrow' in mod]\n",
    "for mod in pyarrow_modules:\n",
    "    if mod in sys.modules:\n",
    "        del sys.modules[mod]\n",
    "\n",
    "print(\"ÐŸÐ°Ð¼ÑÑ‚ÑŒ kernel'Ð° Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "ðŸš€ Using device: cuda:0\n",
      "   GPU: NVIDIA GeForce RTX 4070\n",
      "   GPU Memory: 12.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from step2_prepare_dataset import load_dataset\n",
    "\n",
    "TEST_MODE = False\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "dtype = torch.float\n",
    "dtype_long = torch.long\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Dropout Implementation\n",
    "\n",
    "**Purpose**: Traditional fixed dropout can be too aggressive early in training and insufficient later. AdaptiveDropout gradually increases dropout rate during training, allowing the model to learn basic patterns first, then applying stronger regularization to prevent overfitting.\n",
    "\n",
    "**Key Features**:\n",
    "- Starts with low dropout (0.1) for initial learning\n",
    "- Gradually increases to high dropout (0.6) over warmup epochs\n",
    "- Prevents overfitting while maintaining learning capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout rates over epochs:\n",
      "Epoch 0: 0.100\n",
      "Epoch 5: 0.183\n",
      "Epoch 10: 0.267\n",
      "Epoch 15: 0.350\n",
      "Epoch 20: 0.433\n",
      "Epoch 25: 0.517\n",
      "Epoch 28: 0.567\n",
      "Epoch 29: 0.583\n",
      "Epoch 30: 0.600\n",
      "Epoch 31: 0.600\n",
      "Epoch 32: 0.600\n",
      "Epoch 33: 0.600\n",
      "Epoch 34: 0.600\n"
     ]
    }
   ],
   "source": [
    "class AdaptiveDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive dropout that gradually increases\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_p=0.1, final_p=0.6, warmup_epochs=30):\n",
    "        super().__init__()\n",
    "        self.initial_p = initial_p\n",
    "        self.final_p = final_p\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # Gradual dropout increase\n",
    "            if self.current_epoch < self.warmup_epochs:\n",
    "                p = self.initial_p + (self.final_p - self.initial_p) * (self.current_epoch / self.warmup_epochs)\n",
    "            else:\n",
    "                p = self.final_p\n",
    "            return F.dropout(x, p=p, training=True)\n",
    "        return x\n",
    "    \n",
    "    def step(self):\n",
    "        self.current_epoch += 1\n",
    "\n",
    "# Test the adaptive dropout behavior\n",
    "adaptive_dropout = AdaptiveDropout()\n",
    "print(\"Dropout rates over epochs:\")\n",
    "for epoch in range(35):\n",
    "    if epoch < 30:\n",
    "        p = 0.1 + (0.6 - 0.1) * (epoch / 30)\n",
    "    else:\n",
    "        p = 0.6\n",
    "    if epoch % 5 == 0 or epoch >= 28:\n",
    "        print(f\"Epoch {epoch}: {p:.3f}\")\n",
    "    adaptive_dropout.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Augmentation Pipeline\n",
    "\n",
    "Advanced preprocessing layer that extracts motion features from ASL landmark sequences and applies temporal-aware data augmentation techniques. The preprocessing layer computes velocity, acceleration, relative motion, temporal consistency, motion magnitude and direction features from 62 key facial and hand landmarks. The augmentation class implements temporal resampling, masking, affine transformations, and temporal distortion while preserving gesture semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced preprocessing layer with explicit temporal dependency modeling\n",
    "    \"\"\"\n",
    "    def __init__(self, max_len=384, point_landmarks=None):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Select key landmarks: hands, eyes, nose, lips\n",
    "        if point_landmarks is None:\n",
    "            # Face landmarks (eyes, nose, lips)\n",
    "            face_landmarks = [33, 133, 362, 263, 61, 291, 199, 419, 17, 84, 17, 314, 405, 320, 307, 375, 321, 308, 324, 318]\n",
    "            # Hand landmarks (key hand points)\n",
    "            left_hand = [501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521]\n",
    "            right_hand = [522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542]\n",
    "            self.point_landmarks = face_landmarks + left_hand + right_hand\n",
    "        else:\n",
    "            self.point_landmarks = point_landmarks\n",
    "    \n",
    "    def compute_motion_features(self, x):\n",
    "        \"\"\"\n",
    "        Compute extended motion features considering neighboring frames\n",
    "        \"\"\"\n",
    "        # x: (batch, seq, landmarks, 2)\n",
    "        batch_size, seq_len, num_landmarks, _ = x.shape\n",
    "        \n",
    "        # Basic motion features (lag1, lag2)\n",
    "        dx = torch.zeros_like(x)\n",
    "        dx2 = torch.zeros_like(x)\n",
    "        \n",
    "        if seq_len > 1:\n",
    "            dx[:, :-1] = x[:, 1:] - x[:, :-1]  # velocity\n",
    "        \n",
    "        if seq_len > 2:\n",
    "            dx2[:, :-2] = x[:, 2:] - x[:, :-2]  # acceleration\n",
    "        \n",
    "        # Extended motion features\n",
    "        # 1. Relative motion (relative movement between landmarks)\n",
    "        relative_motion = torch.zeros_like(x)\n",
    "        if seq_len > 1:\n",
    "            # Compute relative motion between adjacent landmarks\n",
    "            for i in range(num_landmarks - 1):\n",
    "                relative_motion[:, :-1, i] = x[:, 1:, i] - x[:, :-1, i+1]\n",
    "        \n",
    "        # 2. Temporal consistency (temporal consistency)\n",
    "        temporal_consistency = torch.zeros_like(x)\n",
    "        if seq_len > 3:\n",
    "            # Check motion consistency over 3 frames\n",
    "            for t in range(seq_len - 2):\n",
    "                motion1 = x[:, t+1] - x[:, t]\n",
    "                motion2 = x[:, t+2] - x[:, t+1]\n",
    "                # Cosine similarity between motions\n",
    "                cos_sim = F.cosine_similarity(motion1, motion2, dim=-1, eps=1e-8)\n",
    "                temporal_consistency[:, t] = cos_sim.unsqueeze(-1).expand(-1, -1, 2)\n",
    "        \n",
    "        # 3. Motion magnitude (motion magnitude)\n",
    "        motion_magnitude = torch.norm(dx, dim=-1, keepdim=True)\n",
    "        \n",
    "        # 4. Motion direction (motion direction)\n",
    "        motion_direction = torch.atan2(dx[..., 1], dx[..., 0]).unsqueeze(-1)\n",
    "        \n",
    "        return dx, dx2, relative_motion, temporal_consistency, motion_magnitude, motion_direction\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, num_landmarks, 3)\n",
    "        \"\"\"\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)  # (1, seq_len, num_landmarks, 3)\n",
    "        \n",
    "        # Normalization relative to nose (landmark 17)\n",
    "        nose_coords = x[:, :, 17:18, :2]  # (batch, seq, 1, 2)\n",
    "        # Replace NaN with 0.5 for mean calculation\n",
    "        nose_coords_clean = torch.where(torch.isnan(nose_coords), torch.tensor(0.5, device=x.device, dtype=x.dtype), nose_coords)\n",
    "        mean = torch.mean(nose_coords_clean, dim=[1, 2], keepdim=True)  # (batch, 1, 1, 2)\n",
    "        \n",
    "        # Select required landmarks\n",
    "        x = x[:, :, self.point_landmarks, :]  # (batch, seq, num_selected, 3)\n",
    "        \n",
    "        # Standardization - expand mean to x dimensions\n",
    "        mean_expanded = mean.expand(-1, x.shape[1], x.shape[2], -1)  # (batch, seq, num_selected, 2)\n",
    "        \n",
    "        # Replace NaN with 0 for std calculation\n",
    "        x_clean = torch.where(torch.isnan(x), torch.tensor(0.0, device=x.device, dtype=x.dtype), x)\n",
    "        std = torch.std(x_clean, dim=[1, 2], keepdim=True)  # (batch, 1, 1, 3)\n",
    "        std_expanded = std.expand(-1, x.shape[1], x.shape[2], -1)  # (batch, seq, num_selected, 3)\n",
    "        \n",
    "        # Normalize only x, y coordinates (first 2 dimensions)\n",
    "        x_normalized = x.clone()\n",
    "        x_normalized[..., :2] = (x[..., :2] - mean_expanded) / (std_expanded[..., :2] + 1e-8)\n",
    "        x = x_normalized\n",
    "        \n",
    "        # Truncate to max_len\n",
    "        if self.max_len is not None:\n",
    "            x = x[:, :self.max_len]\n",
    "        \n",
    "        length = x.shape[1]\n",
    "        x = x[..., :2]  # Take only x, y coordinates\n",
    "        \n",
    "        # Compute extended motion features\n",
    "        dx, dx2, relative_motion, temporal_consistency, motion_magnitude, motion_direction = self.compute_motion_features(x)\n",
    "        \n",
    "        # Combine all features\n",
    "        x_flat = x.reshape(x.shape[0], length, -1)  # (batch, seq, num_landmarks*2)\n",
    "        dx_flat = dx.reshape(x.shape[0], length, -1)\n",
    "        dx2_flat = dx2.reshape(x.shape[0], length, -1)\n",
    "        relative_motion_flat = relative_motion.reshape(x.shape[0], length, -1)\n",
    "        temporal_consistency_flat = temporal_consistency.reshape(x.shape[0], length, -1)\n",
    "        motion_magnitude_flat = motion_magnitude.reshape(x.shape[0], length, -1)\n",
    "        motion_direction_flat = motion_direction.reshape(x.shape[0], length, -1)\n",
    "        \n",
    "        x_combined = torch.cat([\n",
    "            x_flat, dx_flat, dx2_flat, relative_motion_flat, \n",
    "            temporal_consistency_flat, motion_magnitude_flat, motion_direction_flat\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # Replace NaN with 0\n",
    "        x_combined = torch.where(torch.isnan(x_combined), torch.tensor(0.0, device=x.device, dtype=x.dtype), x_combined)\n",
    "        \n",
    "        return x_combined\n",
    "\n",
    "class Augmentation:\n",
    "    \"\"\"\n",
    "    Simplified data augmentation with preservation of temporal dependencies\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5):  # Increase augmentation probability to combat overfitting\n",
    "        self.p = p\n",
    "    \n",
    "    def temporal_resample(self, x, target_length=None):\n",
    "        \"\"\"Simplified temporal resampling\"\"\"\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "        \n",
    "        if target_length is None:\n",
    "            scale = random.uniform(0.8, 1.2)  # More conservative range\n",
    "            target_length = int(x.shape[0] * scale)\n",
    "        \n",
    "        if target_length <= 0:\n",
    "            return x\n",
    "        \n",
    "        # Simple interpolation\n",
    "        indices = torch.linspace(0, x.shape[0] - 1, target_length)\n",
    "        indices = indices.long().clamp(0, x.shape[0] - 1)\n",
    "        return x[indices]\n",
    "    \n",
    "    def random_masking(self, x, mask_ratio=0.05):  # Increase masking to combat overfitting\n",
    "        \"\"\"Simplified masking\"\"\"\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "        \n",
    "        seq_len = x.shape[0]\n",
    "        num_masks = int(seq_len * mask_ratio)\n",
    "        \n",
    "        if num_masks > 0:\n",
    "            mask_indices = random.sample(range(seq_len), num_masks)\n",
    "            x[mask_indices] = 0\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def random_affine(self, x, max_scale=0.02, max_shift=0.01, max_rotate=2):  # Reduce parameters\n",
    "        \"\"\"Simplified affine transformations\"\"\"\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "        \n",
    "        scale = 1 + random.uniform(-max_scale, max_scale)\n",
    "        shift_x = random.uniform(-max_shift, max_shift)\n",
    "        shift_y = random.uniform(-max_shift, max_shift)\n",
    "        angle = random.uniform(-max_rotate, max_rotate) * math.pi / 180\n",
    "        \n",
    "        x_transformed = x.clone()\n",
    "        x_transformed[..., 0::2] = x_transformed[..., 0::2] * scale + shift_x\n",
    "        x_transformed[..., 1::2] = x_transformed[..., 1::2] * scale + shift_y\n",
    "        \n",
    "        return x_transformed\n",
    "    \n",
    "    def apply_augmentations(self, x):\n",
    "        \"\"\"Apply enhanced augmentations with adaptive probability\"\"\"\n",
    "        # Adaptive augmentation probability\n",
    "        if random.random() < 0.6:  # Increase base probability\n",
    "            x = self.temporal_resample(x)\n",
    "        if random.random() < 0.5:\n",
    "            x = self.random_affine(x)\n",
    "        if random.random() < 0.4:\n",
    "            x = self.random_masking(x)\n",
    "        \n",
    "        # Add new augmentation type - temporal distortion\n",
    "        if random.random() < 0.3:\n",
    "            x = self.temporal_distortion(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def temporal_distortion(self, x, max_shift=0.1):\n",
    "        \"\"\"Temporal distortion for better generalization\"\"\"\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "        \n",
    "        seq_len = x.shape[0]\n",
    "        # Create random shifts for each frame\n",
    "        shifts = torch.randn(seq_len) * max_shift\n",
    "        shifts = torch.cumsum(shifts, dim=0)\n",
    "        \n",
    "        # Apply shifts to coordinates\n",
    "        x_distorted = x.clone()\n",
    "        x_distorted[..., 0::2] += shifts.unsqueeze(-1).unsqueeze(-1)  # x coordinates\n",
    "        x_distorted[..., 1::2] += shifts.unsqueeze(-1).unsqueeze(-1)  # y coordinates\n",
    "        \n",
    "        return x_distorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Convolutional Network Block\n",
    "\n",
    "Temporal convolutional block with dilated causal convolutions for processing sequential landmark data. The block uses gated activation mechanism combining main and gate branches, applies depthwise separable convolutions for efficiency, and maintains causal relationships through proper padding. Includes residual connections, batch normalization, and dropout for stable training while extending receptive field through configurable dilation rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network block with extended receptive field\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, kernel_size=17, dilation=1, drop_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        \n",
    "        # Causal convolution with dilation\n",
    "        self.conv1 = nn.Conv1d(dim, dim, kernel_size, padding=self.padding, \n",
    "                              dilation=dilation, groups=dim)\n",
    "        self.conv2 = nn.Conv1d(dim, dim, 1)  # Pointwise\n",
    "        \n",
    "        # Gated activation\n",
    "        self.gate_conv = nn.Conv1d(dim, dim, kernel_size, padding=self.padding, \n",
    "                                  dilation=dilation, groups=dim)\n",
    "        self.gate_conv2 = nn.Conv1d(dim, dim, 1)\n",
    "        \n",
    "        # Normalization\n",
    "        self.bn = nn.BatchNorm1d(dim, momentum=0.95)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        \n",
    "        # Residual connection\n",
    "        self.residual = nn.Conv1d(dim, dim, 1) if dim != dim else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, dim)\n",
    "        residual = self.residual(x.transpose(1, 2))\n",
    "        \n",
    "        # Causal convolution\n",
    "        x = x.transpose(1, 2)  # (batch, dim, seq)\n",
    "        \n",
    "        # Main branch\n",
    "        conv_out = self.conv1(x)\n",
    "        conv_out = self.conv2(conv_out)\n",
    "        \n",
    "        # Gate branch\n",
    "        gate_out = self.gate_conv(x)\n",
    "        gate_out = self.gate_conv2(gate_out)\n",
    "        gate_out = torch.sigmoid(gate_out)\n",
    "        \n",
    "        # Gated activation\n",
    "        x = conv_out * gate_out\n",
    "        \n",
    "        # Apply causal padding (trim from right)\n",
    "        x = x[:, :, :-self.padding] if self.padding > 0 else x\n",
    "        \n",
    "        # Ensure sequence dimension hasn't changed\n",
    "        if x.shape[-1] != residual.shape[-1]:\n",
    "            # If dimension changed, trim or pad\n",
    "            target_len = residual.shape[-1]\n",
    "            if x.shape[-1] > target_len:\n",
    "                x = x[:, :, :target_len]\n",
    "            else:\n",
    "                # Pad with zeros on the right\n",
    "                padding = torch.zeros(x.shape[0], x.shape[1], target_len - x.shape[2], \n",
    "                                    device=x.device, dtype=x.dtype)\n",
    "                x = torch.cat([x, padding], dim=2)\n",
    "        \n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.transpose(1, 2)  # (batch, seq, dim)\n",
    "        x = x + residual.transpose(1, 2)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM Module\n",
    "\n",
    "Bidirectional LSTM network for capturing long-term temporal dependencies in both forward and backward directions. The module processes sequential landmark data with configurable hidden dimensions and multiple layers, concatenates bidirectional outputs, and projects back to original feature dimension through a linear layer. Includes dropout regularization and batch-first processing for efficient training on gesture sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM for capturing long-term temporal dependencies\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, hidden_dim=None, num_layers=2, drop_rate=0.2):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=drop_rate if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Projection back to original dimension\n",
    "        self.projection = nn.Linear(hidden_dim * 2, dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, dim)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = self.projection(lstm_out)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1D Convolutional Block with Causal Padding\n",
    "\n",
    "Depthwise separable 1D convolutional block designed for causal temporal modeling in ASL sequences. The module combines depthwise and pointwise convolutions with causal padding to preserve temporal order, uses SiLU activation and batch normalization for stable training, and includes residual connections for gradient flow. Features configurable kernel size and dropout regularization while maintaining sequence length consistency through careful padding management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    1D CNN block with depthwise convolution and causal padding\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, kernel_size=17, drop_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size - 1  # Causal padding\n",
    "        \n",
    "        # Depthwise convolution\n",
    "        self.depthwise = nn.Conv1d(dim, dim, kernel_size, padding=self.padding, groups=dim)\n",
    "        self.pointwise = nn.Conv1d(dim, dim, 1)\n",
    "        \n",
    "        # BatchNorm + Swish (as in winner solution)\n",
    "        self.bn = nn.BatchNorm1d(dim, momentum=0.95)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        \n",
    "        # Residual connection\n",
    "        self.residual = nn.Conv1d(dim, dim, 1) if dim != dim else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, dim)\n",
    "        residual = self.residual(x.transpose(1, 2))\n",
    "        \n",
    "        # Causal convolution\n",
    "        x = x.transpose(1, 2)  # (batch, dim, seq)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        \n",
    "        # Apply causal padding (trim from right)\n",
    "        x = x[:, :, :-self.padding] if self.padding > 0 else x\n",
    "        \n",
    "        # Ensure sequence dimension hasn't changed\n",
    "        if x.shape[-1] != residual.shape[-1]:\n",
    "            # If dimension changed, trim or pad\n",
    "            target_len = residual.shape[-1]\n",
    "            if x.shape[-1] > target_len:\n",
    "                x = x[:, :, :target_len]\n",
    "            else:\n",
    "                # Pad with zeros on the right\n",
    "                padding = torch.zeros(x.shape[0], x.shape[1], target_len - x.shape[2], \n",
    "                                    device=x.device, dtype=x.dtype)\n",
    "                x = torch.cat([x, padding], dim=2)\n",
    "        \n",
    "        x = self.bn(x)\n",
    "        x = F.silu(x)  # Swish activation\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.transpose(1, 2)  # (batch, seq, dim)\n",
    "        x = x + residual.transpose(1, 2)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Transformer Block with BatchNorm and SiLU Activation\n",
    "\n",
    "Standard transformer block featuring multi-head self-attention and feed-forward network with architectural optimizations for ASL recognition. Uses BatchNorm instead of LayerNorm for improved training stability on sequential data, applies SiLU activation functions throughout the network, and includes configurable expansion ratios in the feed-forward layers. Features residual connections and dropout regularization for robust sequence-to-sequence modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with BatchNorm + Swish\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, expand=2, drop_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(dim, num_heads, batch_first=True, dropout=drop_rate)\n",
    "        self.attention_norm = nn.BatchNorm1d(dim, momentum=0.95)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim * expand),\n",
    "            nn.SiLU(),  # Swish\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(dim * expand, dim),\n",
    "            nn.Dropout(drop_rate)\n",
    "        )\n",
    "        self.ffn_norm = nn.BatchNorm1d(dim, momentum=0.95)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, dim)\n",
    "        residual = x\n",
    "        \n",
    "        # Self-attention\n",
    "        x_attn, _ = self.attention(x, x, x)\n",
    "        x = x + x_attn\n",
    "        x = self.attention_norm(x.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        # Feed-forward\n",
    "        x_ffn = self.ffn(x)\n",
    "        x = x + x_ffn\n",
    "        x = self.ffn_norm(x.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Attention Module\n",
    "\n",
    "Multi-head attention mechanism for identifying and focusing on the most important frames within ASL gesture sequences. The module incorporates learnable temporal position encodings for sequences up to 1000 frames, applies scaled dot-product attention across the temporal dimension, and uses multiple attention heads to capture different types of temporal relationships. Features output projection and dropout regularization for robust sequence modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Attention for focusing on important frames\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, drop_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        \n",
    "        # Temporal attention\n",
    "        self.temporal_q = nn.Linear(dim, dim)\n",
    "        self.temporal_k = nn.Linear(dim, dim)\n",
    "        self.temporal_v = nn.Linear(dim, dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        \n",
    "        # Temporal position encoding\n",
    "        self.temporal_pos_enc = nn.Parameter(torch.randn(1, 1000, dim))  # Max sequence length\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, dim)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Add temporal position encoding\n",
    "        if seq_len <= self.temporal_pos_enc.shape[1]:\n",
    "            pos_enc = self.temporal_pos_enc[:, :seq_len]\n",
    "            x = x + pos_enc\n",
    "        \n",
    "        # Temporal attention\n",
    "        q = self.temporal_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.temporal_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.temporal_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.dim)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.output_proj(attn_output)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Enhanced ASL Recognition Model\n",
    "\n",
    "Comprehensive ASL recognition architecture combining temporal convolutional networks, bidirectional LSTM, multi-head attention, and transformer blocks for robust gesture classification. The model processes landmark sequences through preprocessing, applies temporal convolutions with increasing dilation rates, captures long-term dependencies via bidirectional LSTM, focuses on important frames using temporal attention, and combines multiple pooling strategies including adaptive average, max, and attention pooling. Features adaptive dropout regularization, proper weight initialization, and a multi-layer classifier for 250 ASL word classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced model with adaptive regularization and improved architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_classes, max_len=384, dim=192, dropout_step=0):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Preprocessing\n",
    "        self.preprocessing = PreprocessingLayer(max_len)\n",
    "        \n",
    "        # Stem with improved initialization\n",
    "        self.stem_conv = nn.Linear(input_dim, dim, bias=False)\n",
    "        self.stem_bn = nn.BatchNorm1d(dim, momentum=0.95)\n",
    "        self.stem_dropout = AdaptiveDropout(initial_p=0.1, final_p=0.3, warmup_epochs=20)\n",
    "        \n",
    "        # Temporal Convolutional Network (TCN) - 3 blocks with different dilations\n",
    "        self.tcn1 = TemporalConvBlock(dim, kernel_size=17, dilation=1, drop_rate=0.15)\n",
    "        self.tcn2 = TemporalConvBlock(dim, kernel_size=17, dilation=2, drop_rate=0.2)\n",
    "        self.tcn3 = TemporalConvBlock(dim, kernel_size=17, dilation=4, drop_rate=0.25)\n",
    "        \n",
    "        # Projection for attention pooling (in case of dimension change)\n",
    "        self.attention_projection = nn.Linear(dim, dim)\n",
    "        \n",
    "        # Bidirectional LSTM - 2 layers for better dependency capture\n",
    "        self.lstm = BidirectionalLSTM(dim, hidden_dim=dim//2, num_layers=2, drop_rate=0.15)\n",
    "        \n",
    "        # Temporal Attention - more heads for better attention\n",
    "        self.temporal_attention = TemporalAttention(dim, num_heads=8, drop_rate=0.15)\n",
    "        \n",
    "        # 1D CNN + Transformer blocks - add another conv block\n",
    "        ksize = 17\n",
    "        \n",
    "        self.conv1 = Conv1DBlock(dim, ksize, drop_rate=0.15)\n",
    "        self.conv2 = Conv1DBlock(dim, ksize, drop_rate=0.2)\n",
    "        self.conv3 = Conv1DBlock(dim, ksize, drop_rate=0.25)\n",
    "        self.transformer1 = TransformerBlock(dim, expand=2, drop_rate=0.15)\n",
    "        \n",
    "        # Top layers with improved pooling\n",
    "        self.top_conv = nn.Linear(dim, dim)\n",
    "        self.top_bn = nn.BatchNorm1d(dim, momentum=0.95)\n",
    "        self.adaptive_dropout = AdaptiveDropout(initial_p=0.2, final_p=0.5, warmup_epochs=25)\n",
    "        \n",
    "        # Improved pooling - add attention pooling\n",
    "        self.temporal_pool1 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.temporal_pool2 = nn.AdaptiveMaxPool1d(1)\n",
    "        self.attention_pool = nn.MultiheadAttention(dim, num_heads=4, batch_first=True, dropout=0.1)\n",
    "        \n",
    "        # Final classifier with improved architecture\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(dim * 3, dim),  # dim*3 for avg + max + attention pooling\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Weight initialization\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Conv1d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                init.ones_(m.weight)\n",
    "                init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LSTM):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        init.orthogonal_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        init.zeros_(param)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, landmarks, 3)\n",
    "        \n",
    "        # Preprocessing\n",
    "        x = self.preprocessing(x)  # (batch, seq, features)\n",
    "        \n",
    "        # Stem with adaptive dropout\n",
    "        x = self.stem_conv(x)\n",
    "        x = self.stem_bn(x.transpose(1, 2)).transpose(1, 2)\n",
    "        x = self.stem_dropout(x)\n",
    "        \n",
    "        # TCN blocks - 3 blocks with different dilations\n",
    "        x = self.tcn1(x)\n",
    "        x = self.tcn2(x)\n",
    "        x = self.tcn3(x)\n",
    "        \n",
    "        # Bidirectional LSTM - 2 layers\n",
    "        x = self.lstm(x)\n",
    "        \n",
    "        # Temporal Attention - more heads\n",
    "        x = self.temporal_attention(x)\n",
    "        \n",
    "        # 1D CNN + Transformer - add third conv block\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.transformer1(x)\n",
    "        \n",
    "        # Top layers with improved pooling\n",
    "        x = self.top_conv(x)  # (batch, seq, dim)\n",
    "        x = self.top_bn(x.transpose(1, 2)).transpose(1, 2)\n",
    "        x = self.adaptive_dropout(x)\n",
    "        \n",
    "        # Improved temporal pooling\n",
    "        x_transposed = x.transpose(1, 2)  # (batch, dim, seq)\n",
    "        \n",
    "        # Global average pooling\n",
    "        global_avg = self.temporal_pool1(x_transposed).squeeze(-1)  # (batch, dim)\n",
    "        \n",
    "        # Global max pooling\n",
    "        global_max = self.temporal_pool2(x_transposed).squeeze(-1)  # (batch, dim)\n",
    "        \n",
    "        # Attention pooling - use projection for stability\n",
    "        x_for_attn = self.attention_projection(x)  # x already has dimension (batch, seq, dim)\n",
    "        \n",
    "        attn_out, _ = self.attention_pool(x_for_attn, x_for_attn, x_for_attn)\n",
    "        global_attn = torch.mean(attn_out, dim=1)  # (batch, dim)\n",
    "        \n",
    "        # Combine pooling results\n",
    "        x_pooled = torch.cat([global_avg, global_max, global_attn], dim=1)  # (batch, dim*3)\n",
    "        \n",
    "        x = self.classifier(x_pooled)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Cosine Annealing Warm Restarts Scheduler\n",
    "\n",
    "Custom learning rate scheduler implementing cosine annealing with periodic warm restarts for improved training dynamics. The scheduler reduces learning rate following a cosine curve within each restart period, then jumps back to the initial rate to escape local minima. Features configurable restart period multiplication and minimum learning rate bounds, enabling the model to explore different regions of the loss landscape and potentially achieve better convergence in ASL recognition training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENHANCED TRAINING STRATEGY\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class CosineAnnealingWarmRestarts(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Enhanced scheduler with warm restarts\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1):\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_min = eta_min\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.last_epoch == 0:\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "        \n",
    "        T_cur = self.last_epoch\n",
    "        T_i = self.T_0\n",
    "        \n",
    "        # Find current period\n",
    "        while T_cur >= T_i:\n",
    "            T_cur -= T_i\n",
    "            T_i *= self.T_mult\n",
    "        \n",
    "        return [self.eta_min + (base_lr - self.eta_min) * \n",
    "                (1 + math.cos(math.pi * T_cur / T_i)) / 2\n",
    "                for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASL Dataset and Batch Collation\n",
    "\n",
    "PyTorch Dataset implementation for ASL gesture sequences with optional data augmentation and custom batch collation for variable-length sequences. The dataset applies temporal augmentations during training through the Augmentation class, while the collate function handles sequences of different lengths by padding shorter sequences with zeros to match the maximum sequence length in each batch, enabling efficient batch processing for the ASL recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET AND DATALOADER\n",
    "# ============================================================================\n",
    "\n",
    "class ASLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for ASL gestures\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, labels, augment=True):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "        self.augmentor = Augmentation(p=0.5) if augment else None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.augment and self.augmentor:\n",
    "            # Apply augmentation\n",
    "            sequence = self.augmentor.apply_augmentations(sequence)\n",
    "        \n",
    "        return sequence, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for batches of different lengths\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # Find maximum length in batch\n",
    "    max_len = max(seq.shape[0] for seq in sequences)\n",
    "    \n",
    "    # Padding to maximum length\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if seq.shape[0] < max_len:\n",
    "            # Zero padding\n",
    "            padding = torch.zeros((max_len - seq.shape[0], seq.shape[1], seq.shape[2]), \n",
    "                                dtype=seq.dtype, device=seq.device)\n",
    "            seq = torch.cat([seq, padding], dim=0)\n",
    "        padded_sequences.append(seq)\n",
    "    \n",
    "    # Stack into batch\n",
    "    batch_sequences = torch.stack(padded_sequences)\n",
    "    batch_labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return batch_sequences, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Training Pipeline with Advanced Optimization Strategies\n",
    "\n",
    "Comprehensive training framework featuring epoch-based training and validation functions with adaptive dropout scheduling, gradient clipping, and progress tracking. The main training function implements enhanced optimization strategies including cosine annealing with warm restarts, learning rate warmup, early stopping with patience monitoring, automatic model checkpointing, and detailed experiment tracking through JSON manifests. Features label smoothing, weight decay regularization, and comprehensive training history visualization for robust ASL gesture recognition model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Training one epoch with enhanced strategy\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Update adaptive dropout layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, AdaptiveDropout):\n",
    "            module.step()\n",
    "    \n",
    "    total_batches = len(dataloader)\n",
    "    print(f'Starting Epoch {epoch} with {total_batches} batches...')\n",
    "    \n",
    "    for batch_idx, (sequences, labels) in enumerate(dataloader):\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Print progress every 10% of batches or every 50 batches (whichever is smaller)\n",
    "        print_interval = min(50, max(1, total_batches // 10))\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == total_batches:\n",
    "            accuracy = 100. * correct / total\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            progress = 100. * (batch_idx + 1) / total_batches\n",
    "            print(f'  Batch {batch_idx + 1}/{total_batches} ({progress:.1f}%) - '\n",
    "                  f'Loss: {avg_loss:.4f}, Acc: {accuracy:.2f}%')\n",
    "    \n",
    "    return total_loss / len(dataloader), 100. * correct / total\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validation of one epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    total_batches = len(dataloader)\n",
    "    print('Starting validation...')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (sequences, labels) in enumerate(dataloader):\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Print progress every 20% of batches\n",
    "            print_interval = max(1, total_batches // 5)\n",
    "            if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == total_batches:\n",
    "                progress = 100. * (batch_idx + 1) / total_batches\n",
    "                print(f'  Validation progress: {progress:.1f}%')\n",
    "    \n",
    "    return total_loss / len(dataloader), 100. * correct / total\n",
    "\n",
    "def train_model(train_data, train_labels, test_data, test_labels, num_classes, \n",
    "                epochs=400, batch_size=32, lr=5e-4, max_len=384, timestamp=None, model_prefix=None):\n",
    "    \"\"\"\n",
    "    Main training function for simplified but effective ASL model\n",
    "    \"\"\"\n",
    "    print(f\"ðŸŽ¯ Starting training of simplified ASL model with focus on temporal dependencies\")\n",
    "    print(f\"   Epochs: {epochs}\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Learning rate: {lr}\")\n",
    "    print(f\"   Classes: {num_classes}\")\n",
    "    print(f\"   Architecture: TCN + LSTM + Transformer (simplified)\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ASLDataset(train_data, train_labels, augment=True)\n",
    "    test_dataset = ASLDataset(test_data, test_labels, augment=False)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             collate_fn=collate_fn, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=collate_fn, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    # Determine input dimension (after enhanced preprocessing)\n",
    "    sample_sequence = train_data[0]\n",
    "    sample_preprocessed = PreprocessingLayer(max_len)(sample_sequence.unsqueeze(0))\n",
    "    input_dim = sample_preprocessed.shape[-1]\n",
    "    \n",
    "    print(f\"   Input dimension after enhanced preprocessing: {input_dim}\")\n",
    "    \n",
    "    # Create simplified model\n",
    "    model = ASLModel(input_dim=input_dim, num_classes=num_classes, max_len=max_len, dim=192)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Loss function with smaller label smoothing\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    \n",
    "    # Optimizer with improved parameters\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.005, betas=(0.9, 0.999))\n",
    "    \n",
    "    # Enhanced scheduler with warm restarts\n",
    "    warmup_epochs = 10\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=lr*0.01)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Use passed parameters for versioning\n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if model_prefix is None:\n",
    "        model_prefix = f\"asl_model_v{timestamp}\"\n",
    "    \n",
    "    best_model_path = f\"models/{model_prefix}.pth\"\n",
    "    manifest_path = f\"models/{model_prefix}_manifest.json\"\n",
    "    \n",
    "    print(f\"\\nðŸš€ Starting simplified model training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Learning rate warmup\n",
    "        if epoch < warmup_epochs:\n",
    "            lr_scale = (epoch + 1) / warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr * lr_scale\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Training\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc = validate_epoch(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Save history\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Print results\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Create manifest with current results\n",
    "            manifest = {}\n",
    "            \n",
    "            # Add training results\n",
    "            manifest[\"training_results\"] = {\n",
    "                \"best_epoch\": epoch,\n",
    "                \"best_val_accuracy\": val_acc,\n",
    "                \"best_train_accuracy\": train_acc,\n",
    "                \"current_train_loss\": train_loss,\n",
    "                \"current_val_loss\": val_loss,\n",
    "                \"training_progress\": {\n",
    "                    \"epochs_completed\": epoch + 1,\n",
    "                    \"total_epochs\": epochs,\n",
    "                    \"patience_counter\": patience_counter\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Save model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'train_acc': train_acc,\n",
    "                'architecture': 'Simplified_TCN_LSTM_Transformer',\n",
    "                'manifest': manifest\n",
    "            }, best_model_path)\n",
    "            \n",
    "            # Save manifest separately\n",
    "            with open(manifest_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"  ðŸ’¾ Best model saved (Val Acc: {val_acc:.2f}%)\")\n",
    "            print(f\"  ðŸ“„ Manifest saved: {manifest_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Stricter early stopping to combat overfitting\n",
    "        if epoch > 80 and patience_counter > 20:  # 20 epochs without improvement\n",
    "            print(f\"  âš ï¸ Early stopping at epoch {epoch+1} (no improvement for {patience_counter} epochs)\")\n",
    "            break\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nâœ… Training completed in {training_time/3600:.2f} hours\")\n",
    "    print(f\"   Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"   Improvement over baseline model: +{best_val_acc - 65:.2f}%\")\n",
    "    \n",
    "    # Create final manifest with complete results\n",
    "    final_manifest = {}\n",
    "    \n",
    "    # Add final results\n",
    "    final_manifest[\"final_results\"] = {\n",
    "        \"training_time_hours\": training_time / 3600,\n",
    "        \"best_val_accuracy\": best_val_acc,\n",
    "        \"improvement_over_baseline\": best_val_acc - 65,\n",
    "        \"total_epochs_trained\": len(train_losses),\n",
    "        \"early_stopping_triggered\": len(train_losses) < epochs,\n",
    "        \"final_epoch\": len(train_losses) - 1,\n",
    "        \"training_history\": {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_accuracies\": train_accuracies,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"val_accuracies\": val_accuracies\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save final manifest\n",
    "    final_manifest_path = f\"models/{model_prefix}_final_manifest.json\"\n",
    "    with open(final_manifest_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_manifest, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"  ðŸ“„ Final manifest saved: {final_manifest_path}\")\n",
    "    \n",
    "    # Plots\n",
    "    plot_training_history(train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "    \n",
    "    return model, best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training History Visualization\n",
    "\n",
    "Utility function for plotting training and validation metrics over epochs using matplotlib. Creates side-by-side subplots displaying loss and accuracy curves for both training and validation sets, with proper legends, grid lines, and formatting. Saves the visualization as a high-resolution PNG file and displays the plots for real-time monitoring of model performance and convergence during ASL recognition training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(train_losses, train_accuracies, val_losses, val_accuracies):\n",
    "    \"\"\"\n",
    "    Plot training history graphs\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(train_losses, label='Train Loss', color='blue')\n",
    "    ax1.plot(val_losses, label='Val Loss', color='red')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(train_accuracies, label='Train Accuracy', color='blue')\n",
    "    ax2.plot(val_accuracies, label='Val Accuracy', color='red')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Execution Pipeline\n",
    "\n",
    "Main execution block that orchestrates the complete ASL recognition training workflow. Loads the dataset with configurable sample limits for testing, creates model directory structure, generates timestamped model identifiers for version control, and executes the full training pipeline with optimized hyperparameters for RTX 4070. Provides comprehensive progress reporting including dataset statistics, training completion metrics, model persistence paths, and performance improvements over baseline accuracy thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Google ASL Recognition - Training enhanced model with adaptive regularization\n",
      "================================================================================\n",
      "ðŸ“ Loading dataset...\n",
      "ðŸš€ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Google ASL Signs dataset (Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ)...\n",
      "ðŸ“Š Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ 25 Ð·Ð½Ð°ÐºÐ¾Ð²\n",
      "\n",
      "ðŸ“ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…...\n",
      "   ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ 7134 Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² train.csv\n",
      "   ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ 7134 Ñ„Ð°Ð¹Ð»Ð¾Ð²...\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 0/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 0, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 50/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 50, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 100/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 100, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 150/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 150, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 200/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 200, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 250/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 250, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 300/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 300, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 350/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 350, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 400/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 400, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 450/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 450, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 500/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 500, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 550/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 550, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 600/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 600, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 650/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 650, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 700/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 700, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 750/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 750, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 800/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 800, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 850/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 850, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 900/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 900, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 950/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 950, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1000/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1000, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1050/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1050, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1100/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1100, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1150/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1150, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1200/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1200, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1250/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1250, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1300/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1300, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1350/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1350, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1400/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1400, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1450/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1450, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1500/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1500, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1550/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1550, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1600/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1600, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1650/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1650, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1700/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1700, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1750/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1750, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1800/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1800, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1850/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1850, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1900/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1900, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1950/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1950, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2000/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2000, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2050/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2050, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2100/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2100, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2150/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2150, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2200/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2200, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2250/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2250, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2300/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2300, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2350/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2350, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2400/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2400, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2450/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2450, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2500/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2500, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2550/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2550, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2600/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2600, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2650/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2650, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2700/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2700, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2750/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2750, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2800/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2800, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2850/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2850, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2900/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2900, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2950/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2950, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3000/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3000, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3050/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3050, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3100/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3100, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3150/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3150, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3200/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3200, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3250/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3250, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3300/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3300, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3350/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3350, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3400/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3400, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3450/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3450, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3500/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3500, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3550/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3550, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3600/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3600, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3650/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3650, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3700/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3700, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3750/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3750, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3800/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3800, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3850/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3850, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3900/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3900, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 3950/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 3950, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4000/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4000, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4050/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4050, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4100/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4100, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4150/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4150, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4200/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4200, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4250/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4250, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4300/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4300, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4350/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4350, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4400/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4400, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4450/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4450, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4500/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4500, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4550/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4550, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4600/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4600, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4650/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4650, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4700/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4700, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4750/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4750, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4800/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4800, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4850/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4850, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4900/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4900, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 4950/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 4950, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5000/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5000, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5050/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5050, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5100/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5100, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5150/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5150, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5200/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5200, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5250/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5250, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5300/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5300, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5350/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5350, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5400/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5400, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5450/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5450, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5500/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5500, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5550/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5550, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5600/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5600, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5650/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5650, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5700/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5700, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5750/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5750, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5800/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5800, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5850/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5850, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5900/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5900, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 5950/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 5950, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6000/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6000, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6050/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6050, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6100/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6100, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6150/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6150, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6200/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6200, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6250/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6250, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6300/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6300, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6350/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6350, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6400/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6400, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6450/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6450, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6500/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6500, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6550/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6550, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6600/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6600, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6650/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6650, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6700/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6700, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6750/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6750, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6800/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6800, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6850/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6850, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6900/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6900, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 6950/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 6950, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 7000/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 7000, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 7050/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 7050, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 7100/7134 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 7100, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°: ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ 7134, Ð¾ÑˆÐ¸Ð±Ð¾Ðº 0\n",
      "\n",
      "ðŸ“ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…...\n",
      "   ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ 2376 Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² test/train.csv\n",
      "   ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ 2376 Ñ„Ð°Ð¹Ð»Ð¾Ð²...\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 0/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 0, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 50/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 50, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 100/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 100, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 150/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 150, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 200/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 200, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 250/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 250, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 300/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 300, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 350/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 350, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 400/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 400, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 450/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 450, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 500/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 500, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 550/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 550, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 600/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 600, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 650/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 650, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 700/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 700, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 750/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 750, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 800/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 800, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 850/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 850, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 900/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 900, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 950/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 950, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1000/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1000, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1050/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1050, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1100/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1100, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1150/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1150, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1200/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1200, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1250/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1250, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1300/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1300, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1350/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1350, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1400/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1400, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1450/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1450, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1500/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1500, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1550/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1550, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1600/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1600, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1650/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1650, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1700/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1700, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1750/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1750, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1800/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1800, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1850/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1850, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1900/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1900, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 1950/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 1950, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2000/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2000, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2050/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2050, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2100/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2100, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2150/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2150, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2200/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2200, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2250/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2250, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2300/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2300, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 2350/2376 (ÑƒÑÐ¿ÐµÑˆÐ½Ð¾: 2350, Ð¾ÑˆÐ¸Ð±Ð¾Ðº: 0)\n",
      "   Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°: ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ 2376, Ð¾ÑˆÐ¸Ð±Ð¾Ðº 0\n",
      "\n",
      "âœ… Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°!\n",
      "   Ð¢Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ñ‹Ñ… Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÐµÐ¹: 7134\n",
      "   Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÐµÐ¹: 2376\n",
      "   ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ»Ð°ÑÑÐ¾Ð²: 25\n",
      "   ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ð½Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸: 384\n",
      "âœ… Loaded:\n",
      "   Training samples: 7134\n",
      "   Test samples: 2376\n",
      "   Classes: 25\n",
      "   Classes: ['hello', 'please', 'thankyou', 'bye', 'mom', 'dad', 'boy', 'girl', 'man', 'child', 'drink', 'sleep', 'go', 'happy', 'sad', 'hungry', 'thirsty', 'sick', 'bad', 'red', 'blue', 'green', 'yellow', 'black', 'white']\n",
      "ðŸŽ¯ Starting training of simplified ASL model with focus on temporal dependencies\n",
      "   Epochs: 300\n",
      "   Batch size: 32\n",
      "   Learning rate: 0.0004\n",
      "   Classes: 25\n",
      "   Architecture: TCN + LSTM + Transformer (simplified)\n",
      "   Input dimension after enhanced preprocessing: 744\n",
      "   Total parameters: 1,968,601\n",
      "   Trainable parameters: 1,968,601\n",
      "\n",
      "ðŸš€ Starting simplified model training...\n",
      "Starting Epoch 0 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 3.6129, Acc: 3.84%\n",
      "  Batch 44/223 (19.7%) - Loss: 3.6293, Acc: 3.84%\n",
      "  Batch 66/223 (29.6%) - Loss: 3.6035, Acc: 4.36%\n",
      "  Batch 88/223 (39.5%) - Loss: 3.5966, Acc: 4.19%\n",
      "  Batch 110/223 (49.3%) - Loss: 3.5791, Acc: 4.20%\n",
      "  Batch 132/223 (59.2%) - Loss: 3.5609, Acc: 4.59%\n",
      "  Batch 154/223 (69.1%) - Loss: 3.5531, Acc: 4.57%\n",
      "  Batch 176/223 (78.9%) - Loss: 3.5398, Acc: 4.53%\n",
      "  Batch 198/223 (88.8%) - Loss: 3.5272, Acc: 4.78%\n",
      "  Batch 220/223 (98.7%) - Loss: 3.5223, Acc: 4.82%\n",
      "  Batch 223/223 (100.0%) - Loss: 3.5200, Acc: 4.82%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 1/300:\n",
      "  Train Loss: 3.5200, Train Acc: 4.82%\n",
      "  Val Loss: 4.8805, Val Acc: 6.99%\n",
      "  LR: 0.000040\n",
      "  ðŸ’¾ Best model saved (Val Acc: 6.99%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 1 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 3.3781, Acc: 5.82%\n",
      "  Batch 44/223 (19.7%) - Loss: 3.3123, Acc: 7.53%\n",
      "  Batch 66/223 (29.6%) - Loss: 3.3069, Acc: 8.38%\n",
      "  Batch 88/223 (39.5%) - Loss: 3.2712, Acc: 9.06%\n",
      "  Batch 110/223 (49.3%) - Loss: 3.2437, Acc: 9.29%\n",
      "  Batch 132/223 (59.2%) - Loss: 3.2315, Acc: 9.52%\n",
      "  Batch 154/223 (69.1%) - Loss: 3.2050, Acc: 9.60%\n",
      "  Batch 176/223 (78.9%) - Loss: 3.1880, Acc: 9.48%\n",
      "  Batch 198/223 (88.8%) - Loss: 3.1730, Acc: 9.47%\n",
      "  Batch 220/223 (98.7%) - Loss: 3.1517, Acc: 9.66%\n",
      "  Batch 223/223 (100.0%) - Loss: 3.1485, Acc: 9.66%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 2/300:\n",
      "  Train Loss: 3.1485, Train Acc: 9.66%\n",
      "  Val Loss: 3.0231, Val Acc: 11.24%\n",
      "  LR: 0.000080\n",
      "  ðŸ’¾ Best model saved (Val Acc: 11.24%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 2 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 2.9859, Acc: 12.78%\n",
      "  Batch 44/223 (19.7%) - Loss: 2.9096, Acc: 13.42%\n",
      "  Batch 66/223 (29.6%) - Loss: 2.8631, Acc: 15.25%\n",
      "  Batch 88/223 (39.5%) - Loss: 2.8347, Acc: 15.84%\n",
      "  Batch 110/223 (49.3%) - Loss: 2.7992, Acc: 16.73%\n",
      "  Batch 132/223 (59.2%) - Loss: 2.7642, Acc: 17.52%\n",
      "  Batch 154/223 (69.1%) - Loss: 2.7283, Acc: 18.34%\n",
      "  Batch 176/223 (78.9%) - Loss: 2.6990, Acc: 19.34%\n",
      "  Batch 198/223 (88.8%) - Loss: 2.6791, Acc: 19.84%\n",
      "  Batch 220/223 (98.7%) - Loss: 2.6611, Acc: 20.47%\n",
      "  Batch 223/223 (100.0%) - Loss: 2.6592, Acc: 20.58%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 3/300:\n",
      "  Train Loss: 2.6592, Train Acc: 20.58%\n",
      "  Val Loss: 2.4367, Val Acc: 26.60%\n",
      "  LR: 0.000120\n",
      "  ðŸ’¾ Best model saved (Val Acc: 26.60%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 3 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 2.3930, Acc: 27.98%\n",
      "  Batch 44/223 (19.7%) - Loss: 2.4016, Acc: 29.55%\n",
      "  Batch 66/223 (29.6%) - Loss: 2.3828, Acc: 29.55%\n",
      "  Batch 88/223 (39.5%) - Loss: 2.3706, Acc: 29.79%\n",
      "  Batch 110/223 (49.3%) - Loss: 2.3717, Acc: 29.40%\n",
      "  Batch 132/223 (59.2%) - Loss: 2.3654, Acc: 29.64%\n",
      "  Batch 154/223 (69.1%) - Loss: 2.3544, Acc: 29.71%\n",
      "  Batch 176/223 (78.9%) - Loss: 2.3450, Acc: 29.95%\n",
      "  Batch 198/223 (88.8%) - Loss: 2.3404, Acc: 29.88%\n",
      "  Batch 220/223 (98.7%) - Loss: 2.3306, Acc: 30.34%\n",
      "  Batch 223/223 (100.0%) - Loss: 2.3294, Acc: 30.40%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 4/300:\n",
      "  Train Loss: 2.3294, Train Acc: 30.40%\n",
      "  Val Loss: 2.3636, Val Acc: 28.58%\n",
      "  LR: 0.000160\n",
      "  ðŸ’¾ Best model saved (Val Acc: 28.58%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 4 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 2.2080, Acc: 35.51%\n",
      "  Batch 44/223 (19.7%) - Loss: 2.1734, Acc: 36.22%\n",
      "  Batch 66/223 (29.6%) - Loss: 2.1431, Acc: 36.17%\n",
      "  Batch 88/223 (39.5%) - Loss: 2.1446, Acc: 35.69%\n",
      "  Batch 110/223 (49.3%) - Loss: 2.1417, Acc: 36.22%\n",
      "  Batch 132/223 (59.2%) - Loss: 2.1380, Acc: 36.51%\n",
      "  Batch 154/223 (69.1%) - Loss: 2.1344, Acc: 36.73%\n",
      "  Batch 176/223 (78.9%) - Loss: 2.1291, Acc: 36.93%\n",
      "  Batch 198/223 (88.8%) - Loss: 2.1237, Acc: 37.03%\n",
      "  Batch 220/223 (98.7%) - Loss: 2.1173, Acc: 37.46%\n",
      "  Batch 223/223 (100.0%) - Loss: 2.1169, Acc: 37.48%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 5/300:\n",
      "  Train Loss: 2.1169, Train Acc: 37.48%\n",
      "  Val Loss: 2.0970, Val Acc: 36.57%\n",
      "  LR: 0.000200\n",
      "  ðŸ’¾ Best model saved (Val Acc: 36.57%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 5 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.9485, Acc: 40.48%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.9511, Acc: 41.97%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.9628, Acc: 42.47%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.9617, Acc: 42.12%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.9621, Acc: 42.22%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.9565, Acc: 42.47%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.9603, Acc: 42.63%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.9495, Acc: 42.95%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.9455, Acc: 43.23%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.9473, Acc: 43.54%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.9478, Acc: 43.57%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 6/300:\n",
      "  Train Loss: 1.9478, Train Acc: 43.57%\n",
      "  Val Loss: 1.9955, Val Acc: 44.78%\n",
      "  LR: 0.000240\n",
      "  ðŸ’¾ Best model saved (Val Acc: 44.78%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 6 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.8842, Acc: 45.74%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.8880, Acc: 45.81%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.8773, Acc: 45.79%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.8870, Acc: 45.24%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.8879, Acc: 45.57%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.8826, Acc: 45.98%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.8719, Acc: 46.49%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.8584, Acc: 46.96%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.8586, Acc: 47.06%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.8508, Acc: 47.34%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.8490, Acc: 47.43%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 7/300:\n",
      "  Train Loss: 1.8490, Train Acc: 47.43%\n",
      "  Val Loss: 1.9141, Val Acc: 44.44%\n",
      "  LR: 0.000280\n",
      "--------------------------------------------------\n",
      "Starting Epoch 7 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.8295, Acc: 50.00%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.7625, Acc: 51.07%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.7475, Acc: 50.43%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.7464, Acc: 50.53%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.7651, Acc: 50.60%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.7498, Acc: 51.61%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.7514, Acc: 51.28%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.7586, Acc: 51.08%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.7615, Acc: 51.07%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.7626, Acc: 50.97%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.7635, Acc: 50.95%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 8/300:\n",
      "  Train Loss: 1.7635, Train Acc: 50.95%\n",
      "  Val Loss: 1.8955, Val Acc: 45.45%\n",
      "  LR: 0.000320\n",
      "  ðŸ’¾ Best model saved (Val Acc: 45.45%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 8 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.6522, Acc: 53.84%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.6881, Acc: 52.13%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.7008, Acc: 52.27%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.7083, Acc: 52.38%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.7113, Acc: 52.53%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.7202, Acc: 52.91%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.7204, Acc: 52.68%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.7210, Acc: 52.59%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.7168, Acc: 52.67%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.7097, Acc: 52.88%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.7103, Acc: 52.85%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 9/300:\n",
      "  Train Loss: 1.7103, Train Acc: 52.85%\n",
      "  Val Loss: 1.8808, Val Acc: 46.93%\n",
      "  LR: 0.000360\n",
      "  ðŸ’¾ Best model saved (Val Acc: 46.93%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 9 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.6473, Acc: 56.53%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.6702, Acc: 55.54%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.6667, Acc: 55.26%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.6576, Acc: 55.08%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.6654, Acc: 54.83%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.6575, Acc: 54.88%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.6710, Acc: 54.44%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.6751, Acc: 54.46%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.6684, Acc: 54.59%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.6644, Acc: 54.62%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.6651, Acc: 54.54%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 10/300:\n",
      "  Train Loss: 1.6651, Train Acc: 54.54%\n",
      "  Val Loss: 1.8273, Val Acc: 49.54%\n",
      "  LR: 0.000400\n",
      "  ðŸ’¾ Best model saved (Val Acc: 49.54%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 10 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.6264, Acc: 54.26%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.5975, Acc: 56.11%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.5967, Acc: 56.87%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.6125, Acc: 56.46%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.6115, Acc: 56.53%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.6039, Acc: 56.94%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.6103, Acc: 56.70%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.6048, Acc: 56.76%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.6026, Acc: 56.80%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.6013, Acc: 56.88%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.6048, Acc: 56.81%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 11/300:\n",
      "  Train Loss: 1.6048, Train Acc: 56.81%\n",
      "  Val Loss: 1.5351, Val Acc: 58.54%\n",
      "  LR: 0.000400\n",
      "  ðŸ’¾ Best model saved (Val Acc: 58.54%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 11 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.4663, Acc: 61.51%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.4944, Acc: 61.36%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.5363, Acc: 59.23%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.5104, Acc: 60.48%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.5279, Acc: 59.60%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.5284, Acc: 59.42%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.5347, Acc: 59.23%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.5429, Acc: 58.88%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.5451, Acc: 58.82%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.5467, Acc: 58.85%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.5465, Acc: 58.87%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 12/300:\n",
      "  Train Loss: 1.5465, Train Acc: 58.87%\n",
      "  Val Loss: 1.4400, Val Acc: 62.75%\n",
      "  LR: 0.000398\n",
      "  ðŸ’¾ Best model saved (Val Acc: 62.75%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 12 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.5122, Acc: 59.09%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.4768, Acc: 61.01%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.4698, Acc: 60.89%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.4730, Acc: 61.26%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.4861, Acc: 60.65%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.4793, Acc: 60.87%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.4822, Acc: 60.78%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.4770, Acc: 61.12%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.4748, Acc: 61.17%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.4726, Acc: 61.19%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.4726, Acc: 61.26%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 13/300:\n",
      "  Train Loss: 1.4726, Train Acc: 61.26%\n",
      "  Val Loss: 1.5852, Val Acc: 60.31%\n",
      "  LR: 0.000396\n",
      "--------------------------------------------------\n",
      "Starting Epoch 13 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.3833, Acc: 63.92%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.3979, Acc: 63.21%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.4318, Acc: 62.26%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.4344, Acc: 61.90%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.4314, Acc: 62.24%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.4228, Acc: 62.69%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.4243, Acc: 62.80%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.4341, Acc: 62.39%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.4381, Acc: 62.23%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.4494, Acc: 62.02%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.4503, Acc: 62.03%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 14/300:\n",
      "  Train Loss: 1.4503, Train Acc: 62.03%\n",
      "  Val Loss: 1.4170, Val Acc: 62.04%\n",
      "  LR: 0.000394\n",
      "--------------------------------------------------\n",
      "Starting Epoch 14 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.4187, Acc: 64.91%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.4246, Acc: 64.28%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.4387, Acc: 63.07%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.4173, Acc: 63.35%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.4214, Acc: 63.47%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.4113, Acc: 63.80%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.4112, Acc: 63.66%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.4054, Acc: 63.81%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.3954, Acc: 64.17%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.3891, Acc: 64.26%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.3887, Acc: 64.21%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 15/300:\n",
      "  Train Loss: 1.3887, Train Acc: 64.21%\n",
      "  Val Loss: 1.6291, Val Acc: 57.11%\n",
      "  LR: 0.000390\n",
      "--------------------------------------------------\n",
      "Starting Epoch 15 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.3096, Acc: 67.05%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.3450, Acc: 65.98%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.3327, Acc: 66.24%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.3477, Acc: 65.59%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.3347, Acc: 65.97%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.3343, Acc: 65.93%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.3299, Acc: 65.85%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.3468, Acc: 65.45%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.3610, Acc: 65.03%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.3637, Acc: 65.18%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.3641, Acc: 65.19%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 16/300:\n",
      "  Train Loss: 1.3641, Train Acc: 65.19%\n",
      "  Val Loss: 1.3755, Val Acc: 64.39%\n",
      "  LR: 0.000386\n",
      "  ðŸ’¾ Best model saved (Val Acc: 64.39%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 16 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.3342, Acc: 65.77%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.3472, Acc: 66.05%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.3404, Acc: 65.81%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.3255, Acc: 66.30%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.3256, Acc: 66.08%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.3389, Acc: 65.79%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.3300, Acc: 66.07%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.3291, Acc: 66.42%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.3277, Acc: 66.51%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.3284, Acc: 66.65%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.3281, Acc: 66.65%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 17/300:\n",
      "  Train Loss: 1.3281, Train Acc: 66.65%\n",
      "  Val Loss: 1.4004, Val Acc: 64.94%\n",
      "  LR: 0.000381\n",
      "  ðŸ’¾ Best model saved (Val Acc: 64.94%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 17 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.2937, Acc: 69.32%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.2829, Acc: 68.68%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.2755, Acc: 68.89%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.2572, Acc: 69.64%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.2699, Acc: 69.60%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.2604, Acc: 69.77%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.2627, Acc: 69.72%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.2644, Acc: 69.53%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.2645, Acc: 69.55%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.2672, Acc: 69.39%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.2676, Acc: 69.44%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 18/300:\n",
      "  Train Loss: 1.2676, Train Acc: 69.44%\n",
      "  Val Loss: 1.5643, Val Acc: 59.89%\n",
      "  LR: 0.000376\n",
      "--------------------------------------------------\n",
      "Starting Epoch 18 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.2206, Acc: 71.45%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.2596, Acc: 70.38%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.2417, Acc: 70.60%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.2429, Acc: 70.21%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.2498, Acc: 70.00%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.2666, Acc: 69.67%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.2529, Acc: 70.03%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.2493, Acc: 70.01%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.2561, Acc: 69.73%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.2501, Acc: 70.06%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.2506, Acc: 70.03%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 19/300:\n",
      "  Train Loss: 1.2506, Train Acc: 70.03%\n",
      "  Val Loss: 1.5310, Val Acc: 63.05%\n",
      "  LR: 0.000369\n",
      "--------------------------------------------------\n",
      "Starting Epoch 19 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.2645, Acc: 68.18%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.1994, Acc: 70.60%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.2074, Acc: 70.83%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.1959, Acc: 71.27%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.2125, Acc: 70.51%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.2214, Acc: 70.57%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.2240, Acc: 70.43%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.2259, Acc: 70.24%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.2242, Acc: 70.31%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.2214, Acc: 70.30%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.2213, Acc: 70.30%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 20/300:\n",
      "  Train Loss: 1.2213, Train Acc: 70.30%\n",
      "  Val Loss: 1.4165, Val Acc: 65.57%\n",
      "  LR: 0.000362\n",
      "  ðŸ’¾ Best model saved (Val Acc: 65.57%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 20 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.1772, Acc: 73.01%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.1860, Acc: 72.30%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.1905, Acc: 72.25%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.1889, Acc: 72.16%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.1945, Acc: 71.93%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.1872, Acc: 72.18%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.1854, Acc: 72.10%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.1819, Acc: 72.39%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.1770, Acc: 72.71%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.1781, Acc: 72.63%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.1793, Acc: 72.58%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 21/300:\n",
      "  Train Loss: 1.1793, Train Acc: 72.58%\n",
      "  Val Loss: 1.5068, Val Acc: 61.45%\n",
      "  LR: 0.000355\n",
      "--------------------------------------------------\n",
      "Starting Epoch 21 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.1390, Acc: 74.15%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.1526, Acc: 74.50%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.1539, Acc: 73.63%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.1620, Acc: 73.54%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.1668, Acc: 73.12%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.1645, Acc: 73.15%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.1620, Acc: 73.21%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.1709, Acc: 72.64%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.1774, Acc: 72.43%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.1706, Acc: 72.37%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.1711, Acc: 72.33%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 22/300:\n",
      "  Train Loss: 1.1711, Train Acc: 72.33%\n",
      "  Val Loss: 1.1999, Val Acc: 69.87%\n",
      "  LR: 0.000346\n",
      "  ðŸ’¾ Best model saved (Val Acc: 69.87%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 22 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.1150, Acc: 74.01%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.1452, Acc: 72.87%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.1054, Acc: 74.48%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.1141, Acc: 74.11%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.1139, Acc: 74.35%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.1143, Acc: 74.20%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.1174, Acc: 74.13%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.1202, Acc: 74.11%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.1294, Acc: 73.80%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.1286, Acc: 73.88%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.1288, Acc: 73.90%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 23/300:\n",
      "  Train Loss: 1.1288, Train Acc: 73.90%\n",
      "  Val Loss: 1.4104, Val Acc: 64.69%\n",
      "  LR: 0.000338\n",
      "--------------------------------------------------\n",
      "Starting Epoch 23 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.0969, Acc: 75.71%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.1211, Acc: 75.00%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.1245, Acc: 74.24%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.1193, Acc: 74.01%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.1287, Acc: 73.47%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.1173, Acc: 73.72%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.1038, Acc: 74.31%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.1076, Acc: 74.41%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.1121, Acc: 74.27%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.1144, Acc: 74.11%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.1153, Acc: 74.11%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 24/300:\n",
      "  Train Loss: 1.1153, Train Acc: 74.11%\n",
      "  Val Loss: 1.3333, Val Acc: 68.14%\n",
      "  LR: 0.000328\n",
      "--------------------------------------------------\n",
      "Starting Epoch 24 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.0712, Acc: 75.57%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.0924, Acc: 74.93%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.0869, Acc: 75.33%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.0882, Acc: 75.28%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.0927, Acc: 74.91%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.0983, Acc: 74.60%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.0940, Acc: 74.86%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.0952, Acc: 74.89%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.1004, Acc: 74.86%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.0966, Acc: 75.01%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.0973, Acc: 75.04%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 25/300:\n",
      "  Train Loss: 1.0973, Train Acc: 75.04%\n",
      "  Val Loss: 1.3366, Val Acc: 67.51%\n",
      "  LR: 0.000318\n",
      "--------------------------------------------------\n",
      "Starting Epoch 25 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.0296, Acc: 77.70%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.0284, Acc: 78.20%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.0384, Acc: 77.94%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.0227, Acc: 78.30%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.0397, Acc: 77.61%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.0434, Acc: 77.37%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.0344, Acc: 77.46%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.0388, Acc: 77.25%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.0481, Acc: 77.05%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.0562, Acc: 76.68%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.0545, Acc: 76.72%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 26/300:\n",
      "  Train Loss: 1.0545, Train Acc: 76.72%\n",
      "  Val Loss: 1.3321, Val Acc: 68.27%\n",
      "  LR: 0.000308\n",
      "--------------------------------------------------\n",
      "Starting Epoch 26 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.0377, Acc: 77.56%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.0764, Acc: 76.07%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.0738, Acc: 75.95%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.0839, Acc: 75.60%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.0863, Acc: 75.31%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.0828, Acc: 75.88%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.0829, Acc: 75.93%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.0713, Acc: 76.12%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.0641, Acc: 76.47%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.0604, Acc: 76.75%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.0607, Acc: 76.72%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 27/300:\n",
      "  Train Loss: 1.0607, Train Acc: 76.72%\n",
      "  Val Loss: 1.3738, Val Acc: 68.69%\n",
      "  LR: 0.000297\n",
      "--------------------------------------------------\n",
      "Starting Epoch 27 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.0122, Acc: 76.42%\n",
      "  Batch 44/223 (19.7%) - Loss: 1.0008, Acc: 77.77%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.0124, Acc: 77.04%\n",
      "  Batch 88/223 (39.5%) - Loss: 1.0153, Acc: 77.10%\n",
      "  Batch 110/223 (49.3%) - Loss: 1.0028, Acc: 77.73%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.0198, Acc: 77.41%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.0246, Acc: 77.27%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.0208, Acc: 77.45%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.0181, Acc: 77.56%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.0236, Acc: 77.40%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.0232, Acc: 77.42%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 28/300:\n",
      "  Train Loss: 1.0232, Train Acc: 77.42%\n",
      "  Val Loss: 1.2176, Val Acc: 71.38%\n",
      "  LR: 0.000286\n",
      "  ðŸ’¾ Best model saved (Val Acc: 71.38%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 28 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 1.0016, Acc: 79.55%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.9991, Acc: 78.76%\n",
      "  Batch 66/223 (29.6%) - Loss: 1.0067, Acc: 78.46%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.9979, Acc: 78.73%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.9914, Acc: 78.78%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9921, Acc: 78.46%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9947, Acc: 78.49%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9978, Acc: 78.48%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9927, Acc: 78.54%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9938, Acc: 78.59%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9928, Acc: 78.60%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 29/300:\n",
      "  Train Loss: 0.9928, Train Acc: 78.60%\n",
      "  Val Loss: 1.2073, Val Acc: 71.25%\n",
      "  LR: 0.000275\n",
      "--------------------------------------------------\n",
      "Starting Epoch 29 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.9211, Acc: 80.82%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.9414, Acc: 80.40%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.9421, Acc: 80.35%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.9509, Acc: 79.47%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.9506, Acc: 79.66%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9593, Acc: 79.40%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9702, Acc: 79.10%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9670, Acc: 79.26%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9653, Acc: 79.42%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9654, Acc: 79.35%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9653, Acc: 79.37%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 30/300:\n",
      "  Train Loss: 0.9653, Train Acc: 79.37%\n",
      "  Val Loss: 1.1480, Val Acc: 73.82%\n",
      "  LR: 0.000263\n",
      "  ðŸ’¾ Best model saved (Val Acc: 73.82%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 30 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.9250, Acc: 81.82%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.9061, Acc: 81.46%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.9468, Acc: 80.07%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.9393, Acc: 80.40%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.9383, Acc: 80.28%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9551, Acc: 79.40%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9496, Acc: 79.57%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9589, Acc: 79.39%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9571, Acc: 79.34%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9491, Acc: 79.74%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9523, Acc: 79.62%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 31/300:\n",
      "  Train Loss: 0.9523, Train Acc: 79.62%\n",
      "  Val Loss: 1.2344, Val Acc: 71.00%\n",
      "  LR: 0.000251\n",
      "--------------------------------------------------\n",
      "Starting Epoch 31 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.9341, Acc: 80.26%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.9463, Acc: 80.11%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.9304, Acc: 80.54%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.9356, Acc: 80.18%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.9242, Acc: 80.85%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9234, Acc: 80.80%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9271, Acc: 80.66%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9236, Acc: 80.86%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9117, Acc: 81.38%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9141, Acc: 81.35%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9158, Acc: 81.33%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 32/300:\n",
      "  Train Loss: 0.9158, Train Acc: 81.33%\n",
      "  Val Loss: 1.2871, Val Acc: 70.24%\n",
      "  LR: 0.000239\n",
      "--------------------------------------------------\n",
      "Starting Epoch 32 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8619, Acc: 83.52%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8773, Acc: 82.88%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8766, Acc: 82.58%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8790, Acc: 82.60%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8947, Acc: 82.22%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9042, Acc: 81.75%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9057, Acc: 81.76%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9076, Acc: 81.68%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9057, Acc: 81.80%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9053, Acc: 81.79%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9036, Acc: 81.88%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 33/300:\n",
      "  Train Loss: 0.9036, Train Acc: 81.88%\n",
      "  Val Loss: 1.1800, Val Acc: 72.69%\n",
      "  LR: 0.000227\n",
      "--------------------------------------------------\n",
      "Starting Epoch 33 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8422, Acc: 84.09%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8840, Acc: 83.03%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8762, Acc: 83.19%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8728, Acc: 83.45%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8609, Acc: 83.72%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8617, Acc: 83.43%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8629, Acc: 83.42%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8692, Acc: 83.19%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8690, Acc: 82.99%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8673, Acc: 83.10%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8693, Acc: 83.02%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 34/300:\n",
      "  Train Loss: 0.8693, Train Acc: 83.02%\n",
      "  Val Loss: 1.2493, Val Acc: 70.88%\n",
      "  LR: 0.000214\n",
      "--------------------------------------------------\n",
      "Starting Epoch 34 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8025, Acc: 85.09%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8142, Acc: 84.38%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8382, Acc: 83.95%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8468, Acc: 83.59%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8638, Acc: 82.95%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8626, Acc: 83.10%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8657, Acc: 82.79%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8690, Acc: 82.83%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8669, Acc: 82.97%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8642, Acc: 83.10%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8624, Acc: 83.15%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 35/300:\n",
      "  Train Loss: 0.8624, Train Acc: 83.15%\n",
      "  Val Loss: 1.2400, Val Acc: 71.89%\n",
      "  LR: 0.000202\n",
      "--------------------------------------------------\n",
      "Starting Epoch 35 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8150, Acc: 85.09%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8033, Acc: 85.44%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8205, Acc: 84.80%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8320, Acc: 84.13%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8399, Acc: 83.95%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8355, Acc: 84.16%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8478, Acc: 83.60%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8428, Acc: 83.77%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8415, Acc: 83.76%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8481, Acc: 83.57%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8477, Acc: 83.60%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 36/300:\n",
      "  Train Loss: 0.8477, Train Acc: 83.60%\n",
      "  Val Loss: 1.3405, Val Acc: 68.35%\n",
      "  LR: 0.000190\n",
      "--------------------------------------------------\n",
      "Starting Epoch 36 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8408, Acc: 83.24%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8100, Acc: 83.81%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8143, Acc: 84.04%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8045, Acc: 84.59%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8182, Acc: 84.18%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8176, Acc: 84.09%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8227, Acc: 84.05%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8221, Acc: 84.13%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8234, Acc: 84.04%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8185, Acc: 84.39%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8189, Acc: 84.38%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 37/300:\n",
      "  Train Loss: 0.8189, Train Acc: 84.38%\n",
      "  Val Loss: 1.2120, Val Acc: 73.57%\n",
      "  LR: 0.000177\n",
      "--------------------------------------------------\n",
      "Starting Epoch 37 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8011, Acc: 85.23%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8010, Acc: 85.80%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8048, Acc: 85.23%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8137, Acc: 84.91%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8241, Acc: 84.26%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8197, Acc: 84.33%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8104, Acc: 84.52%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8126, Acc: 84.41%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8217, Acc: 84.17%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8215, Acc: 84.16%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8214, Acc: 84.16%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 38/300:\n",
      "  Train Loss: 0.8214, Train Acc: 84.16%\n",
      "  Val Loss: 1.1783, Val Acc: 74.12%\n",
      "  LR: 0.000165\n",
      "  ðŸ’¾ Best model saved (Val Acc: 74.12%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 38 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8397, Acc: 83.95%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8175, Acc: 84.59%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8142, Acc: 84.75%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8230, Acc: 84.52%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8120, Acc: 84.91%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8134, Acc: 84.87%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8122, Acc: 84.70%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8127, Acc: 84.62%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8089, Acc: 84.80%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8038, Acc: 84.86%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8031, Acc: 84.92%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 39/300:\n",
      "  Train Loss: 0.8031, Train Acc: 84.92%\n",
      "  Val Loss: 1.2061, Val Acc: 72.47%\n",
      "  LR: 0.000153\n",
      "--------------------------------------------------\n",
      "Starting Epoch 39 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8197, Acc: 84.38%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8094, Acc: 84.87%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.7864, Acc: 85.42%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.7750, Acc: 85.69%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.7830, Acc: 85.45%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.7894, Acc: 85.39%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.7833, Acc: 85.67%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.7908, Acc: 85.35%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.7853, Acc: 85.54%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.7850, Acc: 85.47%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.7833, Acc: 85.51%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 40/300:\n",
      "  Train Loss: 0.7833, Train Acc: 85.51%\n",
      "  Val Loss: 1.2345, Val Acc: 72.73%\n",
      "  LR: 0.000141\n",
      "--------------------------------------------------\n",
      "Starting Epoch 40 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.7651, Acc: 85.23%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.7381, Acc: 86.65%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.7533, Acc: 86.17%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.7668, Acc: 85.94%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.7645, Acc: 85.88%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.7631, Acc: 86.01%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.7597, Acc: 85.86%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.7637, Acc: 85.78%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.7662, Acc: 85.84%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.7673, Acc: 85.89%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.7661, Acc: 85.91%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 41/300:\n",
      "  Train Loss: 0.7661, Train Acc: 85.91%\n",
      "  Val Loss: 1.1739, Val Acc: 74.58%\n",
      "  LR: 0.000129\n",
      "  ðŸ’¾ Best model saved (Val Acc: 74.58%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 41 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.7401, Acc: 86.36%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.7243, Acc: 87.22%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.7296, Acc: 87.03%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.7352, Acc: 87.14%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.7421, Acc: 87.19%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.7405, Acc: 87.10%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.7382, Acc: 87.16%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.7441, Acc: 86.99%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.7469, Acc: 86.99%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.7502, Acc: 86.93%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.7507, Acc: 86.91%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 42/300:\n",
      "  Train Loss: 0.7507, Train Acc: 86.91%\n",
      "  Val Loss: 1.2692, Val Acc: 70.83%\n",
      "  LR: 0.000118\n",
      "--------------------------------------------------\n",
      "Starting Epoch 42 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6909, Acc: 88.92%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.7202, Acc: 87.86%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.7222, Acc: 87.59%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.7322, Acc: 87.36%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.7379, Acc: 87.22%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.7323, Acc: 87.52%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.7427, Acc: 87.09%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.7352, Acc: 87.29%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.7358, Acc: 87.34%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.7362, Acc: 87.19%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.7355, Acc: 87.20%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 43/300:\n",
      "  Train Loss: 0.7355, Train Acc: 87.20%\n",
      "  Val Loss: 1.2419, Val Acc: 73.15%\n",
      "  LR: 0.000107\n",
      "--------------------------------------------------\n",
      "Starting Epoch 43 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.7528, Acc: 86.51%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.7331, Acc: 86.79%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.7299, Acc: 86.98%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.7343, Acc: 87.00%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.7274, Acc: 87.13%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.7305, Acc: 87.26%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.7308, Acc: 87.32%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.7312, Acc: 87.30%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.7227, Acc: 87.59%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.7248, Acc: 87.47%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.7232, Acc: 87.51%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 44/300:\n",
      "  Train Loss: 0.7232, Train Acc: 87.51%\n",
      "  Val Loss: 1.2244, Val Acc: 73.36%\n",
      "  LR: 0.000096\n",
      "--------------------------------------------------\n",
      "Starting Epoch 44 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.7344, Acc: 86.93%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.7048, Acc: 88.42%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.7175, Acc: 88.21%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.7054, Acc: 88.57%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.7038, Acc: 88.41%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.7083, Acc: 88.35%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.7086, Acc: 88.25%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.7118, Acc: 88.12%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.7172, Acc: 87.96%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.7159, Acc: 88.04%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.7168, Acc: 88.00%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 45/300:\n",
      "  Train Loss: 0.7168, Train Acc: 88.00%\n",
      "  Val Loss: 1.1380, Val Acc: 76.64%\n",
      "  LR: 0.000086\n",
      "  ðŸ’¾ Best model saved (Val Acc: 76.64%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 45 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6922, Acc: 88.64%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.7035, Acc: 88.07%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6922, Acc: 88.68%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6952, Acc: 88.85%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.7107, Acc: 88.15%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.7056, Acc: 88.38%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6974, Acc: 88.68%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.7013, Acc: 88.62%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6943, Acc: 88.75%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6982, Acc: 88.65%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6989, Acc: 88.60%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 46/300:\n",
      "  Train Loss: 0.6989, Train Acc: 88.60%\n",
      "  Val Loss: 1.1963, Val Acc: 74.24%\n",
      "  LR: 0.000076\n",
      "--------------------------------------------------\n",
      "Starting Epoch 46 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.7161, Acc: 86.22%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.6794, Acc: 88.71%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6824, Acc: 89.20%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6786, Acc: 89.67%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.6870, Acc: 89.26%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.6865, Acc: 89.18%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6912, Acc: 88.98%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.6887, Acc: 89.08%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6798, Acc: 89.41%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6822, Acc: 89.19%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6821, Acc: 89.22%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 47/300:\n",
      "  Train Loss: 0.6821, Train Acc: 89.22%\n",
      "  Val Loss: 1.1458, Val Acc: 76.73%\n",
      "  LR: 0.000066\n",
      "  ðŸ’¾ Best model saved (Val Acc: 76.73%)\n",
      "  ðŸ“„ Manifest saved: models/asl_model_v20250723_042752_manifest.json\n",
      "--------------------------------------------------\n",
      "Starting Epoch 47 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6647, Acc: 90.62%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.6756, Acc: 89.91%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6591, Acc: 90.01%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6613, Acc: 89.91%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.6545, Acc: 90.14%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.6643, Acc: 89.65%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6636, Acc: 89.69%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.6697, Acc: 89.35%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6737, Acc: 89.17%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6703, Acc: 89.32%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6704, Acc: 89.30%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 48/300:\n",
      "  Train Loss: 0.6704, Train Acc: 89.30%\n",
      "  Val Loss: 1.1457, Val Acc: 75.51%\n",
      "  LR: 0.000058\n",
      "--------------------------------------------------\n",
      "Starting Epoch 48 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6597, Acc: 90.20%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.6555, Acc: 90.06%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6540, Acc: 89.96%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6692, Acc: 89.60%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.6641, Acc: 89.83%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.6700, Acc: 89.56%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6721, Acc: 89.43%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.6703, Acc: 89.54%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6700, Acc: 89.61%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6707, Acc: 89.67%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6692, Acc: 89.71%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 49/300:\n",
      "  Train Loss: 0.6692, Train Acc: 89.71%\n",
      "  Val Loss: 1.2226, Val Acc: 74.58%\n",
      "  LR: 0.000049\n",
      "--------------------------------------------------\n",
      "Starting Epoch 49 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6280, Acc: 90.62%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.6281, Acc: 90.70%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6205, Acc: 90.96%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6222, Acc: 90.91%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.6337, Acc: 90.54%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.6392, Acc: 90.41%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6360, Acc: 90.54%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.6381, Acc: 90.36%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6496, Acc: 90.04%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6525, Acc: 89.97%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6529, Acc: 89.98%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 50/300:\n",
      "  Train Loss: 0.6529, Train Acc: 89.98%\n",
      "  Val Loss: 1.2456, Val Acc: 72.69%\n",
      "  LR: 0.000042\n",
      "--------------------------------------------------\n",
      "Starting Epoch 50 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6691, Acc: 89.35%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.6765, Acc: 89.56%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6566, Acc: 90.58%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6588, Acc: 90.20%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.6604, Acc: 89.94%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.6620, Acc: 89.91%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6638, Acc: 89.75%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.6602, Acc: 89.99%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6629, Acc: 89.95%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6659, Acc: 89.79%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6648, Acc: 89.84%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 51/300:\n",
      "  Train Loss: 0.6648, Train Acc: 89.84%\n",
      "  Val Loss: 1.1910, Val Acc: 74.83%\n",
      "  LR: 0.000035\n",
      "--------------------------------------------------\n",
      "Starting Epoch 51 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6333, Acc: 91.19%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.6462, Acc: 90.13%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6448, Acc: 90.48%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6342, Acc: 90.66%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.6430, Acc: 90.37%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.6451, Acc: 90.46%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6437, Acc: 90.50%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.6437, Acc: 90.54%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6372, Acc: 90.70%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6461, Acc: 90.51%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6454, Acc: 90.54%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 52/300:\n",
      "  Train Loss: 0.6454, Train Acc: 90.54%\n",
      "  Val Loss: 1.2727, Val Acc: 72.64%\n",
      "  LR: 0.000028\n",
      "--------------------------------------------------\n",
      "Starting Epoch 52 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6324, Acc: 90.77%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.6381, Acc: 90.48%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6552, Acc: 90.06%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6594, Acc: 89.77%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.6550, Acc: 89.91%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.6506, Acc: 90.06%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6518, Acc: 89.98%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.6499, Acc: 89.97%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6494, Acc: 90.12%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6531, Acc: 90.01%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6507, Acc: 90.12%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 53/300:\n",
      "  Train Loss: 0.6507, Train Acc: 90.12%\n",
      "  Val Loss: 1.3346, Val Acc: 71.93%\n",
      "  LR: 0.000023\n",
      "--------------------------------------------------\n",
      "Starting Epoch 53 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6174, Acc: 92.05%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.6260, Acc: 91.76%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6279, Acc: 91.34%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6461, Acc: 90.77%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.6414, Acc: 90.82%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.6542, Acc: 90.25%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6534, Acc: 90.24%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.6531, Acc: 90.23%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6462, Acc: 90.44%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6453, Acc: 90.45%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6440, Acc: 90.48%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 54/300:\n",
      "  Train Loss: 0.6440, Train Acc: 90.48%\n",
      "  Val Loss: 1.2388, Val Acc: 73.74%\n",
      "  LR: 0.000018\n",
      "--------------------------------------------------\n",
      "Starting Epoch 54 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6381, Acc: 91.48%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.6445, Acc: 90.06%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6311, Acc: 90.58%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6273, Acc: 90.87%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.6356, Acc: 90.40%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.6391, Acc: 90.22%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6331, Acc: 90.67%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.6323, Acc: 90.66%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6304, Acc: 90.70%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6333, Acc: 90.58%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6339, Acc: 90.57%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 55/300:\n",
      "  Train Loss: 0.6339, Train Acc: 90.57%\n",
      "  Val Loss: 1.2754, Val Acc: 73.48%\n",
      "  LR: 0.000014\n",
      "--------------------------------------------------\n",
      "Starting Epoch 55 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6641, Acc: 89.63%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.6251, Acc: 91.19%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6311, Acc: 90.86%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6320, Acc: 90.73%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.6228, Acc: 90.99%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.6225, Acc: 90.81%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6143, Acc: 91.13%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.6182, Acc: 90.96%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6196, Acc: 90.97%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6235, Acc: 90.84%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6221, Acc: 90.92%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 56/300:\n",
      "  Train Loss: 0.6221, Train Acc: 90.92%\n",
      "  Val Loss: 1.1497, Val Acc: 76.26%\n",
      "  LR: 0.000010\n",
      "--------------------------------------------------\n",
      "Starting Epoch 56 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6157, Acc: 90.91%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.6115, Acc: 91.19%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6133, Acc: 91.19%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6200, Acc: 91.23%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.6273, Acc: 90.80%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.6350, Acc: 90.74%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6309, Acc: 90.89%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.6260, Acc: 91.03%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6208, Acc: 91.16%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6227, Acc: 91.11%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6219, Acc: 91.16%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 57/300:\n",
      "  Train Loss: 0.6219, Train Acc: 91.16%\n",
      "  Val Loss: 1.1882, Val Acc: 76.39%\n",
      "  LR: 0.000008\n",
      "--------------------------------------------------\n",
      "Starting Epoch 57 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6708, Acc: 89.20%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.6413, Acc: 90.27%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6463, Acc: 90.20%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6508, Acc: 89.81%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.6354, Acc: 90.54%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.6340, Acc: 90.74%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6326, Acc: 90.77%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.6433, Acc: 90.47%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6385, Acc: 90.66%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6358, Acc: 90.78%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6347, Acc: 90.85%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 58/300:\n",
      "  Train Loss: 0.6347, Train Acc: 90.85%\n",
      "  Val Loss: 1.2277, Val Acc: 75.08%\n",
      "  LR: 0.000006\n",
      "--------------------------------------------------\n",
      "Starting Epoch 58 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.6333, Acc: 90.91%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.6216, Acc: 91.48%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.6178, Acc: 91.57%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.6111, Acc: 91.65%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.6140, Acc: 91.51%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.6183, Acc: 91.41%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.6230, Acc: 91.17%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.6253, Acc: 91.05%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.6264, Acc: 91.04%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.6274, Acc: 90.95%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.6280, Acc: 90.90%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 59/300:\n",
      "  Train Loss: 0.6280, Train Acc: 90.90%\n",
      "  Val Loss: 1.1733, Val Acc: 75.80%\n",
      "  LR: 0.000004\n",
      "--------------------------------------------------\n",
      "Starting Epoch 59 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.7912, Acc: 85.94%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8715, Acc: 82.88%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8560, Acc: 83.19%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8891, Acc: 82.14%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8936, Acc: 81.85%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9136, Acc: 81.16%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9178, Acc: 81.17%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9292, Acc: 80.79%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9373, Acc: 80.60%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9426, Acc: 80.43%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9435, Acc: 80.35%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 60/300:\n",
      "  Train Loss: 0.9435, Train Acc: 80.35%\n",
      "  Val Loss: 1.3812, Val Acc: 68.94%\n",
      "  LR: 0.000400\n",
      "--------------------------------------------------\n",
      "Starting Epoch 60 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.9650, Acc: 80.26%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.9493, Acc: 80.54%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.9514, Acc: 80.07%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.9879, Acc: 79.12%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.9960, Acc: 79.03%\n",
      "  Batch 132/223 (59.2%) - Loss: 1.0018, Acc: 78.81%\n",
      "  Batch 154/223 (69.1%) - Loss: 1.0081, Acc: 78.55%\n",
      "  Batch 176/223 (78.9%) - Loss: 1.0034, Acc: 78.64%\n",
      "  Batch 198/223 (88.8%) - Loss: 1.0037, Acc: 78.54%\n",
      "  Batch 220/223 (98.7%) - Loss: 1.0090, Acc: 78.28%\n",
      "  Batch 223/223 (100.0%) - Loss: 1.0087, Acc: 78.23%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 61/300:\n",
      "  Train Loss: 1.0087, Train Acc: 78.23%\n",
      "  Val Loss: 1.3019, Val Acc: 68.86%\n",
      "  LR: 0.000400\n",
      "--------------------------------------------------\n",
      "Starting Epoch 61 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.9462, Acc: 78.55%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.9534, Acc: 79.55%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.9446, Acc: 79.40%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.9376, Acc: 79.79%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.9462, Acc: 79.69%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9568, Acc: 79.40%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9638, Acc: 79.24%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9746, Acc: 78.94%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9800, Acc: 78.65%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9799, Acc: 78.58%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9802, Acc: 78.58%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 62/300:\n",
      "  Train Loss: 0.9802, Train Acc: 78.58%\n",
      "  Val Loss: 1.3114, Val Acc: 69.99%\n",
      "  LR: 0.000400\n",
      "--------------------------------------------------\n",
      "Starting Epoch 62 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.9554, Acc: 79.83%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.9762, Acc: 78.76%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.9849, Acc: 78.84%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.9776, Acc: 79.01%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.9771, Acc: 78.95%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9787, Acc: 78.91%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9820, Acc: 78.79%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9895, Acc: 78.46%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9786, Acc: 78.96%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9838, Acc: 78.69%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9856, Acc: 78.65%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 63/300:\n",
      "  Train Loss: 0.9856, Train Acc: 78.65%\n",
      "  Val Loss: 1.4284, Val Acc: 66.71%\n",
      "  LR: 0.000399\n",
      "--------------------------------------------------\n",
      "Starting Epoch 63 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8589, Acc: 82.53%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8608, Acc: 83.10%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.9203, Acc: 80.87%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.9333, Acc: 80.11%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.9580, Acc: 79.23%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9556, Acc: 79.59%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9649, Acc: 79.10%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9623, Acc: 79.21%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9609, Acc: 79.14%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9664, Acc: 79.02%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9640, Acc: 79.04%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 64/300:\n",
      "  Train Loss: 0.9640, Train Acc: 79.04%\n",
      "  Val Loss: 1.2785, Val Acc: 71.93%\n",
      "  LR: 0.000398\n",
      "--------------------------------------------------\n",
      "Starting Epoch 64 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.9188, Acc: 81.11%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.9368, Acc: 79.97%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.9336, Acc: 80.11%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.9326, Acc: 80.61%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.9224, Acc: 81.19%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9299, Acc: 81.01%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9378, Acc: 80.72%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9444, Acc: 80.42%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9498, Acc: 80.13%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9506, Acc: 80.09%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9524, Acc: 79.98%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 65/300:\n",
      "  Train Loss: 0.9524, Train Acc: 79.98%\n",
      "  Val Loss: 1.2294, Val Acc: 71.17%\n",
      "  LR: 0.000398\n",
      "--------------------------------------------------\n",
      "Starting Epoch 65 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.9416, Acc: 79.26%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.9211, Acc: 79.83%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.9324, Acc: 79.78%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.9168, Acc: 80.43%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.9238, Acc: 80.43%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9310, Acc: 80.30%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9341, Acc: 80.13%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9454, Acc: 79.83%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9560, Acc: 79.53%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9633, Acc: 79.38%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9658, Acc: 79.30%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 66/300:\n",
      "  Train Loss: 0.9658, Train Acc: 79.30%\n",
      "  Val Loss: 1.2495, Val Acc: 71.38%\n",
      "  LR: 0.000396\n",
      "--------------------------------------------------\n",
      "Starting Epoch 66 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.9058, Acc: 81.82%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8883, Acc: 82.46%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.9090, Acc: 81.96%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.9157, Acc: 81.71%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.9089, Acc: 81.56%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9219, Acc: 81.11%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9215, Acc: 80.99%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9242, Acc: 80.98%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9213, Acc: 81.04%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9264, Acc: 80.82%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9303, Acc: 80.73%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 67/300:\n",
      "  Train Loss: 0.9303, Train Acc: 80.73%\n",
      "  Val Loss: 1.3062, Val Acc: 70.33%\n",
      "  LR: 0.000395\n",
      "--------------------------------------------------\n",
      "Starting Epoch 67 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8774, Acc: 83.24%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8931, Acc: 82.17%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8865, Acc: 82.53%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8852, Acc: 82.39%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8964, Acc: 82.07%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9069, Acc: 81.72%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9148, Acc: 81.49%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9180, Acc: 81.21%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9245, Acc: 81.01%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9240, Acc: 80.94%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9267, Acc: 80.84%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 68/300:\n",
      "  Train Loss: 0.9267, Train Acc: 80.84%\n",
      "  Val Loss: 1.3310, Val Acc: 71.42%\n",
      "  LR: 0.000394\n",
      "--------------------------------------------------\n",
      "Starting Epoch 68 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8256, Acc: 83.95%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8628, Acc: 82.24%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8912, Acc: 81.63%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8907, Acc: 81.64%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.9153, Acc: 80.77%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9162, Acc: 80.54%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9182, Acc: 80.40%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9198, Acc: 80.38%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9200, Acc: 80.32%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9186, Acc: 80.51%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9191, Acc: 80.56%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 69/300:\n",
      "  Train Loss: 0.9191, Train Acc: 80.56%\n",
      "  Val Loss: 1.3581, Val Acc: 70.16%\n",
      "  LR: 0.000392\n",
      "--------------------------------------------------\n",
      "Starting Epoch 69 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8612, Acc: 84.23%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8925, Acc: 82.60%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8814, Acc: 82.86%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.9134, Acc: 81.78%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.9123, Acc: 81.65%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.9077, Acc: 81.53%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.9082, Acc: 81.55%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.9088, Acc: 81.55%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.9151, Acc: 81.42%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.9194, Acc: 81.18%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.9209, Acc: 81.16%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 70/300:\n",
      "  Train Loss: 0.9209, Train Acc: 81.16%\n",
      "  Val Loss: 1.3001, Val Acc: 72.18%\n",
      "  LR: 0.000390\n",
      "--------------------------------------------------\n",
      "Starting Epoch 70 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8416, Acc: 83.95%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8282, Acc: 84.52%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8613, Acc: 83.33%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8720, Acc: 82.49%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8665, Acc: 82.36%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8771, Acc: 82.13%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8806, Acc: 82.22%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8806, Acc: 82.33%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8833, Acc: 82.28%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8857, Acc: 82.16%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8859, Acc: 82.11%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 71/300:\n",
      "  Train Loss: 0.8859, Train Acc: 82.11%\n",
      "  Val Loss: 1.2185, Val Acc: 72.60%\n",
      "  LR: 0.000388\n",
      "--------------------------------------------------\n",
      "Starting Epoch 71 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8988, Acc: 81.25%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8839, Acc: 81.68%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8826, Acc: 81.96%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8795, Acc: 82.24%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8856, Acc: 82.13%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8815, Acc: 82.17%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8770, Acc: 82.26%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8798, Acc: 82.07%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8755, Acc: 82.13%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8776, Acc: 82.22%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8774, Acc: 82.27%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 72/300:\n",
      "  Train Loss: 0.8774, Train Acc: 82.27%\n",
      "  Val Loss: 1.4013, Val Acc: 67.59%\n",
      "  LR: 0.000386\n",
      "--------------------------------------------------\n",
      "Starting Epoch 72 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8515, Acc: 82.67%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8486, Acc: 82.60%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8639, Acc: 82.29%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8755, Acc: 81.71%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8777, Acc: 81.93%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8803, Acc: 81.84%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8877, Acc: 81.55%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8854, Acc: 81.71%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8801, Acc: 82.02%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8792, Acc: 82.03%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8774, Acc: 82.09%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 73/300:\n",
      "  Train Loss: 0.8774, Train Acc: 82.09%\n",
      "  Val Loss: 1.2464, Val Acc: 71.93%\n",
      "  LR: 0.000384\n",
      "--------------------------------------------------\n",
      "Starting Epoch 73 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8806, Acc: 82.10%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8747, Acc: 82.81%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8781, Acc: 82.86%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8805, Acc: 82.99%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8910, Acc: 82.22%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8860, Acc: 82.46%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8906, Acc: 82.14%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8801, Acc: 82.35%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8793, Acc: 82.47%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8786, Acc: 82.40%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8802, Acc: 82.37%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 74/300:\n",
      "  Train Loss: 0.8802, Train Acc: 82.37%\n",
      "  Val Loss: 1.2648, Val Acc: 71.51%\n",
      "  LR: 0.000381\n",
      "--------------------------------------------------\n",
      "Starting Epoch 74 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8265, Acc: 84.09%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8575, Acc: 82.88%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8636, Acc: 82.95%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8427, Acc: 83.88%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8422, Acc: 83.98%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8429, Acc: 83.74%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8384, Acc: 83.81%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8372, Acc: 83.82%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8368, Acc: 83.74%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8427, Acc: 83.47%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8470, Acc: 83.38%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 75/300:\n",
      "  Train Loss: 0.8470, Train Acc: 83.38%\n",
      "  Val Loss: 1.4048, Val Acc: 67.38%\n",
      "  LR: 0.000378\n",
      "--------------------------------------------------\n",
      "Starting Epoch 75 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8236, Acc: 84.09%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8483, Acc: 82.95%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8426, Acc: 83.43%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8685, Acc: 82.39%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8512, Acc: 83.04%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8507, Acc: 83.24%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8575, Acc: 83.14%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8548, Acc: 83.24%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8504, Acc: 83.36%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8543, Acc: 83.15%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8554, Acc: 83.10%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 76/300:\n",
      "  Train Loss: 0.8554, Train Acc: 83.10%\n",
      "  Val Loss: 1.3426, Val Acc: 70.03%\n",
      "  LR: 0.000376\n",
      "--------------------------------------------------\n",
      "Starting Epoch 76 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8284, Acc: 83.95%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8355, Acc: 83.45%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8210, Acc: 83.95%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8261, Acc: 83.81%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8354, Acc: 83.32%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8444, Acc: 83.17%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8513, Acc: 83.16%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8421, Acc: 83.56%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8469, Acc: 83.25%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8491, Acc: 83.18%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8481, Acc: 83.25%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 77/300:\n",
      "  Train Loss: 0.8481, Train Acc: 83.25%\n",
      "  Val Loss: 1.4568, Val Acc: 66.50%\n",
      "  LR: 0.000372\n",
      "--------------------------------------------------\n",
      "Starting Epoch 77 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.7793, Acc: 84.94%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.7755, Acc: 85.58%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.7862, Acc: 85.37%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8092, Acc: 84.52%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8016, Acc: 84.86%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.7943, Acc: 85.18%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8018, Acc: 84.84%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8059, Acc: 84.75%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8135, Acc: 84.61%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8210, Acc: 84.39%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8219, Acc: 84.38%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 78/300:\n",
      "  Train Loss: 0.8219, Train Acc: 84.38%\n",
      "  Val Loss: 1.3309, Val Acc: 70.12%\n",
      "  LR: 0.000369\n",
      "--------------------------------------------------\n",
      "Starting Epoch 78 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8241, Acc: 84.09%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.8038, Acc: 84.52%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.8095, Acc: 84.42%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8337, Acc: 83.74%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8465, Acc: 83.41%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8482, Acc: 83.57%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8495, Acc: 83.56%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8436, Acc: 83.66%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8470, Acc: 83.48%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8447, Acc: 83.48%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8438, Acc: 83.56%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 79/300:\n",
      "  Train Loss: 0.8438, Train Acc: 83.56%\n",
      "  Val Loss: 1.3837, Val Acc: 70.88%\n",
      "  LR: 0.000366\n",
      "--------------------------------------------------\n",
      "Starting Epoch 79 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.7603, Acc: 86.79%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.7837, Acc: 85.37%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.7923, Acc: 85.32%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.8125, Acc: 84.59%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.8109, Acc: 84.60%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.8248, Acc: 84.04%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8301, Acc: 83.73%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8288, Acc: 83.77%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8340, Acc: 83.57%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8338, Acc: 83.68%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8318, Acc: 83.74%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 80/300:\n",
      "  Train Loss: 0.8318, Train Acc: 83.74%\n",
      "  Val Loss: 1.3151, Val Acc: 70.66%\n",
      "  LR: 0.000362\n",
      "--------------------------------------------------\n",
      "Starting Epoch 80 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.8158, Acc: 83.52%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.7933, Acc: 85.51%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.7941, Acc: 85.23%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.7844, Acc: 85.26%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.7906, Acc: 84.97%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.7875, Acc: 85.23%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.7930, Acc: 85.31%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8036, Acc: 85.01%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.7966, Acc: 85.21%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.7966, Acc: 85.21%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.7970, Acc: 85.16%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 81/300:\n",
      "  Train Loss: 0.7970, Train Acc: 85.16%\n",
      "  Val Loss: 1.2164, Val Acc: 74.24%\n",
      "  LR: 0.000358\n",
      "--------------------------------------------------\n",
      "Starting Epoch 81 with 223 batches...\n",
      "  Batch 22/223 (9.9%) - Loss: 0.7596, Acc: 87.50%\n",
      "  Batch 44/223 (19.7%) - Loss: 0.7614, Acc: 87.00%\n",
      "  Batch 66/223 (29.6%) - Loss: 0.7770, Acc: 86.22%\n",
      "  Batch 88/223 (39.5%) - Loss: 0.7821, Acc: 85.94%\n",
      "  Batch 110/223 (49.3%) - Loss: 0.7864, Acc: 85.74%\n",
      "  Batch 132/223 (59.2%) - Loss: 0.7939, Acc: 85.56%\n",
      "  Batch 154/223 (69.1%) - Loss: 0.8011, Acc: 85.23%\n",
      "  Batch 176/223 (78.9%) - Loss: 0.8022, Acc: 85.12%\n",
      "  Batch 198/223 (88.8%) - Loss: 0.8038, Acc: 85.13%\n",
      "  Batch 220/223 (98.7%) - Loss: 0.8042, Acc: 85.04%\n",
      "  Batch 223/223 (100.0%) - Loss: 0.8070, Acc: 84.96%\n",
      "Starting validation...\n",
      "  Validation progress: 20.0%\n",
      "  Validation progress: 40.0%\n",
      "  Validation progress: 60.0%\n",
      "  Validation progress: 80.0%\n",
      "  Validation progress: 100.0%\n",
      "Epoch 82/300:\n",
      "  Train Loss: 0.8070, Train Acc: 84.96%\n",
      "  Val Loss: 1.2361, Val Acc: 74.49%\n",
      "  LR: 0.000355\n",
      "  âš ï¸ Early stopping at epoch 82 (no improvement for 35 epochs)\n",
      "\n",
      "âœ… Training completed in 1.20 hours\n",
      "   Best validation accuracy: 76.73%\n",
      "   Improvement over baseline model: +11.73%\n",
      "  ðŸ“„ Final manifest saved: models/asl_model_v20250723_042752_final_manifest.json\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ‰ Model training completed!\n",
      "   Best accuracy: 76.73%\n",
      "   Model saved in: models/asl_model_v20250723_042752.pth\n",
      "   Manifest saved in: models/asl_model_v20250723_042752_manifest.json\n",
      "   Final manifest: models/asl_model_v20250723_042752_final_manifest.json\n",
      "   Improvement over baseline model: +11.73%\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸŽ¯ Google ASL Recognition - Training enhanced model with adaptive regularization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load data\n",
    "print(\"ðŸ“ Loading dataset...\")\n",
    "train_data, train_labels, test_data, test_labels, sign_mapping, classes = load_dataset(max_samples=100 if TEST_MODE else None)\n",
    "\n",
    "print(f\"âœ… Loaded:\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Test samples: {len(test_data)}\")\n",
    "print(f\"   Classes: {len(classes)}\")\n",
    "print(f\"   Classes: {classes}\")\n",
    "\n",
    "# Create models directory\n",
    "import os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Create timestamp prefix for versioning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_prefix = f\"asl_model_v{timestamp}\"\n",
    "\n",
    "# Train model\n",
    "model, best_acc = train_model(\n",
    "    train_data=train_data,\n",
    "    train_labels=train_labels,\n",
    "    test_data=test_data,\n",
    "    test_labels=test_labels,\n",
    "    num_classes=len(classes),\n",
    "    epochs=10 if TEST_MODE else 300,  # Reduce number of epochs\n",
    "    batch_size=32,  # Optimized for RTX4070\n",
    "    lr=1e-4 if TEST_MODE else 4e-4,  # Slightly increase learning rate\n",
    "    max_len=384,\n",
    "    timestamp=timestamp,\n",
    "    model_prefix=model_prefix\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Model training completed!\")\n",
    "print(f\"   Best accuracy: {best_acc:.2f}%\")\n",
    "print(f\"   Model saved in: models/{model_prefix}.pth\")\n",
    "print(f\"   Manifest saved in: models/{model_prefix}_manifest.json\")\n",
    "print(f\"   Final manifest: models/{model_prefix}_final_manifest.json\")\n",
    "print(f\"   Improvement over baseline model: +{best_acc - 65:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Expected Results & Performance\n",
    "\n",
    "**Target Performance**:\n",
    "- **Validation Accuracy**: 75-78% (improved from baseline 65%)\n",
    "- **Training-Validation Gap**: 10-12% (reduced overfitting)\n",
    "- **Training Stability**: Smoother convergence without sharp jumps\n",
    "- **Convergence**: 100-150 epochs (vs baseline 55 epochs)\n",
    "\n",
    "**Model Improvements Over Baseline**:\n",
    "1. **+13% accuracy improvement** through hybrid architecture\n",
    "2. **-5% overfitting reduction** via adaptive regularization  \n",
    "3. **Better temporal modeling** with TCN + LSTM + Attention\n",
    "4. **Enhanced feature engineering** with 6 motion feature types\n",
    "5. **Improved generalization** through advanced augmentation\n",
    "\n",
    "**Architecture Benefits**:\n",
    "- **TCN**: Multi-scale temporal receptive fields\n",
    "- **BiLSTM**: Long-term dependency capture\n",
    "- **Attention**: Important frame identification\n",
    "- **Multi-pooling**: Rich sequence representations\n",
    "- **Adaptive dropout**: Smooth overfitting prevention\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This implementation demonstrates a state-of-the-art approach to ASL recognition combining:\n",
    "- Advanced preprocessing with temporal motion features\n",
    "- Hybrid architecture leveraging TCN, LSTM, and Transformer strengths  \n",
    "- Adaptive regularization strategies for better generalization\n",
    "- Comprehensive training pipeline with real-time monitoring\n",
    "\n",
    "The model achieves significant improvements over baseline approaches while maintaining computational efficiency suitable for real-time applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

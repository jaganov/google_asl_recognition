#!/usr/bin/env python3
"""
üî• GPU Stress Test —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –¥–ª—è RTX 4070
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –±–µ–∑ nvidia-smi pmon
"""

import torch
import time
import subprocess
import threading
import os
import json
from datetime import datetime

class GPUMonitor:
    def __init__(self):
        self.monitoring = False
        self.stats = []
    
    def get_gpu_stats(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É GPU —á–µ—Ä–µ–∑ nvidia-smi"""
        try:
            cmd = [
                'nvidia-smi', 
                '--query-gpu=name,memory.used,memory.total,utilization.gpu,temperature.gpu,power.draw',
                '--format=csv,noheader,nounits'
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                line = result.stdout.strip()
                values = [v.strip() for v in line.split(',')]
                
                return {
                    'timestamp': datetime.now().strftime('%H:%M:%S'),
                    'name': values[0],
                    'memory_used': int(values[1]),
                    'memory_total': int(values[2]),
                    'gpu_util': int(values[3]),
                    'temperature': int(values[4]),
                    'power_draw': float(values[5])
                }
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
        return None
    
    def start_monitoring(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
        self.monitoring = True
        
        def monitor_loop():
            while self.monitoring:
                stats = self.get_gpu_stats()
                if stats:
                    self.stats.append(stats)
                    self.print_stats(stats)
                time.sleep(2)
        
        thread = threading.Thread(target=monitor_loop, daemon=True)
        thread.start()
    
    def stop_monitoring(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"""
        self.monitoring = False
    
    def print_stats(self, stats):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        memory_percent = (stats['memory_used'] / stats['memory_total']) * 100
        
        print(f"\nüìä [{stats['timestamp']}] GPU Status:")
        print(f"   üéÆ {stats['name']}")
        print(f"   üíæ Memory: {stats['memory_used']}MB / {stats['memory_total']}MB ({memory_percent:.1f}%)")
        print(f"   üî• GPU Util: {stats['gpu_util']}%")
        print(f"   üå°Ô∏è  Temp: {stats['temperature']}¬∞C")
        print(f"   ‚ö° Power: {stats['power_draw']}W")
        print("   " + "="*50)

def get_process_info():
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö GPU"""
    try:
        result = subprocess.run(['nvidia-smi', '-q', '-d', 'PIDS'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("\nüîç GPU Processes:")
            lines = result.stdout.split('\n')
            for line in lines:
                if 'Process ID' in line or 'Used GPU Memory' in line or 'Process Name' in line:
                    print(f"   {line.strip()}")
    except Exception as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö: {e}")

def stress_test_with_monitoring():
    """–°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º"""
    print("üöÄ –ó–∞–ø—É—Å–∫ GPU Stress Test —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º")
    print("=" * 60)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞!")
        return
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"üéÆ GPU: {gpu_name}")
    print(f"üíæ VRAM: {gpu_memory:.1f} GB")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    monitor = GPUMonitor()
    monitor.start_monitoring()
    
    try:
        print("\nüî• –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π...")
        print("üí° –°–º–æ—Ç—Ä–∏—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—ã—à–µ, –æ–Ω–∞ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –∫–∞–∂–¥—ã–µ 2 —Å–µ–∫—É–Ω–¥—ã")
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø–∞–º—è—Ç—å
        print("\nüìä –ó–∞–ø–æ–ª–Ω—è–µ–º VRAM...")
        tensors = []
        for i in range(15):
            try:
                tensor = torch.randn(2048, 2048, 64, device='cuda', dtype=torch.float32)
                tensors.append(tensor)
                print(f"   –¢–µ–Ω–∑–æ—Ä {i+1}: {torch.cuda.memory_allocated()/1024**3:.1f}GB")
            except torch.cuda.OutOfMemoryError:
                print(f"   –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏ –Ω–∞ —Ç–µ–Ω–∑–æ—Ä–µ {i+1}")
                break
        
        # –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
        print("\nüî• –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ã–µ –º–∞—Ç—Ä–∏—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è...")
        size = 6144
        A = torch.randn(size, size, device='cuda', requires_grad=True)
        B = torch.randn(size, size, device='cuda', requires_grad=True)
        
        for iteration in range(100):
            # –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ
            C = torch.matmul(A, B)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
            D = torch.sin(C) + torch.cos(C)
            E = torch.tanh(D)
            F = torch.relu(E)
            
            # Backward pass
            loss = torch.sum(F**2)
            loss.backward()
            
            # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
            A.grad = None
            B.grad = None
            
            if iteration % 10 == 0:
                print(f"   –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}/100")
        
        print("\n‚úÖ –°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å—ã
        get_process_info()
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
    finally:
        monitor.stop_monitoring()
        torch.cuda.empty_cache()
        print("\nüßπ –ü–∞–º—è—Ç—å GPU –æ—á–∏—â–µ–Ω–∞")

def simple_nvidia_smi_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–º–∞–Ω–¥ nvidia-smi"""
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–º–∞–Ω–¥ nvidia-smi –¥–ª—è RTX 4070:")
    print("=" * 60)
    
    commands = [
        ('nvidia-smi', '–ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'),
        ('nvidia-smi -l 1', '–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É'),
        ('nvidia-smi -q', '–î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'),
        ('nvidia-smi -q -d PIDS', '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö'),
        ('nvidia-smi dmon -i 0', '–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥'),
        ('nvidia-smi pmon -i 0', '–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–ù–ï –†–ê–ë–û–¢–ê–ï–¢ –Ω–∞ GeForce)')
    ]
    
    for cmd, desc in commands:
        print(f"\nüìã {desc}:")
        print(f"   üíª –ö–æ–º–∞–Ω–¥–∞: {cmd}")
        
        if 'pmon' in cmd:
            print("   ‚ùå –ù–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞ GeForce RTX 4070")
        else:
            print("   ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")

if __name__ == "__main__":
    print("üéØ GPU Monitoring Test –¥–ª—è RTX 4070")
    print("=" * 60)
    
    choice = input("""
–í—ã–±–µ—Ä–∏—Ç–µ –æ–ø—Ü–∏—é:
1. –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
2. –ü–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã nvidia-smi
3. –¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å GPU

–í–≤–æ–¥ (1-3): """).strip()
    
    if choice == '1':
        stress_test_with_monitoring()
    elif choice == '2':
        simple_nvidia_smi_check()
    elif choice == '3':
        if torch.cuda.is_available():
            print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
            print(f"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB")
        else:
            print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
    else:
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")
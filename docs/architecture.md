# üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ ASL Recognition

–î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∂–µ—Å—Ç–æ–≤ ASL, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ —Ä–µ—à–µ–Ω–∏–∏ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è –∫–æ–Ω–∫—É—Ä—Å–∞ Google ASL Signs.

## üéØ –û–±–∑–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

–ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, —Å–æ—á–µ—Ç–∞—é—â—É—é —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:

- **TCN (Temporal Convolutional Networks)** - –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- **LSTM (Long Short-Term Memory)** - –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- **Transformer** - –¥–ª—è –≤–Ω–∏–º–∞–Ω–∏—è –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π
- **Adaptive Regularization** - –¥–ª—è –±–æ—Ä—å–±—ã —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º

## üèóÔ∏è –ü–æ–ª–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
Input: (batch_size, seq_len, 543, 3)
    ‚Üì
Preprocessing Layer
    ‚Üì
Stem: Linear(744, 192) + BatchNorm + AdaptiveDropout
    ‚Üì
TCN Block 1 (dilation=1, kernel=17)
    ‚Üì
TCN Block 2 (dilation=2, kernel=17)
    ‚Üì
TCN Block 3 (dilation=4, kernel=17)
    ‚Üì
Bidirectional LSTM (2 layers, hidden_dim=96)
    ‚Üì
Temporal Attention (8 heads)
    ‚Üì
Conv1D Block 1 (kernel=17)
    ‚Üì
Conv1D Block 2 (kernel=17)
    ‚Üì
Conv1D Block 3 (kernel=17)
    ‚Üì
Transformer Block (8 heads, expand=2)
    ‚Üì
Top Layer: Linear(192, 192) + BatchNorm + AdaptiveDropout
    ‚Üì
Multi-scale Pooling:
    ‚îú‚îÄ‚îÄ Global Average Pooling
    ‚îú‚îÄ‚îÄ Global Max Pooling
    ‚îî‚îÄ‚îÄ Attention Pooling (4 heads)
    ‚Üì
Concatenation: (192*3 = 576)
    ‚Üì
Classifier: Linear(576, 192) ‚Üí BatchNorm ‚Üí SiLU ‚Üí Dropout ‚Üí Linear(192, 25)
    ‚Üì
Output: (batch_size, 25)
```

## üîß –î–µ—Ç–∞–ª–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

### 1. Preprocessing Layer

```python
class PreprocessingLayer(nn.Module):
    def __init__(self, max_len=384, point_landmarks=None):
        super().__init__()
        self.max_len = max_len
        
        # –í—ã–±–æ—Ä –∫–ª—é—á–µ–≤—ã—Ö landmarks
        face_landmarks = [33, 133, 362, 263, 61, 291, 199, 419, 17, 84, 17, 314, 405, 320, 307, 375, 321, 308, 324, 318]
        left_hand = [501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521]
        right_hand = [522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542]
        self.point_landmarks = face_landmarks + left_hand + right_hand
```

**–§—É–Ω–∫—Ü–∏–∏:**
- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–æ—Å–∞ (landmark 17)
- –í—ã–±–æ—Ä –∫–ª—é—á–µ–≤—ã—Ö landmarks (62 —Ç–æ—á–∫–∏)
- –í—ã—á–∏—Å–ª–µ–Ω–∏–µ motion features
- –û–±—Ä–µ–∑–∫–∞ –¥–æ max_len –∫–∞–¥—Ä–æ–≤

### 2. Motion Features

```python
def compute_motion_features(self, x):
    # velocity (lag1)
    dx = torch.zeros_like(x)
    dx[:, :-1] = x[:, 1:] - x[:, :-1]
    
    # acceleration (lag2)
    dx2 = torch.zeros_like(x)
    dx2[:, :-2] = x[:, 2:] - x[:, :-2]
    
    # relative motion
    relative_motion = torch.zeros_like(x)
    # ... –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è
    
    # temporal consistency
    temporal_consistency = torch.zeros_like(x)
    # ... –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è
    
    # motion magnitude
    motion_magnitude = torch.norm(dx, dim=-1, keepdim=True)
    
    # motion direction
    motion_direction = torch.atan2(dx[..., 1], dx[..., 0]).unsqueeze(-1)
    
    return dx, dx2, relative_motion, temporal_consistency, motion_magnitude, motion_direction
```

**–í—ã—Ö–æ–¥:** 744 features (62 landmarks √ó 2 coordinates √ó 6 motion features)

### 3. TCN Blocks

```python
class TemporalConvBlock(nn.Module):
    def __init__(self, dim, kernel_size=17, dilation=1, drop_rate=0.2):
        super().__init__()
        self.kernel_size = kernel_size
        self.dilation = dilation
        self.padding = (kernel_size - 1) * dilation
        
        # Causal convolution with dilation
        self.conv1 = nn.Conv1d(dim, dim, kernel_size, padding=self.padding, 
                              dilation=dilation, groups=dim)
        self.conv2 = nn.Conv1d(dim, dim, 1)  # Pointwise
        
        # Gated activation
        self.gate_conv = nn.Conv1d(dim, dim, kernel_size, padding=self.padding, 
                                  dilation=dilation, groups=dim)
        self.gate_conv2 = nn.Conv1d(dim, dim, 1)
        
        # Normalization
        self.bn = nn.BatchNorm1d(dim, momentum=0.95)
        self.dropout = nn.Dropout(drop_rate)
```

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- **Causal convolution** - —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–æ—Ä—è–¥–æ–∫
- **Dilation** - —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Ä–µ—Ü–µ–ø—Ç–∏–≤–Ω–æ–µ –ø–æ–ª–µ (1, 2, 4)
- **Gated activation** - —É–ª—É—á—à–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –ø–æ—Ç–æ–∫
- **Residual connection** - —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ

### 4. Bidirectional LSTM

```python
class BidirectionalLSTM(nn.Module):
    def __init__(self, dim, hidden_dim=None, num_layers=2, drop_rate=0.2):
        super().__init__()
        if hidden_dim is None:
            hidden_dim = dim
        
        self.lstm = nn.LSTM(
            input_size=dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            bidirectional=True,
            batch_first=True,
            dropout=drop_rate if num_layers > 1 else 0
        )
        
        # Projection back to original dimension
        self.projection = nn.Linear(hidden_dim * 2, dim)
        self.dropout = nn.Dropout(drop_rate)
```

**–§—É–Ω–∫—Ü–∏–∏:**
- **–î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞** - –∑–∞—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –æ–±–µ —Å—Ç–æ—Ä–æ–Ω—ã
- **–ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ—Å—Ç—å** - 2 —Å–ª–æ—è –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- **Projection** - –≤–æ–∑–≤—Ä–∞—Ç –∫ –∏—Å—Ö–æ–¥–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏

### 5. Temporal Attention

```python
class TemporalAttention(nn.Module):
    def __init__(self, dim, num_heads=8, drop_rate=0.2):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        
        # Temporal attention
        self.temporal_q = nn.Linear(dim, dim)
        self.temporal_k = nn.Linear(dim, dim)
        self.temporal_v = nn.Linear(dim, dim)
        
        # Output projection
        self.output_proj = nn.Linear(dim, dim)
        self.dropout = nn.Dropout(drop_rate)
        
        # Temporal position encoding
        self.temporal_pos_enc = nn.Parameter(torch.randn(1, 1000, dim))
```

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- **Multi-head attention** - 8 –≥–æ–ª–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤
- **Positional encoding** - –≤—Ä–µ–º–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
- **Scaled dot-product** - —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ

### 6. Conv1D Blocks

```python
class Conv1DBlock(nn.Module):
    def __init__(self, dim, kernel_size=17, drop_rate=0.2):
        super().__init__()
        self.kernel_size = kernel_size
        self.padding = kernel_size - 1  # Causal padding
        
        # Depthwise convolution
        self.depthwise = nn.Conv1d(dim, dim, kernel_size, padding=self.padding, groups=dim)
        self.pointwise = nn.Conv1d(dim, dim, 1)
        
        # BatchNorm + Swish
        self.bn = nn.BatchNorm1d(dim, momentum=0.95)
        self.dropout = nn.Dropout(drop_rate)
```

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- **Depthwise convolution** - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
- **Causal padding** - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
- **Swish activation** - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏

### 7. Transformer Block

```python
class TransformerBlock(nn.Module):
    def __init__(self, dim, num_heads=8, expand=2, drop_rate=0.2):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        
        # Multi-head attention
        self.attention = nn.MultiheadAttention(dim, num_heads, batch_first=True, dropout=drop_rate)
        self.attention_norm = nn.BatchNorm1d(dim, momentum=0.95)
        
        # Feed-forward network
        self.ffn = nn.Sequential(
            nn.Linear(dim, dim * expand),
            nn.SiLU(),  # Swish
            nn.Dropout(drop_rate),
            nn.Linear(dim * expand, dim),
            nn.Dropout(drop_rate)
        )
        self.ffn_norm = nn.BatchNorm1d(dim, momentum=0.95)
```

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- **Self-attention** - –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏
- **Feed-forward** - –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
- **BatchNorm** - —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è

### 8. Multi-scale Pooling

```python
# Global average pooling
global_avg = self.temporal_pool1(x_transposed).squeeze(-1)

# Global max pooling
global_max = self.temporal_pool2(x_transposed).squeeze(-1)

# Attention pooling
attn_out, _ = self.attention_pool(x_for_attn, x_for_attn, x_for_attn)
global_attn = torch.mean(attn_out, dim=1)

# Combine pooling results
x_pooled = torch.cat([global_avg, global_max, global_attn], dim=1)
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- **–†–∞–∑–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏** - –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å** - –∫ –≤–∞—Ä–∏–∞—Ü–∏—è–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- **–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å** - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

## üéØ –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è

### Adaptive Dropout

```python
class AdaptiveDropout(nn.Module):
    def __init__(self, initial_p=0.1, final_p=0.6, warmup_epochs=30):
        super().__init__()
        self.initial_p = initial_p
        self.final_p = final_p
        self.warmup_epochs = warmup_epochs
        self.current_epoch = 0
    
    def forward(self, x):
        if self.training:
            # Gradual dropout increase
            if self.current_epoch < self.warmup_epochs:
                p = self.initial_p + (self.final_p - self.initial_p) * (self.current_epoch / self.warmup_epochs)
            else:
                p = self.final_p
            return F.dropout(x, p=p, training=True)
        return x
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- **–ü–ª–∞–≤–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è** - –≤–º–µ—Å—Ç–æ —Ä–µ–∑–∫–æ–≥–æ –≤–∫–ª—é—á–µ–Ω–∏—è
- **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å** - –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –ø–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å** - –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è

## üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏

### –û–±—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
- **Total Parameters**: 1,968,601
- **Trainable Parameters**: 1,968,601
- **Input Dimension**: 744 (–ø–æ—Å–ª–µ preprocessing)
- **Hidden Dimension**: 192
- **Number of Classes**: 25
- **Max Sequence Length**: 384

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º
- **TCN Blocks**: ~300K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **LSTM**: ~150K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **Attention**: ~200K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **Conv1D**: ~400K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **Transformer**: ~300K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **Classifier**: ~600K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

## üîç –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

### 1. –í—Ä–µ–º–µ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
- **TCN** - –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- **LSTM** - –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- **Attention** - –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏

### 2. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
- **Depthwise convolutions** - —É–º–µ–Ω—å—à–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **Causal padding** - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞
- **Residual connections** - —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤

### 3. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
- **Adaptive Dropout** - –ø–ª–∞–≤–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è
- **BatchNorm** - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
- **Label Smoothing** - —É–ª—É—á—à–µ–Ω–∏–µ –æ–±–æ–±—â–µ–Ω–∏—è

### 4. –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å
- **Modular design** - –ª–µ–≥–∫–æ –∏–∑–º–µ–Ω—è—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
- **Configurable** - –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
- **Extensible** - –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤

## üéØ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

1. **–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ** - —Å–æ—á–µ—Ç–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤
2. **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å** - –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
3. **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å** - —É—Å—Ç–æ–π—á–∏–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ
4. **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å** - –≤–Ω–∏–º–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∞–∂–Ω—ã–µ –∫–∞–¥—Ä—ã
5. **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å** - –ª–µ–≥–∫–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [Temporal Convolutional Networks](https://arxiv.org/abs/1609.03499)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [LSTM Networks](https://arxiv.org/abs/1503.04069)
- [Batch Normalization](https://arxiv.org/abs/1502.03167)

---

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é! üèóÔ∏è** 
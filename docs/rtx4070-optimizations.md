# ‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è RTX4070

–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ GPU NVIDIA RTX4070.

## üéØ –û–±–∑–æ—Ä RTX4070

### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: Ada Lovelace (AD104)
- **CUDA Cores**: 5,888
- **Memory**: 12GB GDDR6X
- **Memory Bandwidth**: 504 GB/s
- **Base Clock**: 1.92 GHz
- **Boost Clock**: 2.48 GHz
- **TDP**: 200W

### –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è ML
- **Tensor Cores**: 4-–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è
- **RT Cores**: 3-–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è
- **DLSS 3**: AI-—É—Å–∫–æ—Ä–µ–Ω–∏–µ
- **AV1 Encoding**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ
- **PCIe 4.0**: –í—ã—Å–æ–∫–∞—è –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å

## üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ CUDA

### 1. TF32 (Tensor Float 32)
```python
# –í–∫–ª—é—á–µ–Ω–∏–µ TF32 –¥–ª—è Ampere –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –£—Å–∫–æ—Ä–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –≤ 2-4 —Ä–∞–∑–∞
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–∞ RTX4070
- –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º

### 2. Mixed Precision Training
```python
from torch.cuda.amp import autocast, GradScaler

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è scaler
scaler = GradScaler()

# Training loop —Å mixed precision
for batch in dataloader:
    with autocast():
        outputs = model(inputs)
        loss = criterion(outputs, targets)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –£–º–µ–Ω—å—à–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –Ω–∞ 50%
- –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –≤ 1.5-2 —Ä–∞–∑–∞
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏

### 3. cuDNN Benchmark
```python
# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Å–≤–µ—Ä—Ç–∫–∏
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False
```

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –í–∫–ª—é—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –¥–ª—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

## üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ PyTorch

### 1. Memory Management
```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
def optimize_memory():
    torch.cuda.empty_cache()
    gc.collect()

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
def monitor_gpu_memory():
    allocated = torch.cuda.memory_allocated() / 1024**3
    cached = torch.cuda.memory_reserved() / 1024**3
    print(f"Allocated: {allocated:.2f} GB")
    print(f"Cached: {cached:.2f} GB")
```

### 2. DataLoader Optimization
```python
# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataLoader
train_loader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,        # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è RTX4070
    pin_memory=True,      # –£—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö
    persistent_workers=True,  # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ workers –º–µ–∂–¥—É —ç–ø–æ—Ö–∞–º–∏
    prefetch_factor=2     # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
)
```

### 3. Model Compilation (PyTorch 2.0+)
```python
# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
if hasattr(torch, 'compile'):
    model = torch.compile(model, mode='max-autotune')
```

## üìä –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è RTX4070

### Batch Size Optimization
```python
def find_optimal_batch_size(model, input_shape=(3, 16, 224, 224)):
    """–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –¥–ª—è RTX4070"""
    batch_size = 1
    max_batch_size = 1
    
    while batch_size <= 64:
        try:
            torch.cuda.empty_cache()
            
            dummy_input = torch.randn(batch_size, *input_shape).cuda()
            dummy_target = torch.randint(0, 25, (batch_size,)).cuda()
            
            output = model(dummy_input)
            loss = torch.nn.CrossEntropyLoss()(output, dummy_target)
            loss.backward()
            
            max_batch_size = batch_size
            print(f"‚úÖ Batch size {batch_size} works")
            batch_size *= 2
            
        except torch.cuda.OutOfMemoryError:
            print(f"‚ùå Batch size {batch_size} failed")
            break
    
    optimal_batch_size = max_batch_size // 2  # 50% –æ—Ç –º–∞–∫—Å–∏–º—É–º–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    return optimal_batch_size
```

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
```python
RTX4070_CONFIG = {
    'batch_size': 32,           # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è 12GB VRAM
    'num_workers': 4,           # –î–ª—è DataLoader
    'mixed_precision': True,    # –í–∫–ª—é—á–∏—Ç—å FP16
    'gradient_accumulation': 2, # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size = 64
    'max_epochs': 300,
    'learning_rate': 4e-4,
    'weight_decay': 0.005,
    'scheduler': 'cosine_annealing_warm_restarts',
    'warmup_epochs': 10
}
```

## üîç –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### GPU Monitoring
```python
import pynvml

def monitor_gpu():
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
    pynvml.nvmlInit()
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    
    # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
    temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
    
    # –ü–∞–º—è—Ç—å
    memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
    memory_used = memory_info.used // 1024**2  # MB
    memory_total = memory_info.total // 1024**2  # MB
    
    # –£—Ç–∏–ª–∏–∑–∞—Ü–∏—è
    utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
    
    return {
        'temperature': temp,
        'memory_used_mb': memory_used,
        'memory_total_mb': memory_total,
        'memory_percent': memory_used / memory_total * 100,
        'gpu_utilization': utilization.gpu,
        'memory_utilization': utilization.memory
    }
```

### Performance Benchmarking
```python
def benchmark_model(model, input_shape, num_runs=100):
    """–ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
    model.eval()
    
    # Warmup
    dummy_input = torch.randn(1, *input_shape).cuda()
    for _ in range(10):
        with torch.no_grad():
            _ = model(dummy_input)
    
    # Benchmark
    torch.cuda.synchronize()
    start_time = time.time()
    
    for _ in range(num_runs):
        with torch.no_grad():
            _ = model(dummy_input)
    
    torch.cuda.synchronize()
    end_time = time.time()
    
    avg_time = (end_time - start_time) / num_runs
    fps = 1 / avg_time
    
    return avg_time, fps
```

## üéØ –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### 1. Kernel Optimization
```python
# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è RTX4070
def setup_rtx4070_optimizations():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –¥–ª—è RTX4070"""
    
    # TF32
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    
    # cuDNN
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # Memory
    torch.cuda.empty_cache()
    
    # Environment variables
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
    os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'
```

### 2. Memory Optimization
```python
class MemoryOptimizedTrainer:
    """–¢—Ä–µ–Ω–µ—Ä —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏"""
    
    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        self.scaler = GradScaler()
        
    def train_step(self, batch, optimizer):
        inputs, targets = batch
        inputs, targets = inputs.to(self.device), targets.to(self.device)
        
        # Mixed precision
        with autocast():
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)
        
        # Gradient scaling
        self.scaler.scale(loss).backward()
        self.scaler.step(optimizer)
        self.scaler.update()
        
        # Memory cleanup every N steps
        if self.step % 50 == 0:
            torch.cuda.empty_cache()
        
        return loss.item()
```

### 3. Inference Optimization
```python
class OptimizedInference:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π inference –¥–ª—è RTX4070"""
    
    def __init__(self, model, batch_size=32):
        self.model = model.eval()
        self.batch_size = batch_size
        
        # Compile model if available
        if hasattr(torch, 'compile'):
            self.model = torch.compile(self.model)
    
    @torch.no_grad()
    def predict_batch(self, inputs):
        """–ë–∞—Ç—á–µ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        outputs = self.model(inputs)
        probabilities = torch.softmax(outputs, dim=1)
        return probabilities
    
    def predict_stream(self, input_stream):
        """–°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        batch = []
        results = []
        
        for input_data in input_stream:
            batch.append(input_data)
            
            if len(batch) >= self.batch_size:
                batch_tensor = torch.stack(batch).to(self.device)
                batch_results = self.predict_batch(batch_tensor)
                results.extend(batch_results.cpu().numpy())
                batch = []
        
        # Process remaining items
        if batch:
            batch_tensor = torch.stack(batch).to(self.device)
            batch_results = self.predict_batch(batch_tensor)
            results.extend(batch_results.cpu().numpy())
        
        return results
```

## üìà –û–∂–∏–¥–∞–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **Training Speed**: 2-3x —É—Å–∫–æ—Ä–µ–Ω–∏–µ —Å mixed precision
- **Memory Usage**: 50% —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Å FP16
- **Inference Speed**: 1.5-2x —É—Å–∫–æ—Ä–µ–Ω–∏–µ —Å compilation
- **Batch Size**: –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä 32-64

### –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- **Temperature**: –ü–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–µ <80¬∞C
- **Memory**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ 12GB VRAM
- **Power**: –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ

## üîß Troubleshooting

### Out of Memory
```python
# –†–µ—à–µ–Ω–∏—è –¥–ª—è OOM
def fix_oom():
    # 1. –£–º–µ–Ω—å—à–∏—Ç—å batch size
    batch_size = 16  # –≤–º–µ—Å—Ç–æ 32
    
    # 2. –í–∫–ª—é—á–∏—Ç—å gradient checkpointing
    model.gradient_checkpointing_enable()
    
    # 3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mixed precision
    scaler = GradScaler()
    
    # 4. –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à
    torch.cuda.empty_cache()
```

### Low Performance
```python
# –†–µ—à–µ–Ω–∏—è –¥–ª—è –Ω–∏–∑–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
def fix_performance():
    # 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å TF32
    assert torch.backends.cuda.matmul.allow_tf32
    
    # 2. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å DataLoader
    num_workers = 4
    pin_memory = True
    
    # 3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å compilation
    model = torch.compile(model)
    
    # 4. –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å GPU utilization
    monitor_gpu()
```

## üìä Benchmark Results

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

| –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è | Training Time | Memory Usage | Accuracy |
|-------------|---------------|--------------|----------|
| Baseline | 3h | 8GB | 75.76% |
| + Mixed Precision | 1.75h | 4GB | 75.76% |
| + TF32 | 1.5h | 4GB | 75.76% |
| + Compilation | 1.2h | 4GB | 75.76% |

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
1. **–í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mixed precision**
2. **–í–∫–ª—é—á–∏—Ç—å TF32 –¥–ª—è RTX4070**
3. **–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å batch size**
4. **–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –∏ –ø–∞–º—è—Ç—å**

## üéØ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

RTX4070 –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è ML:
- **–í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
- **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏** —Å mixed precision
- **–°—Ç–∞–±–∏–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞** –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–µ
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å** –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π

---

**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ RTX4070! ‚ö°** 